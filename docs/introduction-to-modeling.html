<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title> 8 Introduction to modeling | Exam PA Study Guide, Spring 2022</title>
  <meta name="description" content=" 8 Introduction to modeling | Exam PA Study Guide, Spring 2022" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content=" 8 Introduction to modeling | Exam PA Study Guide, Spring 2022" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content=" 8 Introduction to modeling | Exam PA Study Guide, Spring 2022" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="images/artificial_actuary_logo_favicon.png" type="image/x-icon" />
<link rel="prev" href="data-exploration.html"/>
<link rel="next" href="generalized-linear-models-glms.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Exam PA Study Manual</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a>
<ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#faq-frequently-asked-questions"><i class="fa fa-check"></i><b>0.1</b> FAQ: Frequently Asked Questions</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="the-exam.html"><a href="the-exam.html"><i class="fa fa-check"></i><b>1</b> The exam</a></li>
<li class="chapter" data-level="2" data-path="prometric-demo.html"><a href="prometric-demo.html"><i class="fa fa-check"></i><b>2</b> Prometric Demo</a></li>
<li class="chapter" data-level="3" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>3</b> Introduction</a></li>
<li class="chapter" data-level="4" data-path="getting-started.html"><a href="getting-started.html"><i class="fa fa-check"></i><b>4</b> Getting started</a>
<ul>
<li class="chapter" data-level="4.1" data-path="getting-started.html"><a href="getting-started.html#installing-r"><i class="fa fa-check"></i><b>4.1</b> Installing R</a></li>
<li class="chapter" data-level="4.2" data-path="getting-started.html"><a href="getting-started.html#installing-rstudio"><i class="fa fa-check"></i><b>4.2</b> Installing RStudio</a></li>
<li class="chapter" data-level="4.3" data-path="getting-started.html"><a href="getting-started.html#download-the-data"><i class="fa fa-check"></i><b>4.3</b> Download the data</a></li>
<li class="chapter" data-level="4.4" data-path="getting-started.html"><a href="getting-started.html#download-islr"><i class="fa fa-check"></i><b>4.4</b> Download ISLR</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="how-much-r-do-i-need-to-know-to-pass.html"><a href="how-much-r-do-i-need-to-know-to-pass.html"><i class="fa fa-check"></i><b>5</b> How much R do I need to know to pass?</a>
<ul>
<li class="chapter" data-level="5.1" data-path="how-much-r-do-i-need-to-know-to-pass.html"><a href="how-much-r-do-i-need-to-know-to-pass.html#how-to-use-the-pa-r-cheat-sheets"><i class="fa fa-check"></i><b>5.1</b> How to use the PA R cheat sheets?</a></li>
<li class="chapter" data-level="5.2" data-path="how-much-r-do-i-need-to-know-to-pass.html"><a href="how-much-r-do-i-need-to-know-to-pass.html#example-soa-pa-61620-task-8"><i class="fa fa-check"></i><b>5.2</b> Example: SOA PA 6/16/20, Task 8</a></li>
<li class="chapter" data-level="5.3" data-path="how-much-r-do-i-need-to-know-to-pass.html"><a href="how-much-r-do-i-need-to-know-to-pass.html#example-2---data-exploration"><i class="fa fa-check"></i><b>5.3</b> Example 2 - Data exploration</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="r-programming.html"><a href="r-programming.html"><i class="fa fa-check"></i><b>6</b> R programming</a>
<ul>
<li class="chapter" data-level="6.1" data-path="r-programming.html"><a href="r-programming.html#notebook-chunks"><i class="fa fa-check"></i><b>6.1</b> Notebook chunks</a></li>
<li class="chapter" data-level="6.2" data-path="r-programming.html"><a href="r-programming.html#basic-operations"><i class="fa fa-check"></i><b>6.2</b> Basic operations</a></li>
<li class="chapter" data-level="6.3" data-path="r-programming.html"><a href="r-programming.html#lists"><i class="fa fa-check"></i><b>6.3</b> Lists</a></li>
<li class="chapter" data-level="6.4" data-path="r-programming.html"><a href="r-programming.html#functions"><i class="fa fa-check"></i><b>6.4</b> Functions</a></li>
<li class="chapter" data-level="6.5" data-path="r-programming.html"><a href="r-programming.html#data-frames"><i class="fa fa-check"></i><b>6.5</b> Data frames</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="data-exploration.html"><a href="data-exploration.html"><i class="fa fa-check"></i><b>7</b> Data exploration</a>
<ul>
<li class="chapter" data-level="7.1" data-path="data-exploration.html"><a href="data-exploration.html#how-to-make-graphs-in-r"><i class="fa fa-check"></i><b>7.1</b> How to make graphs in R?</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="data-exploration.html"><a href="data-exploration.html#add-a-plot"><i class="fa fa-check"></i><b>7.1.1</b> Add a plot</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="data-exploration.html"><a href="data-exploration.html#the-different-graph-types"><i class="fa fa-check"></i><b>7.2</b> The different graph types</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="data-exploration.html"><a href="data-exploration.html#histogram"><i class="fa fa-check"></i><b>7.2.1</b> Histogram</a></li>
<li class="chapter" data-level="7.2.2" data-path="data-exploration.html"><a href="data-exploration.html#box-plot"><i class="fa fa-check"></i><b>7.2.2</b> Box plot</a></li>
<li class="chapter" data-level="7.2.3" data-path="data-exploration.html"><a href="data-exploration.html#scatterplot"><i class="fa fa-check"></i><b>7.2.3</b> Scatterplot</a></li>
<li class="chapter" data-level="7.2.4" data-path="data-exploration.html"><a href="data-exploration.html#bar-charts"><i class="fa fa-check"></i><b>7.2.4</b> Bar charts</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="data-exploration.html"><a href="data-exploration.html#how-to-save-time-with-dplyr"><i class="fa fa-check"></i><b>7.3</b> How to save time with dplyr?</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="data-exploration.html"><a href="data-exploration.html#data-manipulation-chaining"><i class="fa fa-check"></i><b>7.3.1</b> Data manipulation chaining</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="data-exploration.html"><a href="data-exploration.html#how-to-explore-the-data"><i class="fa fa-check"></i><b>7.4</b> How to explore the data?</a></li>
<li class="chapter" data-level="7.5" data-path="data-exploration.html"><a href="data-exploration.html#how-to-transform-the-data"><i class="fa fa-check"></i><b>7.5</b> How to transform the data?</a></li>
<li class="chapter" data-level="7.6" data-path="data-exploration.html"><a href="data-exploration.html#missing-values"><i class="fa fa-check"></i><b>7.6</b> Missing values</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="data-exploration.html"><a href="data-exploration.html#types-of-missing-values"><i class="fa fa-check"></i><b>7.6.1</b> Types of Missing Values</a></li>
<li class="chapter" data-level="7.6.2" data-path="data-exploration.html"><a href="data-exploration.html#missing-value-resolutions"><i class="fa fa-check"></i><b>7.6.2</b> Missing Value Resolutions:</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="data-exploration.html"><a href="data-exploration.html#example-soa-pa-121219-task-1"><i class="fa fa-check"></i><b>7.7</b> Example: SOA PA 12/12/19, Task 1</a>
<ul>
<li class="chapter" data-level="7.7.1" data-path="data-exploration.html"><a href="data-exploration.html#garbage-in-garbage-out"><i class="fa fa-check"></i><b>7.7.1</b> Garbage in; garbage out 🗑</a></li>
<li class="chapter" data-level="7.7.2" data-path="data-exploration.html"><a href="data-exploration.html#be-a-detective"><i class="fa fa-check"></i><b>7.7.2</b> Be a detective 🔍</a></li>
<li class="chapter" data-level="7.7.3" data-path="data-exploration.html"><a href="data-exploration.html#a-picture-is-worth-a-thousand-words"><i class="fa fa-check"></i><b>7.7.3</b> A picture is worth a thousand words 📷</a></li>
<li class="chapter" data-level="7.7.4" data-path="data-exploration.html"><a href="data-exploration.html#factor-or-numeric"><i class="fa fa-check"></i><b>7.7.4</b> Factor or numeric ❓</a></li>
<li class="chapter" data-level="7.7.5" data-path="data-exploration.html"><a href="data-exploration.html#of-statistics-are-false"><i class="fa fa-check"></i><b>7.7.5</b> 73.6% of statistics are false 😲</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="data-exploration.html"><a href="data-exploration.html#exercises"><i class="fa fa-check"></i><b>7.8</b> Exercises</a>
<ul>
<li class="chapter" data-level="7.8.1" data-path="data-exploration.html"><a href="data-exploration.html#data-exploration-practice"><i class="fa fa-check"></i><b>7.8.1</b> Data Exploration Practice</a></li>
<li class="chapter" data-level="7.8.2" data-path="data-exploration.html"><a href="data-exploration.html#dplyr-practice"><i class="fa fa-check"></i><b>7.8.2</b> Dplyr Practice</a></li>
</ul></li>
<li class="chapter" data-level="7.9" data-path="data-exploration.html"><a href="data-exploration.html#answers-to-exercises"><i class="fa fa-check"></i><b>7.9</b> Answers to exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html"><i class="fa fa-check"></i><b>8</b> Introduction to modeling</a>
<ul>
<li class="chapter" data-level="8.1" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#modeling-vocabulary"><i class="fa fa-check"></i><b>8.1</b> Modeling vocabulary</a></li>
<li class="chapter" data-level="8.2" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#modeling-notation"><i class="fa fa-check"></i><b>8.2</b> Modeling notation</a></li>
<li class="chapter" data-level="8.3" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#ordinary-least-squares-ols"><i class="fa fa-check"></i><b>8.3</b> Ordinary Least Squares (OLS)</a></li>
<li class="chapter" data-level="8.4" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#r2-statistic"><i class="fa fa-check"></i><b>8.4</b> R^2 Statistic</a></li>
<li class="chapter" data-level="8.5" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#correlation"><i class="fa fa-check"></i><b>8.5</b> Correlation</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#pearsons-correlation"><i class="fa fa-check"></i><b>8.5.1</b> Pearson’s correlation</a></li>
<li class="chapter" data-level="8.5.2" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#spearman-rank-correlation"><i class="fa fa-check"></i><b>8.5.2</b> Spearman (rank) correlation</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#regression-vs.-classification"><i class="fa fa-check"></i><b>8.6</b> Regression vs. classification</a></li>
<li class="chapter" data-level="8.7" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#regression-metrics"><i class="fa fa-check"></i><b>8.7</b> Regression metrics</a>
<ul>
<li class="chapter" data-level="8.7.1" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#example-soa-pa-61820-task-4"><i class="fa fa-check"></i><b>8.7.1</b> Example: SOA PA 6/18/20, Task 4</a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#example-health-costs"><i class="fa fa-check"></i><b>8.8</b> Example: Health Costs</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html"><i class="fa fa-check"></i><b>9</b> Generalized linear Models (GLMs)</a>
<ul>
<li class="chapter" data-level="9.0.1" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#assumptions-of-ols"><i class="fa fa-check"></i><b>9.0.1</b> Assumptions of OLS</a></li>
<li class="chapter" data-level="9.0.2" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#assumptions-of-glms"><i class="fa fa-check"></i><b>9.0.2</b> Assumptions of GLMs</a></li>
<li class="chapter" data-level="9.1" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#advantages-and-disadvantages"><i class="fa fa-check"></i><b>9.1</b> Advantages and disadvantages</a></li>
<li class="chapter" data-level="9.2" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#glms-for-regression"><i class="fa fa-check"></i><b>9.2</b> GLMs for regression</a></li>
<li class="chapter" data-level="9.3" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#interpretation-of-coefficients"><i class="fa fa-check"></i><b>9.3</b> Interpretation of coefficients</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#identity-link"><i class="fa fa-check"></i><b>9.3.1</b> Identity link</a></li>
<li class="chapter" data-level="9.3.2" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#log-link"><i class="fa fa-check"></i><b>9.3.2</b> Log link</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#other-links"><i class="fa fa-check"></i><b>9.4</b> Other links</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="glms-for-classification.html"><a href="glms-for-classification.html"><i class="fa fa-check"></i><b>10</b> GLMs for classification</a>
<ul>
<li class="chapter" data-level="10.1" data-path="glms-for-classification.html"><a href="glms-for-classification.html#binary-target"><i class="fa fa-check"></i><b>10.1</b> Binary target</a></li>
<li class="chapter" data-level="10.2" data-path="glms-for-classification.html"><a href="glms-for-classification.html#count-target"><i class="fa fa-check"></i><b>10.2</b> Count target</a></li>
<li class="chapter" data-level="10.3" data-path="glms-for-classification.html"><a href="glms-for-classification.html#link-functions"><i class="fa fa-check"></i><b>10.3</b> Link functions</a></li>
<li class="chapter" data-level="10.4" data-path="glms-for-classification.html"><a href="glms-for-classification.html#interpretation-of-coefficients-1"><i class="fa fa-check"></i><b>10.4</b> Interpretation of coefficients</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="glms-for-classification.html"><a href="glms-for-classification.html#logit"><i class="fa fa-check"></i><b>10.4.1</b> Logit</a></li>
<li class="chapter" data-level="10.4.2" data-path="glms-for-classification.html"><a href="glms-for-classification.html#probit-cauchit-cloglog"><i class="fa fa-check"></i><b>10.4.2</b> Probit, Cauchit, Cloglog</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="glms-for-classification.html"><a href="glms-for-classification.html#demo-the-model-for-interpretation"><i class="fa fa-check"></i><b>10.5</b> Demo the model for interpretation</a></li>
<li class="chapter" data-level="10.6" data-path="glms-for-classification.html"><a href="glms-for-classification.html#example---auto-claims"><i class="fa fa-check"></i><b>10.6</b> Example - Auto Claims</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="classification-metrics.html"><a href="classification-metrics.html"><i class="fa fa-check"></i><b>11</b> Classification metrics</a>
<ul>
<li class="chapter" data-level="11.1" data-path="classification-metrics.html"><a href="classification-metrics.html#area-under-the-roc-curve-auc"><i class="fa fa-check"></i><b>11.1</b> Area Under the ROC Curve (AUC)</a></li>
<li class="chapter" data-level="11.2" data-path="classification-metrics.html"><a href="classification-metrics.html#example---auto-claims-1"><i class="fa fa-check"></i><b>11.2</b> Example - Auto Claims</a></li>
<li class="chapter" data-level="11.3" data-path="classification-metrics.html"><a href="classification-metrics.html#example-soa-hr-task-5"><i class="fa fa-check"></i><b>11.3</b> Example: SOA HR, Task 5</a></li>
<li class="chapter" data-level="11.4" data-path="classification-metrics.html"><a href="classification-metrics.html#example-soa-pa-121219-task-11"><i class="fa fa-check"></i><b>11.4</b> Example: SOA PA 12/12/19, Task 11</a></li>
<li class="chapter" data-level="11.5" data-path="classification-metrics.html"><a href="classification-metrics.html#additional-reading"><i class="fa fa-check"></i><b>11.5</b> Additional reading</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html"><i class="fa fa-check"></i><b>12</b> Additional GLM topics</a>
<ul>
<li class="chapter" data-level="12.1" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#residuals"><i class="fa fa-check"></i><b>12.1</b> Residuals</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#raw-residuals"><i class="fa fa-check"></i><b>12.1.1</b> Raw residuals</a></li>
<li class="chapter" data-level="12.1.2" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#deviance-residuals"><i class="fa fa-check"></i><b>12.1.2</b> Deviance residuals</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#example"><i class="fa fa-check"></i><b>12.2</b> Example</a></li>
<li class="chapter" data-level="12.3" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#log-transforms-of-predictors"><i class="fa fa-check"></i><b>12.3</b> Log transforms of predictors</a></li>
<li class="chapter" data-level="12.4" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#example-1"><i class="fa fa-check"></i><b>12.4</b> Example</a></li>
<li class="chapter" data-level="12.5" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#reference-levels"><i class="fa fa-check"></i><b>12.5</b> Reference levels</a></li>
<li class="chapter" data-level="12.6" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#interactions"><i class="fa fa-check"></i><b>12.6</b> Interactions</a></li>
<li class="chapter" data-level="12.7" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#offsets"><i class="fa fa-check"></i><b>12.7</b> Offsets</a></li>
<li class="chapter" data-level="12.8" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#tweedie-regression"><i class="fa fa-check"></i><b>12.8</b> Tweedie regression</a></li>
<li class="chapter" data-level="12.9" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#combinations-of-link-functions-and-target-distributions"><i class="fa fa-check"></i><b>12.9</b> Combinations of Link Functions and Target Distributions</a>
<ul>
<li class="chapter" data-level="12.9.1" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#gaussian-response-with-log-link"><i class="fa fa-check"></i><b>12.9.1</b> Gaussian Response with Log Link</a></li>
<li class="chapter" data-level="12.9.2" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#gaussian-response-with-inverse-link"><i class="fa fa-check"></i><b>12.9.2</b> Gaussian Response with Inverse Link</a></li>
<li class="chapter" data-level="12.9.3" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#gaussian-response-with-identity-link"><i class="fa fa-check"></i><b>12.9.3</b> Gaussian Response with Identity Link</a></li>
<li class="chapter" data-level="12.9.4" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#gaussian-response-with-log-link-and-negative-values"><i class="fa fa-check"></i><b>12.9.4</b> Gaussian Response with Log Link and Negative Values</a></li>
<li class="chapter" data-level="12.9.5" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#gamma-response-with-log-link"><i class="fa fa-check"></i><b>12.9.5</b> Gamma Response with Log Link</a></li>
<li class="chapter" data-level="12.9.6" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#gamma-with-inverse-link"><i class="fa fa-check"></i><b>12.9.6</b> Gamma with Inverse Link</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html"><i class="fa fa-check"></i><b>13</b> GLM variable selection</a>
<ul>
<li class="chapter" data-level="13.1" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html#stepwise-subset-selection"><i class="fa fa-check"></i><b>13.1</b> Stepwise subset selection</a></li>
<li class="chapter" data-level="13.2" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html#example-soa-pa-61219-task-6"><i class="fa fa-check"></i><b>13.2</b> Example: SOA PA 6/12/19, Task 6</a></li>
<li class="chapter" data-level="13.3" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html#penalized-linear-models"><i class="fa fa-check"></i><b>13.3</b> Penalized Linear Models</a></li>
<li class="chapter" data-level="13.4" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html#ridge-regression"><i class="fa fa-check"></i><b>13.4</b> Ridge Regression</a></li>
<li class="chapter" data-level="13.5" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html#lasso"><i class="fa fa-check"></i><b>13.5</b> Lasso</a></li>
<li class="chapter" data-level="13.6" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html#elastic-net"><i class="fa fa-check"></i><b>13.6</b> Elastic Net</a></li>
<li class="chapter" data-level="13.7" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html#advantages-and-disadvantages-1"><i class="fa fa-check"></i><b>13.7</b> Advantages and disadvantages</a></li>
<li class="chapter" data-level="13.8" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html#example-ridge-regression"><i class="fa fa-check"></i><b>13.8</b> Example: Ridge Regression</a></li>
<li class="chapter" data-level="13.9" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html#example-the-lasso"><i class="fa fa-check"></i><b>13.9</b> Example: The Lasso</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="bias-variance-trade-off.html"><a href="bias-variance-trade-off.html"><i class="fa fa-check"></i><b>14</b> Bias-variance trade-off</a></li>
<li class="chapter" data-level="15" data-path="tree-based-models.html"><a href="tree-based-models.html"><i class="fa fa-check"></i><b>15</b> Tree-based models</a>
<ul>
<li class="chapter" data-level="15.1" data-path="tree-based-models.html"><a href="tree-based-models.html#decision-trees"><i class="fa fa-check"></i><b>15.1</b> Decision Trees</a>
<ul>
<li class="chapter" data-level="15.1.1" data-path="tree-based-models.html"><a href="tree-based-models.html#example-soa-pa-6182020-task-6"><i class="fa fa-check"></i><b>15.1.1</b> Example: SOA PA 6/18/2020, Task 6</a></li>
<li class="chapter" data-level="15.1.2" data-path="tree-based-models.html"><a href="tree-based-models.html#advantages-and-disadvantages-2"><i class="fa fa-check"></i><b>15.1.2</b> Advantages and disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="tree-based-models.html"><a href="tree-based-models.html#ensemble-learning"><i class="fa fa-check"></i><b>15.2</b> Ensemble learning</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="tree-based-models.html"><a href="tree-based-models.html#bagging"><i class="fa fa-check"></i><b>15.2.1</b> Bagging</a></li>
<li class="chapter" data-level="15.2.2" data-path="tree-based-models.html"><a href="tree-based-models.html#boosting"><i class="fa fa-check"></i><b>15.2.2</b> Boosting</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="tree-based-models.html"><a href="tree-based-models.html#random-forests"><i class="fa fa-check"></i><b>15.3</b> Random Forests</a>
<ul>
<li class="chapter" data-level="15.3.1" data-path="tree-based-models.html"><a href="tree-based-models.html#example-2"><i class="fa fa-check"></i><b>15.3.1</b> Example</a></li>
<li class="chapter" data-level="15.3.2" data-path="tree-based-models.html"><a href="tree-based-models.html#variable-importance"><i class="fa fa-check"></i><b>15.3.2</b> Variable Importance</a></li>
<li class="chapter" data-level="15.3.3" data-path="tree-based-models.html"><a href="tree-based-models.html#partial-dependence"><i class="fa fa-check"></i><b>15.3.3</b> Partial dependence</a></li>
<li class="chapter" data-level="15.3.4" data-path="tree-based-models.html"><a href="tree-based-models.html#advantages-and-disadvantages-3"><i class="fa fa-check"></i><b>15.3.4</b> Advantages and disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="tree-based-models.html"><a href="tree-based-models.html#gradient-boosted-trees"><i class="fa fa-check"></i><b>15.4</b> Gradient Boosted Trees</a>
<ul>
<li class="chapter" data-level="15.4.1" data-path="tree-based-models.html"><a href="tree-based-models.html#gradient-boosting"><i class="fa fa-check"></i><b>15.4.1</b> Gradient Boosting</a></li>
<li class="chapter" data-level="15.4.2" data-path="tree-based-models.html"><a href="tree-based-models.html#notation"><i class="fa fa-check"></i><b>15.4.2</b> Notation</a></li>
<li class="chapter" data-level="15.4.3" data-path="tree-based-models.html"><a href="tree-based-models.html#parameters"><i class="fa fa-check"></i><b>15.4.3</b> Parameters</a></li>
<li class="chapter" data-level="15.4.4" data-path="tree-based-models.html"><a href="tree-based-models.html#example-3"><i class="fa fa-check"></i><b>15.4.4</b> Example</a></li>
<li class="chapter" data-level="15.4.5" data-path="tree-based-models.html"><a href="tree-based-models.html#advantages-and-disadvantages-4"><i class="fa fa-check"></i><b>15.4.5</b> Advantages and disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="15.5" data-path="tree-based-models.html"><a href="tree-based-models.html#exercises-1"><i class="fa fa-check"></i><b>15.5</b> Exercises</a>
<ul>
<li class="chapter" data-level="15.5.1" data-path="tree-based-models.html"><a href="tree-based-models.html#rf-tuning-with-caret"><i class="fa fa-check"></i><b>15.5.1</b> 1. RF tuning with <code>caret</code></a></li>
<li class="chapter" data-level="15.5.2" data-path="tree-based-models.html"><a href="tree-based-models.html#tuning-a-gbm-with-caret"><i class="fa fa-check"></i><b>15.5.2</b> 2. Tuning a GBM with <code>caret</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html"><i class="fa fa-check"></i><b>16</b> Unsupervised Learning</a>
<ul>
<li class="chapter" data-level="16.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#types-of-learning"><i class="fa fa-check"></i><b>16.1</b> Types of Learning</a></li>
<li class="chapter" data-level="16.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#correlation-analysis"><i class="fa fa-check"></i><b>16.2</b> Correlation Analysis</a>
<ul>
<li class="chapter" data-level="16.2.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#correlation-does-not-equal-causation"><i class="fa fa-check"></i><b>16.2.1</b> Correlation does not equal causation</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#principal-component-analysis-pca"><i class="fa fa-check"></i><b>16.3</b> Principal Component Analysis (PCA)</a>
<ul>
<li class="chapter" data-level="16.3.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#example-us-arrests"><i class="fa fa-check"></i><b>16.3.1</b> Example: US Arrests</a></li>
<li class="chapter" data-level="16.3.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#example-soa-pa-61219-task-3"><i class="fa fa-check"></i><b>16.3.2</b> Example: SOA PA 6/12/19, Task 3</a></li>
<li class="chapter" data-level="16.3.3" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#example-pca-on-cancel-cells"><i class="fa fa-check"></i><b>16.3.3</b> Example: PCA on Cancel Cells</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#clustering"><i class="fa fa-check"></i><b>16.4</b> Clustering</a></li>
<li class="chapter" data-level="16.5" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#k-means-clustering"><i class="fa fa-check"></i><b>16.5</b> K-Means Clustering</a></li>
<li class="chapter" data-level="16.6" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#hierarchical-clustering"><i class="fa fa-check"></i><b>16.6</b> Hierarchical Clustering</a>
<ul>
<li class="chapter" data-level="16.6.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#example-clustering-cancel-cells"><i class="fa fa-check"></i><b>16.6.1</b> Example: Clustering Cancel Cells</a></li>
<li class="chapter" data-level="16.6.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#references"><i class="fa fa-check"></i><b>16.6.2</b> References</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="writing-and-communication.html"><a href="writing-and-communication.html"><i class="fa fa-check"></i><b>17</b> Writing and Communication</a>
<ul>
<li class="chapter" data-level="17.1" data-path="writing-and-communication.html"><a href="writing-and-communication.html#spelling-and-grammar"><i class="fa fa-check"></i><b>17.1</b> Spelling and Grammar</a></li>
<li class="chapter" data-level="17.2" data-path="writing-and-communication.html"><a href="writing-and-communication.html#final-review-webinar"><i class="fa fa-check"></i><b>17.2</b> Final Review Webinar</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="references-1.html"><a href="references-1.html"><i class="fa fa-check"></i><b>18</b> References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Exam PA Study Guide, Spring 2022</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="introduction-to-modeling" class="section level1" number="8">
<h1><span class="header-section-number"> 8</span> Introduction to modeling</h1>
<p>About 40-50% of the exam grade is based on modeling. The goal is to be able to predict an unknown quantity. In actuarial applications, this tends to claim that occur in the future, death, injury, accidents, policy lapse, hurricanes, or some other insurable event.</p>
<p>The next few chapters will cover the following learning objectives.</p>
<p><img src="images/learning_obj6.png" width="400%" style="display: block; margin: auto;" /></p>
<div id="modeling-vocabulary" class="section level2" number="8.1">
<h2><span class="header-section-number">8.1</span> Modeling vocabulary</h2>
<p>Modeling notation is sloppy because many words mean the same thing.</p>
<p>The number of observations will be denoted by <span class="math inline">\(n\)</span>. When we refer to the size of a data set, we are referring to <span class="math inline">\(n\)</span>. Each row of the data is called an observation or record. Observations tend to be people, cars, buildings, or other insurable things. These are always independent in that they do not influence one another. Because the Prometric computers have limited power, <span class="math inline">\(n\)</span> tends to be less than 100,000.</p>
<p>Each observation has known attributes called <em>variables</em>, <em>features</em>, or <em>predictors</em>. We use <span class="math inline">\(p\)</span> to refer the number of input variables that are used in the model.</p>
<p>The <em>target</em>, <em>response</em>, <em>label</em>, <em>dependent variable</em>, or <em>outcome</em> variable is the unknown quantity that is being predicted. We use <span class="math inline">\(Y\)</span> for this. This can be either a whole number, in which case we are performing regression, or a category, in which case we perform classification.</p>
<p>For example, say that you are a health insurance company that wants to set the premiums for a group of people. The premiums for people who are likely to incur high health costs need to be higher than those likely to be low-cost.</p>
<p>Older people tend to use more of their health benefits than younger people, but there are always exceptions for those who are very physically active and healthy. Those who have an unhealthy Body Mass Index (BMI) tend to have higher costs than those who have a healthy BMI, but this has less impact on younger people.</p>
<p>In short, we want to predict the future health costs of a person by taking into account many of their attributes at once.</p>
<p>This can be done in the <code>health_insurance</code> data by fitting a model to predict the annual health costs of a person. The target variable is <code>y = charges</code>, and the predictor variables are <code>age</code>, <code>sex</code>, <code>bmi</code>, <code>children</code>, <code>smoker</code> and <code>region</code>. These six variables mean that <span class="math inline">\(p = 6\)</span>. The data is collected from 1,338 patients, which means that <span class="math inline">\(n = 1,338\)</span>.</p>
</div>
<div id="modeling-notation" class="section level2" number="8.2">
<h2><span class="header-section-number">8.2</span> Modeling notation</h2>
<p>Scalar numbers are denoted by ordinary variables (i.e., <span class="math inline">\(x = 2\)</span>, <span class="math inline">\(z = 4\)</span>), and vectors are denoted by bold-faced letters</p>
<p><span class="math display">\[\mathbf{a} = \begin{pmatrix} a_1 \\ a_2 \\ a_3 \end{pmatrix}\]</span></p>
<p>We organize these variables into matrices. Take an example with <span class="math inline">\(p\)</span> = 2 columns and 3 observations. The matrix is said to be <span class="math inline">\(3 \times 2\)</span> (read as “3-by-2”) matrix.</p>
<p><span class="math display">\[
\mathbf{X} = \begin{pmatrix}x_{11} &amp; x_{21}\\
x_{21} &amp; x_{22}\\
x_{31} &amp; x_{32}
\end{pmatrix}
\]</span></p>
<p>In the health care costs example, <span class="math inline">\(y_1\)</span> would be the costs of the first patient, <span class="math inline">\(y_2\)</span> the costs of the second patient, and so forth. The variables <span class="math inline">\(x_{11}\)</span> and <span class="math inline">\(x_{12}\)</span> might represent the first patient’s age and sex respectively, where <span class="math inline">\(x_{i1}\)</span> is the patient’s age, and <span class="math inline">\(x_{i2} = 1\)</span> if the ith patient is male and 0 if female.</p>
<p>Modeling is about using <span class="math inline">\(X\)</span> to predict <span class="math inline">\(Y\)</span>. We call this “y-hat,” or simply the <em>prediction</em>. This is based on a function of the data <span class="math inline">\(X\)</span>.</p>
<p><span class="math display">\[\hat{Y} = f(X)\]</span></p>
<p>This is almost never going to happen perfectly, and so there is always an error term, <span class="math inline">\(\epsilon\)</span>. This can be made smaller, but is never exactly zero.</p>
<p><span class="math display">\[
\hat{Y} + \epsilon = f(X) + \epsilon
\]</span></p>
<p>In other words, <span class="math inline">\(\epsilon = y - \hat{y}\)</span>. We call this the <em>residual</em>. When we predict the health care costs of a person, this is the difference between the predicted costs (which we had created the year before) and the actual costs that the patient experienced (of that current year).</p>
<p>Another way of saying this is in terms of expected value: the model <span class="math inline">\(f(X)\)</span> estimates the expected value of the target <span class="math inline">\(E[Y|X]\)</span>. That is, once we condition on the data <span class="math inline">\(X\)</span>, we can make a guess as to what we expect <span class="math inline">\(Y\)</span> to be “close to.” We will see that there are many ways of measuring “closeness.”</p>
</div>
<div id="ordinary-least-squares-ols" class="section level2" number="8.3">
<h2><span class="header-section-number">8.3</span> Ordinary Least Squares (OLS)</h2>
<p>Also known as <em>simple linear regression</em>, OLS predicts the target as a weighted sum of the variables.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/PaFPbb66DxQ?rel=0&amp;showinfo=1&amp;playlist=nk2CQITm_eo,u1cc1r_Y7M0" frameborder="0" allowfullscreen>
</iframe>
<p></br></p>
<p>We find a <span class="math inline">\(\mathbf{\beta}\)</span> so that</p>
<p><span class="math display">\[
\hat{Y} = E[Y] =  \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_p X_p
\]</span></p>
<p>Each <span class="math inline">\(y_i\)</span> is a <em>linear combination</em> of <span class="math inline">\(x_{i1}, ..., x_{ip}\)</span>, plus a constant <span class="math inline">\(\beta_0\)</span> which is called the <em>intercept</em> term.</p>
<p>In the one-dimensional case, this creates a line connecting the observations. In higher dimensions, this creates a hyper-plane.</p>
<p><img src="Exam-PA-Study-Manual_files/figure-html/unnamed-chunk-102-1.png" width="576" /></p>
<p>The red line shows the <em>expected value</em> of the target, as the target <span class="math inline">\(\hat{Y}\)</span> is actually a random variable. For each observation, the model assumes a Gaussian distribution. If there is just a single predictor, <span class="math inline">\(x\)</span>, then the mean is <span class="math inline">\(\beta_0 + \beta_1 x\)</span>.</p>
<p><img src="images/conditional_response.jpg" width="200%" style="display: block; margin: auto;" /></p>
<p>The question then is <strong>how can we choose the best values of</strong> <span class="math inline">\(\beta?\)</span> First of all, we need to define what we mean by “best.” Ideally, we will choose these values which will create close predictions of <span class="math inline">\(Y\)</span> on new, unseen data.</p>
<p>To solve for <span class="math inline">\(\mathbf{\beta}\)</span>, we first need to define a <strong>loss function</strong>. This would let us compare how well a model fits the data. The most commonly used loss function is the residual sum of squares (RSS), also called the <strong>squared error loss or the L2 norm</strong>. When RSS is small, then the predictions are close to the actual values, and the model is a good fit. When RSS is large, the model is a poor fit.</p>
<p><span class="math display">\[\text{RSS} = \sum_i(y_i - \hat{y})^2\]</span></p>
<p>When you replace <span class="math inline">\(\hat{y_i}\)</span> in the above equation with <span class="math inline">\(\beta_0 + \beta_1 x_1 + ... + \beta_p x_p\)</span>, take the derivative with respect to <span class="math inline">\(\beta\)</span>, set equal to zero, and solve, we can find the optimal values. This turns the problem of statistics into a problem of numeric optimization, which computers can do quickly.</p>
<p>You will also see the term <strong>Root Mean Squared Error</strong> (RMSE) which is just the average of the square root of the <span class="math inline">\(\text{RSS}\)</span>, or just <strong>Mean Squared Error</strong> (MSE).</p>
<p>You might be wondering: why does this need to be the squared error? Why not the absolute error or the cubed error? Technically, these could be used, but the betas would not be the maximum likelihood parameters. Using the absolute error results in the model predicting the median as opposed to the mean. Two reasons behind the popularity of RSS are:</p>
<ul>
<li>It provides the same solution if we assume that the distribution of <span class="math inline">\(Y|X\)</span> is Gaussian and maximize the likelihood function. This method is used for GLMs, in the next chapter.</li>
<li>It is computationally easier, and computers used to have a difficult time optimizing for MAE.</li>
</ul>
<blockquote>
<p>What does it mean when a log transform is applied to <span class="math inline">\(Y\)</span>? I remember from my statistics course on regression that this was done.</p>
</blockquote>
<p>This is done so that the variance is closer to being constant. For example, if the units are in dollars, it is very common for the values to fluctuate more for higher values than for lower values. Other types of transformations can correct for skewness.</p>
<p>Consider a stock price, for instance. If the stock is $50 per share, it will go up or down less than $1000 per share. However, the log of 50 is about 3.9, and the log of 1000 is only 6.9, so this difference is smaller. In other words, the variance is smaller.</p>
<p>Transforming the target means that instead of the model predicting <span class="math inline">\(E[Y]\)</span>, it predicts <span class="math inline">\(E[log(Y)]\)</span>. A common mistake is to then the take the exponent in an attempt to “undo” this transform, but <span class="math inline">\(e^{E[log(Y)]}\)</span> is not the same as <span class="math inline">\(E[Y]\)</span>.</p>
</div>
<div id="r2-statistic" class="section level2" number="8.4">
<h2><span class="header-section-number">8.4</span> R^2 Statistic</h2>
<p>One of the most common ways of measuring model fit is the “R-Squared” statistic. The RSS provides an absolute measure of fit because it can be any positive value, but it is not always clear what a “good” RSS is because it is measured in units of <span class="math inline">\(Y\)</span>.</p>
<p>The <span class="math inline">\(R^2\)</span> statistic provides an alternative measure of fit. It takes the proportion of variance explained - so that it is always a value between 0 and 1 and is independent of the scale of <span class="math inline">\(Y\)</span>.</p>
<p><span class="math display">\[R^2 = \frac{\text{TSS} - \text{RSS}}{\text{TSS}} = 1 - \frac{\text{RSS}}{\text{TSS}}\]</span></p>
<p>Where <span class="math inline">\(\text{TSS} = \sum(y_i - \hat{y})^2\)</span> is the total sum of squares. TSS measures the total variance in the response YY and can be considered the amount of variability inherent in the response before the regression is performed. In contrast, RSS measures the amount of variability that is left unexplained after performing the regression.</p>
<p>Hench, <span class="math inline">\(\text{TSS} - \text{RSS}\)</span> measures the amount of variability in the response that is explained (or removed) by performing the regression, and R^2 measures the proportion of variability in <span class="math inline">\(Y\)</span> that can be explained using <span class="math inline">\(X\)</span>.</p>
<p>A value near 1 indicates that the regression has explained a large proportion of the variability in the response, whereas, a number near 0 indicates that the regression did not explain much of the variability in the response. This might occur because the linear model is wrong.</p>
<p>The <span class="math inline">\(R^2\)</span> sstatistic has an interpretational advantage over the RSE. In actuarial applications, it is useful to use an absolute measure of model fit, such as RSS, to train the model, and then use <span class="math inline">\(R^2\)</span> when explaining it to your clients so that it is easier to communicate.</p>
<p>This chapter was based on Chapter 3, <em>Linear Regression</em>, of <em>An Introduction to Statistical Learning.</em></p>
</div>
<div id="correlation" class="section level2" number="8.5">
<h2><span class="header-section-number">8.5</span> Correlation</h2>
<p><em>Correlation does not imply causation.</em></p>
<p>This is a common saying. Just because two things are correlated does not necessarily mean that one causes the other. Just because most actuaries work remotely when there is cold and snowing does not mean that cold and snow cause anti-social, introverted work habits. A more likely explanation is that actuaries are concerned about driving safely on icy roads and avoiding being involved in a car accident.</p>
<div id="pearsons-correlation" class="section level3" number="8.5.1">
<h3><span class="header-section-number">8.5.1</span> Pearson’s correlation</h3>
<p><strong>Pearson correlation:</strong> Measures a linear dependence between two variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. This is the most commonly used correlation method.</p>
<p>The <em>correlation</em> is defined by <span class="math inline">\(r\)</span>,</p>
<p><span class="math display">\[r = Cor(X,Y) = \frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum(x_i - \bar{x})^2}\sqrt{(y_i - \bar{y})^2}}\]</span></p>
<p>and this is also a measure of the linear relationship between two vectors, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. This suggests that we might be able to use <span class="math inline">\(r = Cor(X,Y)\)</span> instead of <span class="math inline">\(R^2\)</span> to assess the model fit. In the case of simple linear regression, where there is only one predictor variable, it is tree that <span class="math inline">\(R^2 = r^2\)</span>; however, this relationship does not extend automatically when there are more than one predictor variable. This is because <span class="math inline">\(X\)</span> becomes a <em>matrix</em> instead of a single <em>vector.</em></p>
</div>
<div id="spearman-rank-correlation" class="section level3" number="8.5.2">
<h3><span class="header-section-number">8.5.2</span> Spearman (rank) correlation</h3>
<p><strong>Spearman correlation:</strong> Computes the correlation between the rank of x and the rank of y variables.</p>
<p><span class="math display">\[rho = \frac{\sum(x&#39; - m_{x&#39;})(y&#39;_i - m_{y&#39;})}{\sqrt{\sum(x&#39; - m_{x&#39;})^2 \sum(y&#39; - m_{y&#39;})^2}}\]</span>
Where <span class="math inline">\(x′=rank(x)\)</span> and <span class="math inline">\(y′=rank(y)\)</span></p>
<p>Most questions on Exam PA will ask you about Pearson’s correlation. One advantage to Spearman over Pearson is that Spearman works for ordinal variables. See Chapter 6 for the difference between <strong>ordinal</strong> and <strong>numeric</strong> variables.</p>
</div>
</div>
<div id="regression-vs.-classification" class="section level2" number="8.6">
<h2><span class="header-section-number">8.6</span> Regression vs. classification</h2>
<p>Regression modeling is when a target is a number. Binary classification is when there are two outcomes, such as “Yes/No,” “True/False,” or “0/1.” Multi-class regression is when there are more than two categories such as “Red, Yellow, Green” or “A, B, C, D, E.” There are many other types of regression that are not covered on this exam, such as ordinal regression, where the outcome is an ordered category, or time-series regression, where the data is time-dependent.</p>
</div>
<div id="regression-metrics" class="section level2" number="8.7">
<h2><span class="header-section-number">8.7</span> Regression metrics</h2>
<p>For any model, the goal is always to reduce an error metric. This is a way of measuring how well the model can explain the target.</p>
<p>The phrases “reducing error,” “improving performance,” or “making a better fit” are synonymous with reducing the error. The word “better” means “lower error,” and “worse” means “higher error.”</p>
<p>The choice of error metric has a big difference in the outcome. When explaining a model to a businessperson, using simpler metrics such as R-Squared and Accuracy is convenient. When training the model, however, using a more nuanced metric is almost always better.</p>
<p>These are the regression metrics that are most likely to appear on Exam PA. Memorizing these formulas for AIC and BIC is unnecessary as they are in the R documentation by typing
<code>?AIC</code> or <code>?BIC</code> into the R console.</p>
<p><img src="images/regression_metrics.png" width="1300%" style="display: block; margin: auto;" /></p>
<p></br></p>
<div class="studytip">
<p>
Do not forget the most important metric: “usefulness!” A model with high predictive accuracy but does not meet the needs of the business problem has low usefulness. A model which is easy to explain to the PA exam graders has high usefulness.
</p>
<blockquote>
<p>
“Some candidates did not consider both predictive power and applicability to the business problem and others gave justifications based on one of these but then chose a model based on the other.” - SOA PA 6/18/20, Task 12 Solution
</p>
</blockquote>
<blockquote>
<p>
“Which of the three models would you recommend for this analysis? Do not base your recommendation solely on the mean squared errors (RMSE) from each model.” - SOA PA 6/13/19, Task 9 Project Statement
</p>
</blockquote>
</div>
<p></br></p>
<div id="example-soa-pa-61820-task-4" class="section level3" number="8.7.1">
<h3><span class="header-section-number">8.7.1</span> Example: SOA PA 6/18/20, Task 4</h3>
<blockquote>
<p>Three points of Investigate correlations.</p>
</blockquote>
<blockquote>
<ol style="list-style-type: decimal">
<li>Create a correlation coefficient matrix for all the numeric variables in the dataset.</li>
</ol>
</blockquote>
<blockquote>
<ol start="2" style="list-style-type: decimal">
<li>Among these pairwise correlations, determine which correlations concern you in building GLM and tree models. The response may differ by model.</li>
</ol>
</blockquote>
<blockquote>
<ol start="3" style="list-style-type: decimal">
<li>State a method other than principal components analysis (PCA) that can be used to handle the correlated variables. Do not implement this method.</li>
</ol>
</blockquote>
</div>
</div>
<div id="example-health-costs" class="section level2" number="8.8">
<h2><span class="header-section-number">8.8</span> Example: Health Costs</h2>
<p>In our health insurance data, we can predict the health costs of a person based on their age, body mass index and gender. Intuitively, we expect that these costs would increase with the increase in the age of the person, be different for men than for women, and be higher for those who have a less healthy BMI. We create a linear model using <code>bmi</code>, <code>age</code>, and <code>sex</code> as an inputs.</p>
<p>The <code>formula</code> controls which variables are included. There are a few shortcuts for using R formulas.</p>
<table>
<colgroup>
<col width="43%" />
<col width="56%" />
</colgroup>
<thead>
<tr class="header">
<th>Formula</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>charges</code> ~ <code>bmi</code> + <code>age</code></td>
<td>Use <code>age</code> and <code>bmi</code> to predict <code>charges</code></td>
</tr>
<tr class="even">
<td><code>charges</code> ~ <code>bmi</code> + <code>age</code> + <code>bmi</code>*<code>age</code></td>
<td>Use <code>age</code>,<code>bmi</code> as well as an interaction to predict <code>charges</code></td>
</tr>
<tr class="odd">
<td><code>charges</code> ~ (<code>bmi &gt; 20</code>) + <code>age</code></td>
<td>Use an indicator variable for <code>bmi &gt; 20</code> <code>age</code> to predict <code>charges</code></td>
</tr>
<tr class="even">
<td>log(<code>charges</code>) ~ log(<code>bmi</code>) + log(<code>age</code>)</td>
<td>Use the logs of <code>age</code> and <code>bmi</code> to predict log(<code>charges</code>)</td>
</tr>
<tr class="odd">
<td><code>charges</code> ~ .</td>
<td>Use all variables to predict <code>charges</code></td>
</tr>
</tbody>
</table>
<p>While you can use formulas to create new variables, the exam questions tend to have you do this in the data itself. For example, if taking the log transform of a <code>bmi</code>, you would add a column <code>log_bmi</code> to the data and remove the original <code>bmi</code> column.</p>
<p>Below we fit a simple linear model to predict charges.</p>
<div class="sourceCode" id="cb188"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb188-1"><a href="introduction-to-modeling.html#cb188-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ExamPAData)</span>
<span id="cb188-2"><a href="introduction-to-modeling.html#cb188-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb188-3"><a href="introduction-to-modeling.html#cb188-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb188-4"><a href="introduction-to-modeling.html#cb188-4" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">data =</span> health_insurance, <span class="at">formula =</span> charges <span class="sc">~</span> bmi <span class="sc">+</span> age <span class="sc">+</span> sex)</span></code></pre></div>
<p>The <code>summary</code> function gives details about the model. First, the <code>Estimate</code>, gives you the coefficients. The <code>Std. Error</code> is the error of the estimate for the coefficient. Higher standard error means greater uncertainty. This is relative to the average value of that variable. The <code>p value</code> tells you how “big” this error really is based on standard deviations. A small p-value (<code>Pr (&gt;|t|))</code>) means that we can safely reject the null hypothesis that says the coefficient is equal to zero.</p>
<p>The little <code>*</code>, <code>**</code>, <code>***</code> tell you the significance level. A variable with a <code>***</code> means that the probability of getting a coefficient of that size given that the data was randomly generated is less than 0.001. The <code>**</code> has a significance level of 0.01, and <code>*</code> of 0.05.</p>
<div class="sourceCode" id="cb189"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb189-1"><a href="introduction-to-modeling.html#cb189-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = charges ~ bmi + age + sex, data = health_insurance)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -14974  -7073  -5072   6953  47348 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -6986.82    1761.04  -3.967 7.65e-05 ***
## bmi           327.54      51.37   6.377 2.49e-10 ***
## age           243.19      22.28  10.917  &lt; 2e-16 ***
## sexmale      1344.46     622.66   2.159    0.031 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 11370 on 1334 degrees of freedom
## Multiple R-squared:  0.1203, Adjusted R-squared:  0.1183 
## F-statistic: 60.78 on 3 and 1334 DF,  p-value: &lt; 2.2e-16</code></pre>
<blockquote>
<p>For this exam, variable selection tends to be based on the 0.05 significance level (single star *).</p>
</blockquote>
<p>When evaluating model performance, you should not rely on the <code>summary</code> alone, based on the training data. To look at performance, test the model on validation data. This can be done by either using a hold-out set or cross-validation, which is even better.</p>
<p>Let us create an 80% training set and 20% testing set. You do not need to worry about understanding this code as the exam will always give this to you.</p>
<div class="sourceCode" id="cb191"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb191-1"><a href="introduction-to-modeling.html#cb191-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb191-2"><a href="introduction-to-modeling.html#cb191-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb191-3"><a href="introduction-to-modeling.html#cb191-3" aria-hidden="true" tabindex="-1"></a><span class="co">#create a train/test split</span></span>
<span id="cb191-4"><a href="introduction-to-modeling.html#cb191-4" aria-hidden="true" tabindex="-1"></a>index <span class="ot">&lt;-</span> <span class="fu">createDataPartition</span>(<span class="at">y =</span> health_insurance<span class="sc">$</span>charges, </span>
<span id="cb191-5"><a href="introduction-to-modeling.html#cb191-5" aria-hidden="true" tabindex="-1"></a>                             <span class="at">p =</span> <span class="fl">0.8</span>, <span class="at">list =</span> F) <span class="sc">%&gt;%</span> <span class="fu">as.numeric</span>()</span>
<span id="cb191-6"><a href="introduction-to-modeling.html#cb191-6" aria-hidden="true" tabindex="-1"></a>train <span class="ot">&lt;-</span>  health_insurance <span class="sc">%&gt;%</span> <span class="fu">slice</span>(index)</span>
<span id="cb191-7"><a href="introduction-to-modeling.html#cb191-7" aria-hidden="true" tabindex="-1"></a>test <span class="ot">&lt;-</span> health_insurance <span class="sc">%&gt;%</span> <span class="fu">slice</span>(<span class="sc">-</span>index)</span></code></pre></div>
<p>Train the model on the <code>train</code> and test on <code>test</code>.</p>
<div class="sourceCode" id="cb192"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb192-1"><a href="introduction-to-modeling.html#cb192-1" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">data =</span> train, <span class="at">formula =</span> charges <span class="sc">~</span> bmi <span class="sc">+</span> age)</span>
<span id="cb192-2"><a href="introduction-to-modeling.html#cb192-2" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">=</span> <span class="fu">predict</span>(model, test)</span></code></pre></div>
<p>Let’s look at the Root Mean Squared Error (RMSE).</p>
<div class="sourceCode" id="cb193"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb193-1"><a href="introduction-to-modeling.html#cb193-1" aria-hidden="true" tabindex="-1"></a>get_rmse <span class="ot">&lt;-</span> <span class="cf">function</span>(y, y_hat){</span>
<span id="cb193-2"><a href="introduction-to-modeling.html#cb193-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sqrt</span>(<span class="fu">mean</span>((y <span class="sc">-</span> y_hat)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb193-3"><a href="introduction-to-modeling.html#cb193-3" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb193-4"><a href="introduction-to-modeling.html#cb193-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-5"><a href="introduction-to-modeling.html#cb193-5" aria-hidden="true" tabindex="-1"></a><span class="fu">get_rmse</span>(pred, test<span class="sc">$</span>charges)</span></code></pre></div>
<pre><code>## [1] 11421.96</code></pre>
<p>And the Mean Absolute Error as well.</p>
<div class="sourceCode" id="cb195"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb195-1"><a href="introduction-to-modeling.html#cb195-1" aria-hidden="true" tabindex="-1"></a>get_mae <span class="ot">&lt;-</span> <span class="cf">function</span>(y, y_hat){</span>
<span id="cb195-2"><a href="introduction-to-modeling.html#cb195-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sqrt</span>(<span class="fu">mean</span>(<span class="fu">abs</span>(y <span class="sc">-</span> y_hat)))</span>
<span id="cb195-3"><a href="introduction-to-modeling.html#cb195-3" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb195-4"><a href="introduction-to-modeling.html#cb195-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb195-5"><a href="introduction-to-modeling.html#cb195-5" aria-hidden="true" tabindex="-1"></a><span class="fu">get_mae</span>(pred, test<span class="sc">$</span>charges)</span></code></pre></div>
<pre><code>## [1] 94.32336</code></pre>
<p>The above metrics do not tell us if this is a good model or not by itself. We need a comparison. The fastest check is to compare against a prediction of the mean. In other words, all values of the <code>y_hat</code> are the average of <code>charges</code>, which is about $13,000.</p>
<div class="sourceCode" id="cb197"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb197-1"><a href="introduction-to-modeling.html#cb197-1" aria-hidden="true" tabindex="-1"></a><span class="fu">get_rmse</span>(<span class="fu">mean</span>(test<span class="sc">$</span>charges), test<span class="sc">$</span>charges)</span></code></pre></div>
<pre><code>## [1] 12574.97</code></pre>
<div class="sourceCode" id="cb199"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb199-1"><a href="introduction-to-modeling.html#cb199-1" aria-hidden="true" tabindex="-1"></a><span class="fu">get_mae</span>(<span class="fu">mean</span>(test<span class="sc">$</span>charges), test<span class="sc">$</span>charges)</span></code></pre></div>
<pre><code>## [1] 96.63604</code></pre>
<p>The RMSE and MAE are both higher (worse) when using just the mean, which we expect. If you ever fit a model and get an error that is worse than the average prediction, something must be wrong.</p>
<p>The next test is to see if any assumptions have been violated.</p>
<p>First, is there a pattern in the residuals? If there is, this means that the model is missing key information.</p>
<p>For the model below, this is a yes, which means that this is a bad model. Because this is just for illustration, we are going to continue using it.</p>
<div class="sourceCode" id="cb201"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb201-1"><a href="introduction-to-modeling.html#cb201-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(model, <span class="at">which =</span> <span class="dv">1</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-113"></span>
<img src="Exam-PA-Study-Manual_files/figure-html/unnamed-chunk-113-1.png" alt="Residuals vs. Fitted" width="576" />
<p class="caption">
Figure 8.1: Residuals vs. Fitted
</p>
</div>
<p>The normal QQ shows how well the quantiles of the predictions fit a theoretical normal distribution. If this is true, then the graph is a straight 45-degree line. In this model, you can see that this is not the case. If this were a good model, this distribution would be closer to normal.</p>
<div class="sourceCode" id="cb202"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb202-1"><a href="introduction-to-modeling.html#cb202-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(model, <span class="at">which =</span> <span class="dv">2</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-114"></span>
<img src="Exam-PA-Study-Manual_files/figure-html/unnamed-chunk-114-1.png" alt="Normal Q-Q" width="576" />
<p class="caption">
Figure 8.2: Normal Q-Q
</p>
</div>
<p>Once you have chosen your model, you should re-train over the entire data set. This is to make the coefficients more stable because <code>n</code> is larger. Below you can see that the standard error is lower after training over the entire data set.</p>
<div class="sourceCode" id="cb203"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb203-1"><a href="introduction-to-modeling.html#cb203-1" aria-hidden="true" tabindex="-1"></a>all_data <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">data =</span> health_insurance, </span>
<span id="cb203-2"><a href="introduction-to-modeling.html#cb203-2" aria-hidden="true" tabindex="-1"></a>               <span class="at">formula =</span> charges <span class="sc">~</span> bmi <span class="sc">+</span> age)</span>
<span id="cb203-3"><a href="introduction-to-modeling.html#cb203-3" aria-hidden="true" tabindex="-1"></a>testing <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">data =</span> test, </span>
<span id="cb203-4"><a href="introduction-to-modeling.html#cb203-4" aria-hidden="true" tabindex="-1"></a>              <span class="at">formula =</span> charges <span class="sc">~</span> bmi <span class="sc">+</span> age)</span></code></pre></div>
<table>
<thead>
<tr class="header">
<th align="left">term</th>
<th align="right">full_data_std_error</th>
<th align="right">test_data_std_error</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">(Intercept)</td>
<td align="right">1744.1</td>
<td align="right">3824.2</td>
</tr>
<tr class="even">
<td align="left">bmi</td>
<td align="right">51.4</td>
<td align="right">111.1</td>
</tr>
<tr class="odd">
<td align="left">age</td>
<td align="right">22.3</td>
<td align="right">47.8</td>
</tr>
</tbody>
</table>
<p>All interpretations should be based on the model which was trained on the entire data set. This only makes a difference if you are interpreting the precise values of the coefficients. If you are just looking at which variables are included or at the size and sign of the coefficients, then this would probably not make a difference.</p>
<div class="sourceCode" id="cb204"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb204-1"><a href="introduction-to-modeling.html#cb204-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coefficients</span>(model)</span></code></pre></div>
<pre><code>## (Intercept)         bmi         age 
##  -4526.5284    286.8283    228.4372</code></pre>
<p>Translating the above into an equation we have</p>
<p><span class="math display">\[\hat{y_i} = -4,526 + 287 \space\text{bmi} + 228\space \text{age}\]</span></p>
<p>For example, if a patient has <code>bmi = 27.9</code> and <code>age = 19</code> then predicted value is</p>
<p><span class="math display">\[\hat{y_1} = 4,526 + (287)(27.9) + (228)(19) = 16,865\]</span></p>
<p>This model structure implies that each of the variables <span class="math inline">\(x_1, ..., x_p\)</span> each change the predicted <span class="math inline">\(\hat{y}\)</span>. If <span class="math inline">\(x_{ij}\)</span> increases by one unit, then <span class="math inline">\(y_i\)</span> increases by <span class="math inline">\(\beta_j\)</span> units, regardless of what happens to all of the other variables. This is one of the main assumptions of linear models: <em>variable independence</em>. If the variables are correlated, say, then this assumption will be violated.</p>
<table>
<thead>
<tr class="header">
<th>Readings</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ISLR 2.1 What is statistical learning?</td>
<td></td>
</tr>
<tr class="even">
<td>ISLR 2.2 Assessing model accuracy</td>
<td></td>
</tr>
</tbody>
</table>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="data-exploration.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="generalized-linear-models-glms.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/sdcastillo/PA-R-Study-Manual/edit/master/04-linear-models.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Exam-PA-Study-Manual.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
