--- 
title: "Exam PA Study Guide, Spring 2021"
github-repo: 
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
classoption: openany
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
favicon: images/artificial_actuary_logo_favicon.png
---

# Welcome {-}

<hr>

<img src="https://raw.githubusercontent.com/sdcastillo/ExamPAContent/master/Exam PA Study Guide.png" width="250" height="340" alt="Cover image" align="right" style="margin: 0 1em 0 1em" />This is the study guide for [ExamPA.net](https://exampa.net), the online course for the Predictive Analytics exam of SOA. While meeting all of the learning requirements of Exam PA, this 250-page study guide gives data science and machine learning training. It would be taught how to get data into R, clean it, visualize it, and use models to derive business value. Just as scientists set up lab experiments to form and test hypotheses, it would be required to build models and test them on holdout sets.

The chapters on R-programming cover the foundational concepts with a focus on modern data science applications. Time-saving coding tips along with ways of checking answers within RStudio will be taught.

All of the statistical theory is explained, from linear regression to gradient boosted trees, and examples are provided of each model that can be reproduced. Following the course materials “An Introduction to Statistical Learning, “we discuss model training, validation, and the advantages and disadvantages of each algorithm.


<iframe width="560" height="315" src="https://www.youtube.com/embed/F2okL4a2YcM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

</br>

```{block, type='rcorners'}
<font size="10">**[Join the Online Course](https://www.exampa.net/)**</font>
```

</br>

*We are thankful to all of the reviewers, guest editors, and past exam-takers who have helped to improve this book.  Thanks to the following persons who made changes to this book and its past versions: David Hill, Erlan Wheeler, Caden Collier, Peter Shelbe, Abhinav Gadde, Allen Meriken, [Kevin Kuo](https://github.com/kevinykuo), Aamir Ali, Matthew Caseras, and Liu Chang.*

## FAQ: Frequently Asked Questions

**Does this replace the modules?**  No.  You can use this study guide to *supplement* rather than replace the modules of SOA. The online course is comprehensive and completely replaces the modules and textbooks. We strive to make this study guide as thorough as possible, but a passive-learning textbook format is not as efficient as giving hands-on practice questions under exam conditions and video tutorials of how to arrive at each answer. After all, our goal is to reduce study time as much as possible. The online course provides everything that one would need to pass.

**Why is this free when other study manuals cost hundreds of dollars?** We are making this available to everyone to study for PA and try out our study materials. We are so confident of the benefits from our hands-on teaching style that everyone will want to join our online course after getting a taste of this textbook.  

**How many study hours are needed?**  [Here is a study template](https://exampa.net/study-schedule/) that allocates 160 hours for this course and the modules of SOA. The exact number of hours needed varies by individual. Is English your native language? Have you taken SRM? Do you have prior R experience? Are you a confident communicator? All of these factors play a role. Planning a study schedule for PA is not different from any of the other exams. Important deadlines, travel plans, or other obligations must be accommodated.  

# The exam

The main challenge  of this exam is in communication: both understanding what they want you to do as well as telling the grader what it is that you did.

The main challenge of this exam is in communication: both understanding what they expect and telling the grader what was done.

You will have 5 hours and 15 minutes to use RStudio and Excel to fill out a report in Word on a Prometric computer. The syllabus uses fancy language to describe the topics covered on the exam, making it sound more complicated than it should be. A good analogy is a job description that has many complex-sounding tasks when in reality, the day-to-day operations of the employees are far more straightforward.


<iframe width="560" height="315" src="https://www.youtube.com/embed/LbJIFJH1m2g" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

A non-technical translation is as follows:

**Writing in Microsoft Word (30-40%)**

- Write in professional language
- Type more than 50 words-per-minute

**Manipulating Data in R (15-25%)**

- Quickly clean data sets
- Find data errors planted by the SOA
- Perform queries (aggregations, summaries, transformations)

**Machine learning and statistics (40-50%)**

- Interpret results within a business context
- Change model parameters

Follow the pages of SOA for the latest updates

https://www.soa.org/education/exam-req/edu-exam-pa-detail/

The exam pass rates are about 50%.

http://www.actuarial-lookup.com/exams/pa

# Prometric Demo

The following video from Prometric shows what the computer setup will look like. You will have 9 minutes to complete the tutorial before starting the 5-hours and 15-minute timer and then 5 minutes at the end of the exam to upload your files.

<iframe width="560" height="315" src="https://www.youtube.com/embed/TBATFFsHa7E" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

In the past, candidates have been given out a printed Project Statement, which they were allowed to read during the 15-minute tutorial before the timer started; however, as of [Fall 2020](https://www.soa.org/globalassets/assets/files/edu/2020/fall/soa-exams-additional-details.pdf), this may no longer be the case, and all files will be given to you on the computer. This announcement by the SOA may be referring to only those exams which used to be on paper but are transferring to computer-based testing, in which case PA candidates would still get the printed statement. It may be best to practice assuming that you will need to work on a single monitor. We will update this once we confirm with the SOA.

</br>

```{block, type='studytip'}
**Are you a lefty?** Prometric computers will be set up with the mouse on the right side. Arrive a few minutes early and request the change. Many lefties also switch the configuration so that the right button, which is hit with the index finger, is a normal click. The left button, hit with the middle finger, is what you think of as a “right-click.” Unfortunately, if you use this configuration, then you will need to suffer through the exam anyways. Just be sure to practice with the same setup which you will use.
```

</br>

```{block, type='studytip'}
**Warning:** Be careful to submit the correct version of your RMD file! If you upload the wrong version of the Prometric file, you will be graded unfairly, and there is no way to recover your lost file. The SOA will only confirm that they received your files. You can email them after your exam (before the grading starts) to confirm that they received your files.
```

</br>

# Introduction

[![Lecture](images/lesson-whatispa.png)](https://exampa.net/)


While “machine learning” is relatively new, the process of learning itself is not. All of us are already familiar with how to learn - by improving from our mistakes. By repeating what is successful and avoiding what fails, we learn by doing, by experience, or trial-and-error. Machines learn similarly.

Take, for example, the process of studying for an exam. Some study methods work well, but other methods do not. The “data” is the practice problem, and the “label” is the answer (A, B, C, D, E). We want to build a mental “model” that reads the question and predicts the answer.

We all know that memorizing answers without understanding concepts is ineffective, and statistics calls this “overfitting.” Conversely, not learning enough of the details and only learning the high-level concepts is “underfitting.”

The more practice problems we do, the larger the training data set, and the better the prediction. When we see new problems that have not appeared in the practice exams, we often have difficulty. Quizzing ourselves on real questions estimates our preparedness, which is identical to a process known as “holdout testing” or “cross-validation.”

We can clearly state our objective: get as many correct answers as possible! We want to predict the solution to every problem correctly. Said another way, we are trying to minimize the error, known as the “loss function.”

Different study methods work well for different people. Some cover material quickly, and others slowly absorb every detail. A model has many “parameters,” such as the “learning rate.” The only way to know which parameters are best is to test them on real data, known as “training.”

<!--chapter:end:index.Rmd-->

---
output:
  html_document: default
  pdf_document: default
---
# Getting started 

This chapter tells you how to install the necessary software on your computer.

## Installing R

To begin, you will need to install R. This is the engine that runs the code. You need to install the newest version **(v4)** as well as an older version **(v3.6.2)**. 

The older version will be used when running the code from the modules of SOA, which needs to be consistent with what will be installed on the Prometric computers. This course uses the newest version so that you will not have as many errors during practice.

Download it from here: https://cran.r-project.org/mirrors.html

You can easily switch between both of them after installation.

```{r echo = F, fig.align="center", warning=F, out.width="50%"}
knitr::include_graphics("images/rversion1.png")
```


```{r echo = F, fig.align="center", warning=F, out.width="50%"}
knitr::include_graphics("images/rversion2.png")
```

Remember: 

For this course, use a version later than 4.0.2.

For the module of SOA, use version 3.6.2.

## Installing RStudio

Just as MS Word creates documents, RStudio creates R scripts and other documents. This is the tool that helps you to *write* the code. Download the free edition of RStudio Desktop and instal it at your selected location.

Download it from here: https://rstudio.com/products/rstudio/download/

Next you need to set the R library. R code is organized into libraries. You want to use the same code that will be on the Prometric Computers. It would require installing older versions of libraries. Change your R library to the one which was included within the Modules of SOA.

```{r eval = F}
.libPaths("PATH_TO_SOAS_LIBRARY/PAlibrary")
```

## Download the data

For your convenience, all data in this book, including data from prior exams and sample solutions, has been put into a library called `ExamPAData` which is available on CRAN.

```{r eval = F}
install.packages("ExamPAData")
library(ExamPAData)
```

To get the data dictionary for a data set, use `?dataset_name` such as `?customer_phone_calls`. For your convenience, you can use these data sets for your practice. They make great candidates for machine learning problems.

| Dataset   |      Description      |
|----------|-------------|
|`customer_phone_calls`| Data used on June 18, 2020 Exam PA |
|`patient_length_of_stay`| Data used on June 16, 2020 Exam PA|
|`patient_num_labs`| Data used on June 19, 2020 Exam PA |
|`actuary_salaries`| DW Simpson actuarial salary data|
|`june_pa`|Auto crash data set from SOA June 2019 PA|
|`customer_value`| Customer value data set from December 2019 PA|
|`exam_pa_titanic`| Titanic passengers as used in ExamPA.net's practice exam |
|`apartment_apps`| Apartment applications as used in ExamPA.net's Practice Exam|
|`health_insurance`| Health insurance claims as used in ExamPA.net's Practice Exam|
|`student_success`| SOA Student Success PA Sample Project, 2019|
|`readmission`| SOA Hospital Readmissions Sample Exam, 2019 |
|`auto_claim`| Automotive claims |
|`boston`| Boston housing data set |
|`bank_loans`| Credit data from UCI Machine Learning Repository |


## Download ISLR

This book references the publically-available textbook “An Introduction to Statistical Learning,” which can be downloaded for free

http://faculty.marshall.usc.edu/gareth-james/ISL/

If you already have R and RStudio installed then skip to "Download the data". 

<!--chapter:end:01-getting-started.Rmd-->

# How much R do I need to know to pass?

<span style="color: blue;">**This is a communication exam **</span>  because 30-40% of the points are based on your writing and data storytelling quality.

You do not need to become an R expert for this exam. While you are expected to develop a basic familiarity with R, the .Rmd template will provide you with the vast majority of the R commands needed. You will need to practice taking code templates and adjusting them to the specific variables and formulas you need. The most difficult coding questions will be during data exploration. These will ask you to

- Use basic mathematical operators and functions such as `exp()` and `log()`
- Select, modify, and summarize data in a dataframe
- Display data from a dataframe in common types of plots, using ggplot2

When fitting predictive models, you will also need to

- Modify or add a formula or other parameters to model-fitting functions like `glm()` and
`rpart()`
- Extract and displaying results from a fitted predictive model

You are **not** expected to construct loops, write functions, or use other programmatic techniques with R. The scope is limited to single-line commands.

You will have two cheat sheets for [data visualization](https://contentpreview.s3.us-east-2.amazonaws.com/Exam+PA+Spring+2021+Simplified+Data+Viz+Cheat+Sheet.pdf) and base R. You can use this in your study to become familiar with how to find the R code quickly. These cheat sheets that the SOA gives you were designed by the RStudio team for everyone who uses R, and so we have gone through and removed the parts that you will not need to learn. For example, no one will be making new types of ggplot graphs under the “graphical primitives” section, which has been blocked out. **Enroll in either of our online courses to get our simplified Base R cheat sheet and tutorial.**

## How to use the PA R cheat sheets?

<iframe width="560" height="315" src="https://www.youtube.com/embed/UVE9Zm6mqvM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Be observant and expect to spend twice as long explaining what your code is doing as you write the code itself. A few observations are based on the organization your .Rmd file, although you do not need this to read like an essay. The vast majority of your time will be spent on your Word document.

The June 16 2020 Project Statement has this under "General information for candidates."

>Each task will be graded on the quality of your thought process, **added or modified code**, and conclusions

>At a minimum, you must submit your completed report template and an .Rmd file that supports your work. Graders expect that your .Rmd code can be run from beginning to end. The code snippets provided should either be commented out or adapted for execution. Ensure that it is clear where in the code each of the tasks is addressed. 

In other words, the results of your report must be consistent with what the grading team finds when they run your .Rmd file.

## Example: SOA PA 6/16/20, Task 8

This question is from the June 16, 2020 exam. You can see that significantly only minor code changes need to be made. The remainder of this question consists of a short-answer response. This is very typical of Exam PA.  

<iframe src="https://player.vimeo.com/video/467866045?title=0&byline=0&portrait=0" width="640" height="360" frameborder="0" allow="autoplay; fullscreen" allowfullscreen></iframe>

Already enrolled?  Watch the full video: <a target="_parent" href="https://course.exampa.net/mod/page/view.php?id=210&forceview=1
">Practice Exams</a> | <a target="_parent" href="https://course.exampa.net/mod/page/view.php?id=161
">Practice Exams + Lessons</a>

>8. (4 obserations) Perform feature selection with lasso regression.
>
>Run a lasso regression using the code chunk provided. **The code will need to be modified to
reflect your decision in Task 7 regarding the PCA variable.**

You probably read this and asked "what is a lasso regression?" and with good reason - we haven't yet covered this topic.  All that you need to know is **highlighted in black:** you will need to change the code that they give you, which is below.

You need to choose between using one of two data sets:

- DATA SET A
- DATA SET B

Then ignore everything else!

```{r eval = F , echo = T}
# Format data as matrices (necessary for glmnet). 
# Uncomment two items that reflect your decision from Task 7.

#DATA SET A
lasso.mat.train <- model.matrix(days ~ . - PC1, data.train)
lasso.mat.test <- model.matrix(days ~ . - PC1, data.test)

#DATA SET B
# lasso.mat.train <- model.matrix(days ~ . - num_procs - num_meds - num_ip - num_diags, data.train)
# lasso.mat.test <- model.matrix(days ~ . - num_procs - num_meds - num_ip - num_diags, data.test)

set.seed(789)

lasso.cv <- cv.glmnet(
  x = lasso.mat.train,
  y = data.train$days,
  family = "poisson", # Do not change.
  alpha = 1 # alpha = 1 for lasso
)
```


If you wanted to use data set B, you would just add comments to data set A and uncomment B.

```{r eval = F , echo = T}
#DATA SET A
# lasso.mat.train <- model.matrix(days ~ . - PC1, data.train)
# lasso.mat.test <- model.matrix(days ~ . - PC1, data.test)

#DATA SET B
lasso.mat.train <- model.matrix(days ~ . - num_procs - num_meds - num_ip - num_diags, data.train)
lasso.mat.test <- model.matrix(days ~ . - num_procs - num_meds - num_ip - num_diags, data.test)

```

## Example 2 - Data exploration

That last example was easy. They might ask you to do something like the following:

**Template code: **

```{r eval = F, echo = T}
# This code takes a continuous variable and creates a binned factor variable. 
# The code applies it directly to the capital gain variable as an 
# example. right = FALSE means that the left number is included and 
# the right number excluded. So, in this case, the first bin runs from 0 to 
# 1000 and includes 0 and excludes 1000. Note that the code creates a new 
# variable, so the original variable is retained.
df$cap_gain_cut <- cut(df$cap_gain, breaks = c(0, 1000, 5000, Inf), right = FALSE, labels = c("lowcg", "mediumcg", "highcg"))
```

To answer this question correctly, you would need to 

- Understand that the code is taking the capital gains recorded on investments, `cap_gain`, and then creating bins so that the new variable is "lowcg" for values between 0 and 1000, "mediumcp" from 1000 to 5000, and "highcg" for all values above 5000.  
- Then you would need to interpret a statistical model
- Finally, use this result to change these cutoff values so that "low cg" is all values less than 5095.5, "medium cg" is all values from 5095.5 to 7055.5, and so forth.  You would need to do this for two data sets, `data.train`, and `data.test`.

**Solution code: **

```{r eval = F}
# This code cuts a continuous variable into buckets. 
# The process is applied to both the training and test sets. 

data.train$cap_gain_cut <- cut(data.train$cap_gain, breaks = c(0, 5095.5, 7055.5, Inf), right = FALSE, labels = c("lowcg", "mediumcg", "highcg"))

data.test$cap_gain_cut <- cut(data.test$cap_gain, breaks = c(0, 5095.5, 7055.5, Inf), right = FALSE, labels = c("lowcg", "mediumcg", "highcg"))
```

Do not panic if all of this code is confusing. Just focus on reading the comments. As you can see, this is less of a programming question than it is a “logic and reasoning” question.  

# R programming

This chapter teaches you the R skills that are needed to pass PA. 

<iframe src="https://player.vimeo.com/video/487660455" width="640" height="360" frameborder="0" allow="autoplay; fullscreen" allowfullscreen></iframe>

## Notebook chunks

On the Exam, you will start with an .Rmd (R Markdown) template, which organize 
code into [R Notebooks](https://bookdown.org/yihui/rmarkdown/notebook.html). 
Within each notebook, code is organized into chunks.  

```{r}
# This is a chunk
```

Your time is valuable.  Throughout this book, I will include useful keyboard shortcuts.

</br>

```{block, type='studytip'}
**Shortcut:** To run everything in a chunk quickly, press `CTRL + SHIFT + ENTER`. 
To create a new chunk, use `CTRL + ALT + I`.
```
</br>

## Basic operations

The usual math operations apply.

```{r}
# Addition
1 + 2 
3 - 2

# Multiplication
2 * 2

# Division
4 / 2

# Exponentiation
2^3
```

There are two assignment operators: `=` and `<-`.  The latter is preferred because 
it is specific to assigning a variable to a value.  The `=` operator is also used 
for specifying arguments in functions (see the functions section).  

</br>
```{block, type='studytip'}
**Shortcut:** `ALT + -` creates a `<-`..
```
</br>

```{r}
# Variable assignment
y <- 2

# Equality
4 == 2
5 == 5
3.14 > 3
3.14 >= 3
```

Vectors can be added just like numbers.  The `c` stands for "concatenate", which
creates vectors.

```{r}
x <- c(1, 2)
y <- c(3, 4)
x + y
x * y

z <- x + y
z^2
z / 2
z + 3
```

I already mentioned `numeric` types. There are also `character` (string) types, 
`factor` types, and `boolean` types.

```{r}
character <- "The"
character_vector <- c("The", "Quick")
```

Character vectors can be combined with the `paste()` function.

```{r}
a <- "The"
b <- "Quick"
c <- "Brown"
d <- "Fox"
paste(a, b, c, d)
```

Factors look like character vectors but can only contain a finite number of predefined 
values.

The below factor has only one "level", which is the list of assigned values.

```{r}
factor <- as.factor(character)
levels(factor)
```

The levels of a factor are by default in R in alphabetical order (Q comes alphabetically 
before T).

```{r}
factor_vector <- as.factor(character_vector)
levels(factor_vector)
```

**In building linear models, the order of the factors matters.**  In GLMs, the 
"reference level" or "base level" should always be the level which has the most
observations.  This will be covered in the section on linear models.

Booleans are just `TRUE` and `FALSE` values.  R understands `T` or `TRUE` in the 
same way, but the latter is preferred.  When doing math, bools are converted to 
0/1 values where 1 is equivalent to TRUE and 0 FALSE.

```{r}
bool_true <- TRUE
bool_false <- FALSE
bool_true * bool_false
```

Booleans are automatically converted into 0/1 values when there is a math operation.

```{r}
bool_true + 1
```

Vectors work in the same way.

```{r}
bool_vect <- c(TRUE, TRUE, FALSE)
sum(bool_vect)
```

Vectors are indexed using `[`. If you are only extracting a single element, you
should use `[[` for clarity.

```{r}
abc <- c("a", "b", "c")
abc[[1]]
abc[[2]]
abc[c(1, 3)]
abc[c(1, 2)]
abc[-2]
abc[-c(2, 3)]
```


## Lists

Lists are vectors that can hold mixed object types.

```{r}
my_list <- list(TRUE, "Character", 3.14)
my_list
```

Lists can be named.

```{r}
my_list <- list(bool = TRUE, character = "character", numeric = 3.14)
my_list
```

The `$` operator indexes lists.

```{r}
my_list$numeric

my_list$numeric + 5
```

Lists can also be indexed using `[[`.

```{r}
my_list[[1]]
my_list[[2]]
```


Lists can contain vectors, other lists, and any other object.

```{r}
everything <- list(vector = c(1, 2, 3), 
                   character = c("a", "b", "c"), 
                   list = my_list)
everything
```

To find out the type of an object, use `class` or `str` or `summary`.

```{r}
class(x)
class(everything)
str(everything)
summary(everything)
```


## Functions

You only need to understand the very basics of functions. The big picture is that understanding functions help you to understand  *everything* in R, since R is a 
functional [programming language](http://adv-r.had.co.nz/Functional-programming.html), 
unlike Python, C, VBA, Java, all object-oriented, or SQL, which is not a language but a series of set-operations.

Functions do things.  The convention is to name a function as a verb.  The function `make_rainbows()` would create a rainbow.  The function `summarise_vectors()` would summarise vectors.  Functions may or may not have an input and output.  

If you need to do something in R, there is a high probability that someone has already written a function to do it. That being said, creating simple functions is quite helpful.

Here is an example that has a side effect of printing the input:

```{r}
greet_me <- function(my_name){
  print(paste0("Hello, ", my_name))
}

greet_me("Future Actuary")
```

**A function that returns something**

When returning the last evaluated expression, the `return` statement is optional. In fact, it is discouraged by convention.

```{r}
add_together <- function(x, y) {
  x + y
}

add_together(2, 5)

add_together <- function(x, y) {
  # Works, but bad practice
  return(x + y)
}

add_together(2, 5)
```

Binary operations in R are vectorized. In other words, they are applied element-wise.

```{r}
x_vector <- c(1, 2, 3)
y_vector <- c(4, 5, 6)
add_together(x_vector, y_vector)
```

Many functions in R actually return lists!  This is why R objects can be indexed 
with dollar sign.

```{r}
library(ExamPAData)
model <- lm(charges ~ age, data = health_insurance)
model$coefficients
```

Here's a function that returns a list.

```{r}
sum_multiply <- function(x,y) {
  sum <- x + y
  product <- x * y
  list("Sum" = sum, "Product" = product)
}

result <- sum_multiply(2, 3)
result$Sum
result$Product
```

## Data frames

You can think of a data frame as a table that is implemented as a list of vectors.

```{r}
df <- data.frame(
  age = c(25, 35),
  has_fsa = c(FALSE, TRUE)
)
df
```

You can also work with tibbles, which are data frames but have nicer printing:

```{r}
# The tidyverse library has functions for making tibbles
library(tidyverse) 
df <- tibble(
  age = c(25, 35), has_fsa = c(FALSE, TRUE)
)
df
```

To index columns in a tibble, the same "$" is used as indexing a list.

```{r}
df$age
```

To find the number of rows and columns, use `dim`.

```{r}
dim(df)
```

To find a summary, use `summary`

```{r}
summary(df)
```

## Pipes

The pipe operator `%>%` is a way of making code *modular*, meaning that it can be written and executed in incremental steps. Those familiar with Python's Pandaswill be see that `%>%` is quite similar to ".".  This also makes code easier to 
read.

In five seconds, tell me what the below code is doing.

```{r}
log(sqrt(exp(log2(sqrt((max(c(3, 4, 16))))))))
```

Getting to the answer of 1 requires starting from the inner-most nested brackets and moving outwards from right to left.

The math notation would be slightly easier to read, but still painful.

$$log(\sqrt{e^{log_2(\sqrt{max(3,4,16)})}})$$

Here is the same algebra using the pipe.  To read this, replace the `%>%` with 
the word `THEN`.

```{r message = F, warning = F}

max(c(3, 4, 16)) %>% 
  sqrt() %>% 
  log2() %>% 
  exp() %>% 
  sqrt() %>% 
  log()

# max(c(3, 4, 16) THEN  # The max of 3, 4, and 16 is 16
#  sqrt() THEN          # The square root of 16 is 4
#  log2() THEN          # The log in base 2 of 4 is 2
#  exp() THEN           # The exponent of 2 is e^2
#  sqrt() THEN          # The square root of e^2 is e
#  log()                # The natural logarithm of e is 1

```

Pipes are exceptionally useful for data manipulations, which is covered in the next chapter.

</br>
```{block, type='studytip'}
**Shortcut:** To quickly produce pipes, use `CTRL + SHIFT + M`. 
```
</br>

By highlighting only certain sections, we can run the code in steps as if we were using a debugger. This makes testing out code much faster.

```{r}
max(c(3, 4, 16))
```

```{r}
max(c(3, 4, 16)) %>% 
  sqrt() 
```

```{r}
max(c(3, 4, 16)) %>% 
  sqrt() %>% 
  log2() 
```

```{r}
max(c(3, 4, 16)) %>% 
  sqrt() %>% 
  log2() %>% 
  exp()
```

```{r}
max(c(3, 4, 16)) %>% 
  sqrt() %>% 
  log2() %>% 
  exp() %>% 
  sqrt() 
```

```{r}
max(c(3, 4, 16)) %>% 
  sqrt() %>% 
  log2() %>% 
  exp() %>% 
  sqrt() %>% 
  log()
```

## The code of SOA does not use pipes or dplyr, so can I skip learning this?

Yes, if you really want to.  

The advantages to learning pipes, and the reason why this manual uses them are

1) It saves you time. 
2) It will help you in real life data science projects.
3) The majority of the R community uses this style.
4) The SOA actuaries who create the Exam PA content will eventually catch on.
5) 5.	Most modern R software is designed around them. The overall trend is towards greater adoption, as seen from the CRAN download statistics [here](https://hadley.shinyapps.io/cran-downloads/) after filtering to "magrittr" which is the library where the pipe comes from. 


<!--chapter:end:02-r-programming.Rmd-->

# Data exploration

```{r warning = F, echo = F, message = F}
library(dplyr)
library(ggplot2)
library(caret)
library(magick)

#set figure size
knitr::opts_chunk$set(fig.width=6, fig.height=4) 

#This is an optional library to add emojis to .Rmd files
# install.packages("devtools")
#devtools::install_github("hadley/emo")
library(emo)

library(ExamPAData)
library(tidyverse)#always load AFTER plyr.  Otherwise there are errors because some functions have the same names in both libraries.

library(kableExtra)
format_table <- function(data){data %>% kableExtra::kable("html") %>% kable_styling(bootstrap_options = "striped", full_width = F)}
```

[![Lecture](images/data_exploration.png)](https://exampa.net/)

Already enrolled?  Watch the full video: <a target="_parent" href="https://course.exampa.net/mod/page/view.php?id=133">Practice Exams + Lessons</a>

Exploration is about making discoveries and you should have a curious mind and strong technical skills to make sense of the data. This chapter will give you the tools to take dirty, unprocessed data, clean it up, discover interesting trends using graphs and summary statistics, and communicate your results to a business audience.

About 10 - 15% of your grade will be based on data exploration. Every version of Exam PA, such as December 2018, June 2019, and December 2019, have all had questions about data exploration. Putting in extra practice in this area is guaranteed to give you a better score because it will free up time that you can use elsewhere.

The syllabus divides this into three learning objectives, which can make the topic intimidating. Data Visualization, Data Types and Exploration, and Data Issues and Resolutions, but you will always be doing these things together, and so this chapter will cover all three at once.


```{r echo = F, fig.align="center", warning=F, out.width="400%"}
knitr::include_graphics("images/learning_obj3.PNG")
```

```{r echo = F, fig.align="center", warning=F, out.width="400%"}
knitr::include_graphics("images/learning_obj4.PNG")
```

```{r echo = F, fig.align="center", warning=F, out.width="400%"}
knitr::include_graphics("images/learning_obj5.PNG")
```


Exploratory Data Analysis (EDA) is usually the first phase of an analytics project. This is as much of an art as it is a science because everyone has their style. If you ask two actuaries to perform EDA on the same data set, they will likely use different methods. These can be broken down into phases.  

```{r echo = F, fig.cap="Data Exploration Phases - From *R for Data Science*"}
image_read('images/hadley_data_lifecycle.png') %>% image_trim() %>% image_scale("1400")
```

From the language on the December 2020 Syllabus, these are

**Import:** 

- Evaluate the quality of appropriate data sources for a problem.
- Identify the regulations, standards, and ethics surrounding predictive modeling and data

**Tidy:**

- Identify structured, unstructured, and semi-structured data.
- Identify the types of variables and terminology used in predictive modeling.
- Understand basic methods of handling missing data.
- Implement effective data design with respect to time frame, sampling, and granularity.

**Transform:**

- Identify opportunities to create features from the primary data that may add value.
- Identify outliers and other data issues.
- Handle non-linear relationships via transformations.

**Visualize:**

- Apply univariate and bivariate data exploration techniques.
- Understand the fundamental principles of constructing graphs.
- Create a variety of graphs using the ggplot2 package.

**Model:**

- Fit and interpret models

**Communicate:**

- Write the report

## How to make graphs in R?

Let us create a histogram of the claims. The first step is to create a blank canvas that holds the columns that are needed. The library to make this is called  [ggplot2](https://ggplot2.tidyverse.org/).  

The `aesthetic` argument, `aes`, means that the variable shown will the the claims.

```{r message = F, echo = F}
library(Cairo)
library(tidyverse)
theme_set(theme_bw())
```

The `sample_frac` means that only 20% of the data is used.  This makes the visuzlization less cluttered.

```{r message=F}
library(ExamPAData)
df <- readmission %>% sample_frac(0.05)
p <- df %>% 
  ggplot(aes(HCC.Riskscore))
```

If we look at `p`, we see that it is nothing but white space with axis for `count` and `income`.

```{r message=F}
p
```

### Add a plot

We add a histogram

```{r warning = F, message=F}
p + geom_histogram()
```

Different plots are called “geoms” for “geometric objects.” Geometry = Geo (space) + meter (measure), and graphs measure data. For instance, instead of creating a histogram, we can draw a gamma distribution with `stat_density`.

```{r message=F}
p + stat_density()
```

Create an xy plot by adding and `x` and a `y` argument to `aesthetic`.

```{r message=F}
df %>% 
  ggplot(aes(x = HCC.Riskscore, y = Age)) + 
  geom_point()
```

## The different graph types

Only four types of graphs are used for data exploration. You will only need to understand how to interpret them. The SOA will provide you with the code needed to create them. There will not be enough time for you to make your graphs.

### Histogram

The [histogram](https://ggplot2.tidyverse.org/reference/geom_histogram.html) is used when you want to look at the probability distribution of a continuous variable.

The template code in your .Rmd file will look like this. Just change “variable” to the name without quotes of the variable and then copy and paste.

### Box plot

The [boxplot](https://ggplot2.tidyverse.org/reference/geom_boxplot.html) compactly displays the distribution of a continuous variable. It visualizes five summary statistics (the median, two hinges, and two whiskers) and all “outlying” observations individually.

### Scatterplot

The [point geom](https://ggplot2.tidyverse.org/reference/geom_point.html) is used to create scatterplots. The scatterplot is most useful for displaying the relationship between two continuous variables. It can be used to compare one continuous and one categorical variable or two categorical variables.

### Bar charts

There are two types of [bar charts](https://ggplot2.tidyverse.org/reference/geom_bar.html): geom_bar() (Univariate) and geom_col() (Bivatiate). geom_bar() makes the height of the bar proportional to the number of cases in each group (or if the weight aesthetic is supplied, the sum of the weights). If you want the heights of the bars to represent values in the data, use geom_col() instead. geom_bar() uses stat_count() by default: it counts the number of cases at each x position.

## How to save time with dplyr?

You may have noticed that writing code for data manipulation can be slow. Fortunately, there is a faster, 100%-legal way of doing data manipulation that has worked for hundreds of exam candidates (the author included) who have taken Exam PA.

Up to this point, we have been using old R libraries. By making use of newer R libraries, we can save ourselves time. These will all be provided for you at Prometric within the [tidyverse](https://www.tidyverse.org/) library.

### Data manipulation chaining

Pipes allow for data manipulations to be chained with visualizations.  The possibilities are nearly limitless.

```{r message=F}
library(tidyverse)
iris %>%
  select_if(is.numeric) %>%
  gather(feature,value) %>%
  ggplot(aes(value)) +
  geom_histogram() +
  facet_wrap(vars(feature))
```

Suggested reading of *R for Data Science* (https://r4ds.had.co.nz/index.html):


| Chapter | Topic           |
|---------|-----------------|
| 9       | Introduction    |
| 10      | Tibbles         |
| 12      | Tidy data       |
| 15      | Factors         |
| 17      | Introduction    |
| 18      | Pipes           |
| 19      | Functions       |
| 20      | Vectors         |



## How to explore the data?

Let us look at the health insurance data set that contains information on patients along with their annual health care costs.

The descriptions of the columns are below.
  

- `age`: Age of the individual
- `sex`: Sex
- `bmi`: Body Mass Index
- `children`: Number of children
- `smoker`: Is this person a smoker?
- `region`: Region
- `charges`: Annual health care costs.

`head()` shows the top n rows.  `head(20)` shows the top 20 rows.  

```{r message = F}
head(health_insurance)
```

Using a pipe is an alternative way of doing this. 

```{r eval = F}
health_insurance %>% head()
```

</br>

```{block, type='studytip'}
**Shortcut:** Use `CTRL` + `SHFT` + `M` to create pipes `%>%`
```

</br>

The `glimpse` function is similar to `str()`.

```{r}
health_insurance %>% glimpse()
```

One of the most useful data science tools is counting things.  The function `count()` gives the number of observations by a categorical feature.   

```{r}
health_insurance %>% dplyr::count(children)
```

Two categories can be counted at once.  This creates a table with all combinations of `region` and `sex` and shows the number ofobservations in each category.

```{r}
health_insurance %>% count(region, sex)
```

The `summary()` function is shows a statistical summary.  One caveat is that each column needs to be in its appropriate type.  For example, `smoker`, `region`, and `sex` are all listed as characters when if they were factors, `summary` would give you count info.

**With incorrect data types**

```{r}
health_insurance %>% summary()
```

**With correct data types**

This tells you that there are 324 patients in the northeast, 325 in the northwest, 364 in the southeast, etc.

```{r}
health_insurance <- health_insurance %>% 
  mutate_if(is.character, as.factor)

health_insurance %>% 
  summary()
```

</br>

```{block, type='studytip'}
**Shortcut:** This "mutate_if" trick can save you time from having to copy and paste from the template code of SOA and convert each variable to a factor individually
```

</br>

## How to transform the data?

Transforming, manipulating, querying, and wrangling are synonyms in data terminology.

R syntax is designed to be similar to SQL.  They begin with a `SELECT`, use `GROUP BY` to aggregate, and have a `WHERE` to remove observations.  Unlike SQL, the ordering of these does not matter.  `SELECT` can come after a `WHERE`.

**R to SQL translation**

```
select() -> SELECT
mutate() -> user-defined columns
summarize() -> aggregated columns
left_join() -> LEFT JOIN
filter() -> WHERE
group_by() -> GROUP BY
filter() -> HAVING
arrange() -> ORDER BY

```

```{r}
health_insurance %>% 
  select(age, region) %>% 
  head()
```

Let us look at only those in the southeast region.  Instead of `WHERE`, use `filter`.

```{r}
health_insurance %>% 
  filter(region == "southeast") %>% 
  select(age, region) %>% 
  head()
```

The SQL translation is

```{sql eval = F}
SELECT age, region
FROM health_insurance
WHERE region = 'southeast'
```


Instead of `ORDER BY`, use `arrange`.  Unlike SQL, the order does not matter and `ORDER BY` doesn't need to be last.

```{r}
health_insurance %>% 
  arrange(age) %>% 
  select(age, region) %>% 
  head()
```

The `group_by` comes before the aggregation, unlike in SQL where the `GROUP BY` comes last.

```{r}
health_insurance %>% 
  group_by(region) %>% 
  summarise(avg_age = mean(age))
```

In SQL, this would be

```{sql eval = F}
SELECT region, 
       AVG(age) as avg_age
FROM health_insurance
GROUP BY region
```


Just like in SQL, many different aggregate functions can be used such as `SUM`, `MEAN`, `MIN`, `MAX`, and so forth.

```{r}
health_insurance %>% 
  group_by(region) %>% 
  summarise(avg_age = mean(age),
            max_age = max(age),
            median_charges = median(charges),
            bmi_std_dev = sd(bmi))
```

To create new columns, the `mutate` function is used.  For example, if we wanted a column of the annual charges of a person divided by their age

```{r}
health_insurance %>% 
  mutate(charges_over_age = charges/age) %>% 
  select(age, charges, charges_over_age) %>% 
  head(5)
```

We can create as many new columns as we want.

```{r}
health_insurance %>% 
  mutate(age_squared  = age^2,
         age_cubed = age^3,
         age_fourth = age^4) %>% 
  head(5)
```

The `CASE WHEN` function is quite similar to SQL.  For example, we can create a column which is `0` when `age < 50`, `1` when `50 <= age <= 70`, and `2` when `age > 70`.

```{r}
health_insurance %>% 
  mutate(age_bucket = case_when(age < 50 ~ 0,
                                age <= 70 ~ 1,
                                age > 70 ~ 2)) %>% 
  select(age, age_bucket)
```

SQL translation:

```{sql eval = F}
SELECT CASE WHEN AGE < 50 THEN 0
       ELSE WHEN AGE <= 70 THEN 1
       ELSE 2
FROM health_insurance
```

## Missing values

The most recent PA exams have had questions related to missing values. You need to perform checks before moving on to the model-building phases. In real life, man-made data always have missing observations.

In R, there are two ways that data can be missing.  The first is the `NA` value.  If you read in a csv file that has blank values, R will translate them into NAs.

```{r}
df <- tibble(x = c(1,2,NA), y = c(NA, NA, 2))
```

You can check for `NA` values with `summary` or `sum(is.na(df))`.

```{r}
summary(df)
sum(is.na(df))
```

The second type is for "Not A Number" (`NaN`) or infinity `Inf` values.  Dividing by zero produces `Inf` and taking the square root of a negative number produces `NaN`.  Notice that `NaN` counts as `NA` but `Inf` does not.  R says that there are only two missing values in this table even though there are two values which are infinity.

```{r}
df <- tibble(x = c(1/0, 4/0, 4, 1), y = c(sqrt(-1), NA, 2, 4))

sum(is.na(df))
summary(df)
```


### Types of Missing Values

Read the Data Dictionary from the Project Statement and check for these three types of missingness.

- **Missing at random (MAR):** There is no pattern between the missingness and the value of missing variable.
- **Missing not at random (MNAR):**  The value of the missing variable is related to the reason it is missing. Example: A survey concerning illegal drug use where respondents are reluctant to admit that they have broken the law.
- **Hidden missingness:** When a value is coded incorrectly, such as when a numeric variable with “999” or “0” or a factor does not match the Data Dictionary.

### Missing Value Resolutions:

You can use any of these methods but make sure that you describe your reasoning in the report.

- **Remove observations:** Use when there are only a few missing values or when they are MAR.
- **Create new level “missing:"** Use when there are more values or when they are MNAR.  
- **Impute with the mean:** Use when there are numeric values MAR.
- **Remove variable altogether:** Use when most observations are missing (≥ 50% of observations), and there is no way to repair the data. 

While you are reviewing the data for missingness, you may also decide to remove some variables altogether. This could be because of racial or ethical concerns, limitations of future availability, instability of the data over time, or inexplicability.  


## Example: SOA PA 12/12/19, Task 1

This chapter is based on the first task of the December 2019 Exam PA.

<iframe src="https://player.vimeo.com/video/467838183?title=0&byline=0&portrait=0" width="640" height="360" frameborder="0" allow="autoplay; fullscreen" allowfullscreen></iframe>

Already enrolled?  Watch the full video: <a target="_parent" href="https://course.exampa.net/mod/page/view.php?id=206
">Practice Exams</a> | <a target="_parent" href="https://course.exampa.net/mod/page/view.php?id=157
">Practice Exams + Lessons</a>

**TASK 1 (12 points)**

> Examine each variable and make appropriate adjustments.

> Examine each predictor variable other than cap_gain both on its own and for value_flag. Make appropriate adjustments. Do not make any adjustments to the cap_gain variable at this time.

> There should be no further variable adjustments unless specifically requested.

As a reminder, all data for this book can be accessed from the package `ExamPAData`.  In the actual exam, you will read the file from the Prometric computer.

Because the data is already loaded, use the below code to access the data.


```{r}
library(ExamPAData)
```

To save keystrokes, give the data a short name such as `df` for "data frame".

```{r echo = T}
df <- customer_value %>% mutate_if(is.character, as.factor)
```

### Garbage in; garbage out `r emo::ji("trash")`

This is a common saying when working with predictive models. No matter how complex the model, the actual result will be wrong if the data is terrible. For this exam, master the art of data manipulation, and everything becomes easier!

Begin by looking at a summary.


```{r}
summary(df)
str(df)
```

### Be a detective `r emo::ji("magnifying")`

Sherlock Holmes is famous for saying, “You see, Watson, but you do not observe!”

Just like detectives, actuaries need to collect data and make observations. Each exam has a few “anomalies” in the data, which they expect candidates to mention. These could be

* Any value that does not match the Data Dictionary in the Project Statement
* Variables that have `NA` values
* Variables that have many factor levels
* Incorrect data types
    * Factors that are read as characters or vice versa
    * Numeric variables that are factor/character
* Extreme values (Numeric values are too high or low)

You will get very good at spotting these with practice.  Just from looking at the above `summary`, we can observe the following:

**Observations**

* The data consists of 48,842 obs. of 8 variables.
* The lowest `age` is 17 but the Project Statement says to only include observations with `age` >= 25.
* The `cap_gain` distribution is right-skewed because the median (0) is less than the mean (1079), but the Project Statement said not to transform this. Otherwise, I would apply a log transform.
* `education_num` takes integer values between 1 and 16. There are many values that are low.
* There are missing values when `occupation` is `group NA`, which means that the occupation of the person is unknown.
* The amount that people work per work, `hours_per_week`, varies by a lot. The lowest is 1 hour, and the highest is 99. Most people work 40 hours per week.

</br>

```{block, type='studytip'}
**Study Tip:** Never apply a log transform to the target variable. Only the predictor variables get logs taken. At this stage, we note if it is right-skewed or not.
```

</br>

The solution of SOA recommends leaving comments in your .Rmd file. This helps to give you partial credit on questions that you may answer incorrectly.
  

**Good comments**

```{r}
#I observe that there are no missing values other than those indicated by Group NA for occupation.

#I removed the code provided by my assistant. It is embedded in later chunks as needed.

#I excluded people under the age of 25
df <- df[df$age >= 25,]

#I convert the target to 0-1.
df$value_flag <- ifelse(df$value_flag == "High",1,0)
```

**Useless comments**

```{r}
#this is a comment

#this loads a library called ggplot2
library(ggplot2)
```

### A picture is worth a thousand words `r emo::ji("camera")`

What is your favorite type of graph? Mine is a radar chart. This is a graphical method of displaying multivariate data in the form of a two-dimensional chart of three or more quantitative variables represented on axes starting from the same point.

That is pretty, right?


```{r echo = F, fig.align="center", warning=F, message = F, out.width="1300%"}
knitr::include_graphics("images/radar_graph.png")
```

**Do not waste time trying to make the graphs perfect!**

Only change code that you **need** to change. If you need assistance to do it faster then SOA can help you save time by giving you templates. 

This is the code template that they give you. You only need to change the “variable” names.


```{r eval = F}
# This code makes a histogram for a continuous variable.
ggplot(df, aes(x = variable)) + 
  geom_histogram(bins = 30) +
  labs(x = "variable") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# This code makes a bar chart for a factor variable.
ggplot(df, aes(x = variable)) +
  geom_bar() +
  labs(x = "variable") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

Okay, that was not 100% true. It is common for the code to require  **small** changes. changes. For instance, the histogram that they give you has bins that are too narrow for `age`, which causes these spikes to show up in the graph.

> The best candidates altered the code to create more appropriate plots and summary tables.

```{r message = F}
# This code makes a histogram for a continuous variable.
ggplot(df, aes(x = age)) + 
  geom_histogram(bins = 30) + #not the right number of bins
  labs(x = "age") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  ggtitle("Bad: Default histogram with spikes")

#Histograms of continuous variables
ggplot(df, aes(x = age)) +
  geom_histogram(breaks = seq(24.5,99.5, by = 5)) +  #make bins wider and set range 
  labs(x = "Age") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  ggtitle("Good: After adjusting the breaks")
```

How do you know when a variable should a factor and when it should be numeric?

### Factor or numeric `r emo::ji("question")`

Which variables should be converted to factors and which should be numeric? 

Questions of this sort have come up twice. On Hospital Readmissions, a Length of Stay variable was numeric but had only a few values, so some candidates treated it as a factor. The `education_num` variable here is also numeric but only has 16 unique values. So should this be a numeric or a factor?

```{r}
table(df$education_num)
```

Ask yourself this question: is there a way of comparing two values of the variable together?

* If yes, then use numeric  
* If no, then use a factor

For exmaple, we can say that `education_num = 2` is less than `education_num = 4`, which means there is a natural order. This is also known as an **ordinal**.

If the factor is say, `color`, which can be `red`, `blue`, or `green`, then there is no way of comparing values together.  Is `red` greater than `blue`?  This question has no meaning.


```{r}
ggplot(df, aes(x = education_num)) +
  geom_histogram(bins = 30) +
  labs(x = "Education") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  ggtitle("Default number of bins (30)")

ggplot(df, aes(x = education_num)) +
  geom_histogram(bins = 16) +
  labs(x = "Education") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  ggtitle("Set number of bins to number of factor levels (16) ")
# Set bins equal to number of levels, could have made bar chart.
```

We could also use a bar plot.

```{r}
ggplot(df, aes(x = education_num)) + 
  geom_bar(stat = "count") + 
  ggtitle("Bar plot")
```

Lastly, read the Project Statement carefully and only do what it tells you to do.

> Candidates were not required to make a plot for cap_gain. This solution has the plot made here rather than in Task 6.

### 73.6% of statistics are false `r emo::ji("shocked")`

Really? No, but statistics can help you see patterns that data visualization by itself can miss. Along with the ggplot codes, there will be code to look at summary statistics. Here is a refresher on what these statistics mean (no pun intended).

* **Mean**: The average. This gets skewed by outliers easily. If the mean is greater than the median, then the distribution is right-skewed. 
* **Median**: The “middle” value. This is an average that reduces the impact of outliers.
* **Variance**: The amount by which each observation differs from the mean.  
* **Standard Deviation**: The square root of the variance.
* **n()**: The number of observations. Always take note of groups that do not have many observations.

```{r eval =F}
# This code provides, for each level of a factor variable, the number for which value_flag is zero, the number for which it is one, the total number of observations, and the proportion of ones.
# Note that the variable name should not be enclosed in quotation marks.
df %>%
  group_by(variable) %>%
  dplyr::summarise(
    zeros = sum(value_flag == 0),
    ones = sum(value_flag == 1),
    n = n(),
    proportion = mean(value_flag)
  )
```

Factors levels should be simplified. If a group has only a few observations, then there will be problems with the model. In our data, take a look at the `marital_status` column. Do you observe anything unusual?

```{r}
table(df$marital_status)
```

Only 31 observations have `Married-AF-spouse`.  This is because the sample size `n = 31` is too small.  In modeling jargon, this is "statistical insignificant" and will cause the p-value on `marital_status` to be large.  You can fix this in a few different ways

* Delete these observations (Not recommended)
* Group these observations together with `Married-spouse` (Simplest method)

Let us use the second method.

First, look at the levels of the factor variable.

```{r}
levels(df$marital_status)
```

Now look at the profitability across marital status.  For `Married-AF-spouse` and `Married-civ-spouse` the proportion of high profit customers is **high**, but for `Married-spouse-absent` it is **low.**  Even though these are all "married", it would be a bad idea to combine them because the profitability is so different.

```{r}
#Proportion of ones by category of factor variable
df %>%
  group_by(marital_status) %>%
  dplyr::summarise(
    zeros = sum(value_flag == 0),
    ones = sum(value_flag == 1),
    n = n(),
    proportion = mean(value_flag)
  )
```


Then create a vector that has the simpler levels that you want. The order needs to be the same.

```{r}
simple_levels <- c("Divorced", "Married-spouse", "Married-spouse", "Married-spuouse-absent", "Neber-married", "Separated", "Widowed")
```

The function `mapvalues` takes in three arguments.  You can read about this by typing `?mapvalues` into the console.

`x`	: the factor or vector to modify

`from` : a vector of the items to replace

`to`	: a vector of replacement values

Then map the old values to the simpler values.

```{r echo = F, message= F, warning=F}
library(plyr)
```

```{r}
#Combine the two marital status levels
var.levels <- levels(df$marital_status)
df$marital_status <- mapvalues(x = df$marital_status,
                               from = var.levels, 
                               to = simple_levels) 
```

```{r echo = F, message= F, warning=F}
#detach functions which are from plyr
#An easier alternative is just to restart R
if(any(grepl("package:plyr", search()))) detach("package:plyr") else message("plyr not loaded")
```



Now, when you look at the `marital_status` levels, you will see the simpler levels.  

```{r}
levels(df$marital_status)
```

You can also check that the number of observations is what you expect.

```{r}
table(df$marital_status)
```

## Exercises

### Data Exploration Practice

[![Lecture](images/tutorial-data-exploration.PNG)](https://exampa.net/)

### Dplyr Practice

[![Lecture](images/lesson-dplyr.PNG)](https://exampa.net/)

Run this code on your computer to answer these exercises.

The data `actuary_salaries` contains the salaries of actuaries collected from the DW Simpson survey. Use this data to answer the exercises below.

```{r }
actuary_salaries %>% glimpse()
```

1.  How many industries are represented?
2.  The `salary_high` column is a character type when it should be numeric. Change this column to numeric.
3.  3.	What are the highest and lowest salaries for an actuary in Health with 5 exams passed?
4.  Create a new column called `salary_mid` which has the middle of the `salary_low` and `salary_high` columns.
5.  When grouping by industry, what is the highest `salary_mid`?  What about `salary_high`?  What is the lowest `salary_low`?
6.  There is a mistake when `salary_low == 11`.  Find and fix this mistake, and then rerun the code from the previous task.
7.  Create a new column, called `n_exams`, which is an integer.  Use 7 for ASA/ACAS and 10 for FSA/FCAS.  Use the code below as a starting point and fill in the `_` spaces
8. Create a column called `social_life`, which is equal to `n_exams`/`experience`.  What is the average (mean) `social_life` by industry?  Bonus question: what is wrong with using this as a statistical measure?


```{r eval = F, echo = T}
actuary_salaries <- actuary_salaries %>% 
  mutate(n_exams = case_when(exams == "FSA" ~ _,
                             exams == "ASA" ~ _,
                             exams == "FCAS" ~ _,
                             exams == "ACAS" ~ _,
                             TRUE ~ as.numeric(substr(exams,_,_)))) 
```

8. Create a column called `social_life`, which is equal to `n_exams`/`experience`.  What is the average (mean) `social_life` by industry?  Bonus question: what is wrong with using this as a statistical measure?

## Answers to exercises

Answers to these exercises, along with a video tutorial, are available at [ExamPA.net](https://exampa.net/).

<!--chapter:end:03-data-exploration.Rmd-->

---
output:
  html_document: default
  pdf_document: default
---

```{r echo = F, message = F}
set.seed(1)
library(ggplot2)
theme_set(theme_bw())
```


# Introduction to modeling

About 40-50% of the exam grade is based on modeling. The goal is to be able to predict an unknown quantity. In actuarial applications, this tends to claim that occur in the future, death, injury, accidents, policy lapse, hurricanes, or some other insurable event.

The next few chapters will cover the following learning objectives.


```{r echo = F, fig.align="center", warning=F, out.width="400%"}
knitr::include_graphics("images/learning_obj6.png")
```

## Modeling vocabulary
Modeling notation is sloppy because many words mean the same thing.

The number of observations will be denoted by  $n$.  When we refer to the size of a data set, we are referring to $n$.  Each row of the data is called an observation or record. Observations tend to be people, cars, buildings, or other insurable things. These are always independent in that they do not influence one another. Because the Prometric computers have limited power, $n$ tends to be less than 100,000.

Each observation has known attributes called *variables*, *features*, or *predictors*.  We use $p$ to refer the number of input variables that are used in the model.  

The *target*, *response*, *label*, *dependent variable*, or *outcome* variable is the unknown quantity that is being predicted.  We use $Y$ for this.  This can be either a whole number, in which case we are performing regression, or a category, in which case we perform classification.

For example, say that you are a health insurance company that wants to set the premiums for a group of people. The premiums for people who are likely to incur high health costs need to be higher than those likely to be low-cost. 

Older people tend to use more of their health benefits than younger people, but there are always exceptions for those who are very physically active and healthy. Those who have an unhealthy Body Mass Index (BMI) tend to have higher costs than those who have a healthy BMI, but this has less impact on younger people. 

In short, we want to predict the future health costs of a person by taking into account many of their attributes at once.

This can be done in the `health_insurance` data by fitting a model to predict the annual health costs of a person.  The target variable is `y = charges`, and the predictor variables are `age`, `sex`, `bmi`, `children`, `smoker` and `region`.  These six variables mean that $p = 6$.  The data is collected from 1,338 patients, which means that $n = 1,338$.   

## Modeling notation

Scalar numbers are denoted by ordinary variables (i.e., $x = 2$, $z = 4$), and vectors are denoted by bold-faced letters 

$$\mathbf{a} = \begin{pmatrix} a_1 \\ a_2 \\ a_3 \end{pmatrix}$$

We organize these variables into matrices.  Take an example with $p$ = 2 columns and 3 observations.  The matrix is said to be $3 \times 2$ (read as "3-by-2") matrix.

$$
\mathbf{X} = \begin{pmatrix}x_{11} & x_{21}\\
x_{21} & x_{22}\\
x_{31} & x_{32}
\end{pmatrix}
$$

In the health care costs example, $y_1$ would be the costs of the first patient, $y_2$ the costs of the second patient, and so forth.  The variables $x_{11}$ and $x_{12}$ might represent the first patient's age and sex respectively, where $x_{i1}$ is the patient's age, and $x_{i2} = 1$ if the ith patient is male and 0 if female.

Modeling is about using $X$ to predict $Y$. We call this "y-hat", or simply the *prediction*.  This is based on a function of the data $X$.

$$\hat{Y} = f(X)$$

This is almost never going to happen perfectly, and so there is always an error term, $\epsilon$.  This can be made smaller, but is never exactly zero.  

$$
\hat{Y} + \epsilon = f(X) + \epsilon
$$

In other words, $\epsilon = y - \hat{y}$.  We call this the *residual*.  When we predict the health care costs of a person, this is the difference between the predicted costs (which we had created the year before) and the actual costs that the patient experienced (of that current year).

Another way of saying this is in terms of expected value: the model $f(X)$ estimates the expected value of the target $E[Y|X]$.  That is, once we condition on the data $X$, we can make a guess as to what we expect $Y$ to be "close to". We will see that there are many ways of measuring "closeness." 

## Ordinary Least Squares (OLS)

Also known as *simple linear regression*, OLS predicts the target as a weighted sum of the variables.

<iframe width="560" height="315" src="https://www.youtube.com/embed/PaFPbb66DxQ?rel=0&showinfo=1&playlist=nk2CQITm_eo,u1cc1r_Y7M0" frameborder="0" allowfullscreen></iframe>

</br>

We find a $\mathbf{\beta}$ so that 

$$
\hat{Y} = E[Y] =  \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_p X_p
$$

Each $y_i$ is a *linear combination* of $x_{i1}, ..., x_{ip}$, plus a constant $\beta_0$ which is called the *intercept* term.  

In the one-dimensional case, this creates a line connecting the observations.  In higher dimensions, this creates a hyper-plane.

```{r message = F, echo = F,caption = "Linear Regression for 1-dimensional model"}
library(tidyverse)
tibble(y = rnorm(10, 0, 1),
       x = y + 0.4*rnorm(10,0, 1)) %>% 
  ggplot(aes(x,y)) + 
  geom_point( show.legend = F) + 
  geom_smooth(method = "lm", se = F, aes(fill = "linear regression"), color = "red", show.legend = T) + 
  scale_fill_manual(name="legend", values=c("blue", "red")) + 
  theme(legend.position = "top")
```

The red line shows the *expected value* of the target, as the target $\hat{Y}$ is actually a random variable.  For each observation, the model assumes a Gaussian distribution. If there is just a single predictor, $x$, then the mean is $\beta_0 + \beta_1 x$.

```{r echo = F, fig.align="center", message=F, warning = F, out.width="200%"}
knitr::include_graphics("images/conditional_response.jpg")
```

The question then is **how can we choose the best values of** $\beta?$  First of all, we need to define what we mean by "best".  Ideally, we will choose these values which will create close predictions of $Y$ on new, unseen data.  

To solve for $\mathbf{\beta}$, we first need to define a **loss function**. This would let us compare how well a model fits the data. The most commonly used loss function is the residual sum of squares (RSS), also called the **squared error loss or the L2 norm**. When RSS is small, then the predictions are close to the actual values, and the model is a good fit. When RSS is large, the model is a poor fit.

$$\text{RSS} = \sum_i(y_i - \hat{y})^2$$

When you replace $\hat{y_i}$ in the above equation with $\beta_0 + \beta_1 x_1 + ... + \beta_p x_p$, take the derivative with respect to $\beta$, set equal to zero, and solve, we can find the optimal values.  This turns the problem of statistics into a problem of numeric optimization, which computers can do quickly.  

You will also see the term **Root Mean Squared Error** (RMSE) which is just the average of the square root of the $\text{RSS}$, or just **Mean Squared Error** (MSE).

You might be wondering: why does this need to be the squared error? Why not the absolute error or the cubed error? Technically, these could be used, but the betas would not be the maximum likelihood parameters. Using the absolute error results in the model predicting the median as opposed to the mean. Two reasons behind the popularity of RSS are:  

- It provides the same solution if we assume that the distribution of $Y|X$ is Gaussian and maximize the likelihood function.  This method is used for GLMs, in the next chapter.
- It is computationally easier, and computers used to have a difficult time optimizing for MAE.

>What does it mean when a log transform is applied to $Y$?  I remember from my statistics course on regression that this was done.  

This is done so that the variance is closer to being constant. For example, if the units are in dollars, it is very common for the values to fluctuate more for higher values than for lower values. 

Consider a stock price, for instance. If the stock is $50 per share, it will go up or down less than $1000 per share. However, the log of 50 is about 3.9, and the log of 1000 is only 6.9, so this difference is smaller. In other words, the variance is smaller.

Transforming the target means that instead of the model predicting $E[Y]$, it predicts $E[log(Y)]$.  A common mistake is to then the take the exponent in an attempt to "undo" this transform, but $e^{E[log(Y)]}$ is not the same as $E[Y]$.


## R^2 Statistic
One of the most common ways of measuring model fit is the “R-Squared” statistic. The RSS provides an absolute measure of fit because it can be any positive value, but it is not always clear what a “good” RSS is because it is measured in units of $Y$.  

The $R^2$ statistic provides an alternative measure of fit. It takes the proportion of variance explained - so that it is always a value between 0 and 1 and is independent of the scale of $Y$.

$$R^2 = \frac{\text{TSS} - \text{RSS}}{\text{TSS}} = 1 - \frac{\text{RSS}}{\text{TSS}}$$

Where $\text{TSS} = \sum(y_i - \hat{y})^2$ is the total sum of squares. TSS measures the total variance in the response YY and can be considered the amount of variability inherent in the response before the regression is performed. In contrast, RSS measures the amount of variability that is left unexplained after performing the regression. 

Hench, $\text{TSS} - \text{RSS}$ measures the amount of variability in the response that is explained (or removed) by performing the regression, and R^2 measures the proportion of variability in $Y$ that can be explained using $X$. 

A value near 1 indicates that the regression has explained a large proportion of the variability in the response, whereas, a number near 0 indicates that the regression did not explain much of the variability in the response. This might occur because the linear model is wrong.

The $R^2$ sstatistic has an interpretational advantage over the RSE. In actuarial applications, it is useful to use an absolute measure of model fit, such as RSS, to train the model, and then use $R^2$ when explaining it to your clients so that it is easier to communicate.

This chapter was based on Chapter 3, *Linear Regression*, of *An Introduction to Statistical Learning.*

## Correlation

*Correlation does not imply causation.*

This is a common saying. Just because two things are correlated does not necessarily mean that one causes the other. Just because most actuaries work remotely when there is cold and snowing does not mean that cold and snow cause anti-social, introverted work habits. A more likely explanation is that actuaries are concerned about driving safely on icy roads and avoiding being involved in a car accident.

### Pearson's correlation

**Pearson correlation:** Measures a linear dependence between two variables $X$ and $Y$. This is the most commonly used correlation method.  

The *correlation* is defined by $r$,

$$r = Cor(X,Y) = \frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum(x_i - \bar{x})^2}\sqrt{(y_i - \bar{y})^2}}$$

and this is also a measure of the linear relationship between two vectors, $X$ and $Y$.  This suggests that we might be able to use $r = Cor(X,Y)$ instead of $R^2$ to assess the model fit.  In the case of simple linear regression, where there is only one predictor variable, it is tree that $R^2 = r^2$; however, this relationship does not extend automatically when there are more than one predictor variable.  This is because $X$ becomes a *matrix* instead of a single *vector.*

### Spearman (rank) correlation

**Spearman correlation:** Computes the correlation between the rank of x and the rank of y variables.

$$rho = \frac{\sum(x' - m_{x'})(y'_i - m_{y'})}{\sqrt{\sum(x' - m_{x'})^2 \sum(y' - m_{y'})^2}}$$
Where $x′=rank(x)$ and $y′=rank(y)$

Most questions on Exam PA will ask you about Pearson’s correlation. One advantage to Spearman over Pearson is that Spearman works for ordinal variables. See Chapter 6 for the difference between **ordinal** and **numeric** variables.

## Regression vs. classification

Regression modeling is when a target is a number. Binary classification is when there are two outcomes, such as “Yes/No,” “True/False,” or “0/1”. Multi-class regression is when there are more than two categories such as “Red, Yellow, Green” or “A, B, C, D, E.” There are many other types of regression that are not covered on this exam, such as ordinal regression, where the outcome is an ordered category, or time-series regression, where the data is time-dependent.

## Regression metrics

For any model, the goal is always to reduce an error metric. This is a way of measuring how well the model can explain the target. 

The phrases “reducing error,” “improving performance,” or “making a better fit” are synonymous with reducing the error. The word “better” means “lower error,” and “worse” means “higher error.”

The choice of error metric has a big difference in the outcome. When explaining a model to a businessperson, using simpler metrics such as R-Squared and Accuracy is convenient. When training the model, however, using a more nuanced metric is almost always better.

These are the regression metrics that are most likely to appear on Exam PA. Memorizing these formulas for AIC and BIC is unnecessary as they are in the R documentation by typing
 `?AIC` or `?BIC` into the R console.

```{r echo = F, fig.align="center", warning=F, message = F, out.width="1300%"}
knitr::include_graphics("images/regression_metrics.png")
```

</br>

```{block, type='studytip'}
Do not forget the most important metric: “usefulness!”. A model with high predictive accuracy but does not meet the needs of the business problem has low usefulness. A model which is easy to explain to the PA exam graders has high usefulness.

>"Some candidates did not consider both predictive power and applicability to the business problem and others gave justifications based on one of these but then chose a model based on the other.” - SOA PA 6/18/20, Task 12 Solution

> "Which of the three models would you recommend for this analysis? Do not base your recommendation solely on the mean squared errors (RMSE) from each model.” - SOA PA 6/13/19, Task 9 Project Statement

```

</br>


### Example: SOA PA 6/18/20, Task 4

> Three points of Investigate correlations.

>1) Create a correlation coefficient matrix for all the numeric variables in the dataset.

>2) Among these pairwise correlations, determine which correlations concern you in building GLM and tree models. The response may differ by model.

>3) State a method other than principal components analysis (PCA) that can be used to handle the correlated variables. Do not implement this method.
 
 

## Example: Health Costs

In our health insurance data, we can predict the health costs of a person based on their age, body mass index and gender. Intuitively, we expect that these costs would increase with the increase in the age of the person, be different for men than for women, and be higher for those who have a less healthy BMI. We create a linear model using `bmi`, `age`, and `sex` as an inputs.

The `formula` controls which variables are included.  There are a few shortcuts for using R formulas.  

| Formula | Meaning  | 
|-------|---------|
| `charges` ~ `bmi` + `age` | Use `age` and `bmi` to predict `charges` |
| `charges` ~ `bmi` + `age` + `bmi`*`age` | Use `age`,`bmi` as well as an interaction to predict `charges` |
| `charges` ~ (`bmi > 20`) + `age` | Use an indicator variable for `bmi > 20` `age` to predict `charges` |
| log(`charges`) ~ log(`bmi`) + log(`age`) | Use the logs of `age` and `bmi` to predict  log(`charges`) |
| `charges` ~ . | Use all variables to predict `charges`|


While you can use formulas to create new variables, the exam questions tend to have you do this in the data itself.  For example, if taking the log transform of a `bmi`, you would add a column `log_bmi` to the data and remove the original `bmi` column.

Below we fit a simple linear model to predict charges.

```{r message = F}
library(ExamPAData)
library(tidyverse)

model <- lm(data = health_insurance, formula = charges ~ bmi + age + sex)
```

The `summary` function gives details about the model.  First, the `Estimate`, gives you the coefficients.  The `Std. Error` is the error of the estimate for the coefficient.  Higher standard error means greater uncertainty.  This is relative to the average value of that variable.  The `p value` tells you how "big" this error really is based on standard deviations.  A small p-value (`Pr (>|t|))`) means that we can safely reject the null hypothesis that says the coefficient is equal to zero.

The little `*`, `**`, `***` tell you the significance level.  A variable with a `***` means that the probability of getting a coefficient of that size given that the data was randomly generated is less than 0.001.  The `**` has a significance level of 0.01, and `*` of 0.05.

```{r}
summary(model)
```

>For this exam, variable selection tends to be based on the 0.05 significance level (single star *).

When evaluating model performance, you should not rely on the  `summary` alone, based on the training data. To look at performance, test the model on validation data. This can be done by either using a hold-out set or cross-validation, which is even better.

Let us create an 80% training set and 20% testing set. You do not need to worry about understanding this code as the exam will always give this to you.


```{r message = F}
set.seed(1)
library(caret)
#create a train/test split
index <- createDataPartition(y = health_insurance$charges, 
                             p = 0.8, list = F) %>% as.numeric()
train <-  health_insurance %>% slice(index)
test <- health_insurance %>% slice(-index)
```

Train the model on the `train` and test on `test`.  

```{r}
model <- lm(data = train, formula = charges ~ bmi + age)
pred = predict(model, test)
```

Let's look at the Root Mean Squared Error (RMSE).  

```{r}
get_rmse <- function(y, y_hat){
  sqrt(mean((y - y_hat)^2))
}

get_rmse(pred, test$charges)
```

And the Mean Absolute Error as well.

```{r}
get_mae <- function(y, y_hat){
  sqrt(mean(abs(y - y_hat)))
}

get_mae(pred, test$charges)
```

The above metrics do not tell us if this is a good model or not by itself. We need a comparison. The fastest check is to compare against a prediction of the mean. In other words, all values of the `y_hat` are the average of `charges`, which is about \$13,000.

```{r}
get_rmse(mean(test$charges), test$charges)
get_mae(mean(test$charges), test$charges)
```

The RMSE and MAE are both higher (worse) when using just the mean, which we expect. If you ever fit a model and get an error that is worse than the average prediction, something must be wrong.

The next test is to see if any assumptions have been violated.

First, is there a pattern in the residuals? If there is, this means that the model is missing key information. 

For the model below, this is a yes, which means that this is a bad model. Because this is just for illustration, we are going to continue using it.
  

```{r fig.cap="Residuals vs. Fitted"}
plot(model, which = 1)
```
The normal QQ shows how well the quantiles of the predictions fit a theoretical normal distribution. If this is true, then the graph is a straight 45-degree line. In this model, you can see that this is not the case. If this were a good model, this distribution would be closer to normal.

```{r fig.cap="Normal Q-Q"}
plot(model, which = 2)
```

Once you have chosen your model, you should re-train over the entire data set. This is to make the coefficients more stable because `n` is larger. Below you can see that the standard error is lower after training over the entire data set.

```{r }
all_data <- lm(data = health_insurance, 
               formula = charges ~ bmi + age)
testing <- lm(data = test, 
              formula = charges ~ bmi + age)
```

```{r fig.cap="Coefficient Standard Error on Test vs. All Data", echo = F, message=F}
library(broom)
library(kableExtra)
all_data %>% 
  tidy() %>% 
  select(term, std.error) %>% 
  left_join(testing %>% tidy() %>% select(term, std.error), by = "term") %>% 
  rename(full_data_std_error = std.error.x, test_data_std_error = std.error.y) %>% 
  mutate_if(is.numeric, ~round(.x, 1)) %>% 
  kableExtra::kable("markdown")
```

All interpretations should be based on the model which was trained on the entire data set. This only makes a difference if you are interpreting the precise values of the coefficients. If you are just looking at which variables are included or at the size and sign of the coefficients, then this would probably not make a difference.

```{r}
coefficients(model)
```

Translating the above into an equation we have

$$\hat{y_i} = -4,526 + 287 \space\text{bmi} + 228\space \text{age}$$

For example, if a patient has `bmi = 27.9` and `age = 19` then predicted value is 

$$\hat{y_1} = 4,526 + (287)(27.9) + (228)(19) = 16,865$$

This model structure implies that each of the variables $x_1, ..., x_p$ each change the predicted $\hat{y}$.  If $x_{ij}$ increases by one unit, then $y_i$ increases by $\beta_j$ units, regardless of what happens to all of the other variables.  This is one of the main assumptions of linear models: *variable independence*.  If the variables are correlated, say, then this assumption will be violated.  

| Readings |  | 
|-------|---------|
| ISLR 2.1 What is statistical learning?|  |
| ISLR 2.2 Assessing model accuracy|  |


# Generalized linear Models (GLMs)

[![Lecture](images/glms lesson.png)](https://exampa.net/)

Already enrolled?  Watch the full video: <a target="_parent" href="https://course.exampa.net/mod/page/view.php?id=136">Practice Exams + Lessons</a>

GLMs are a broad category of models. Ordinary Least Squares and Logistic Regression are both examples of GLMs.

### Assumptions of OLS

We assume that the target is Gaussian with a mean equal to the linear predictor. This can be broken down into two parts:

1. A *random component*: The target variable $Y|X$ is normally distributed with mean $\mu = \mu(X) = E(Y|X)$

2. A link between the target and the covariates (also known as the systemic component) $\mu(X) = X\beta$

This says that each observation follows a normal distribution that has a mean that is equal to the linear predictor. Another way of saying this is that “after we adjust for the data, the error is normally distributed and the variance is constant.” If $I$ is an n-by-in identity matrix, and $\sigma^2 I$ is the covariance matrix, then

$$
\mathbf{Y|X} \sim N( \mathbf{X \beta}, \mathbf{\sigma^2} I)
$$

### Assumptions of GLMs

GLMs are more general which eludes that they are more flexible. We relax these two assumptions by saying that the model is defined by

1. A random component: $Y|X \sim \text{some exponential family distribution}$

2. A link: between the random component and covariates: 

$$g(\mu(X)) = X\beta$$
where $g$ is called the *link function* and $\mu = E[Y|X]$.

Each observation follows some type of exponential distribution (Gamma, Inverse Gaussian, Poisson, Binomial, etc.), and that distribution has a mean which is related to the linear predictor through the link function. Additionally, there is a dispersion parameter, but that is more info is needed here. For an explanation, see [Ch. 2.2 of CAS Monograph 5](https://contentpreview.s3.us-east-2.amazonaws.com/CAS+Monograph+5+-+Generalized+Linear+Models+for+Insurance+Ratemaking.pdf).

## Advantages and disadvantages

There is usually at least one question on the PA exam which asks you to “list some of the advantages and disadvantages of using this particular model,” and so here is one such list. It is unlikely that the grader will take off points for including too many comments and so a good strategy is to include everything that comes to mind.

**GLM Advantages**

- Easy to interpret
- Can easily be deployed in spreadsheet format
- Handles different response/target distributions
- Is commonly used in insurance ratemaking

**GLM Disadvantages**

- Does not select features (without stepwise selection)
- Strict assumptions around distribution shape and randomness of error terms
- Predictor variables need to be uncorrelated
- Unable to detect non-linearity directly (although this can manually be addressed through feature engineering)
- Sensitive to outliers
- Low predictive power

## GLMs for regression

For regression problems, we try to match the actual distribution to the distribution of the model being used in the GLM. These are the most likely distributions.

The choice of target distribution should be similar to the actual distribution of $Y$.  For instance, if $Y$ is never less than zero, then using the Gaussian distribution is not ideal because this can allow for negative values. If the distribution is right-skewed, then the Gamma or Inverse Gaussian may be appropriate because they are also right-skewed. 

```{r fig.height = 4, message = F, echo = F, warning = F, fig.align="center"}
library(mgcv)
library(ggpubr)
sample_size = 3000
p1 <- tibble(gaussian = rnorm(sample_size),
       gamma = rgamma(sample_size, 1, 1/2),
       inverse_gaussian = rig(sample_size, 1, 2)) %>% 
  gather(distribution, values) %>% 
  ggplot(aes(values)) + 
  geom_density(fill = "blue", alpha = 0.2) + 
  facet_wrap(vars(distribution), scales = "free") + 
  ggtitle("Continuous Target Distributions") + 
  xlab("")

p2 <- tibble(poisson = rpois(sample_size, 1),
       binomial = rbinom(sample_size, 10, 0.5)) %>% 
  gather(distribution, values) %>% 
  ggplot(aes(values)) + 
  geom_histogram(fill = "blue", alpha = 0.2) + 
  facet_wrap(vars(distribution), scales = "free") + 
  ggtitle("Discrete Target Distributions") + 
  xlab("")

ggarrange(p1,p2,nrow=2)
```

Notice that the top three distributions are continuous but the bottom two are discrete.  

```{r echo = F, fig.align="center", warning=F, out.width="250%"}
knitr::include_graphics("images/response_distributions.png")
```

There are five link functions for a continuous $Y$, , although the choice of distribution family will typically rule out several of these immediately. The linear predictor (a.k.a., the systemic component) is $z$ and the link function is how this connects to the expected value of the response.

$$z = X\beta = g(\mu)$$

```{r echo = F, fig.align="center", warning=F, message = F, out.width="200%"}
knitr::include_graphics("images/link_functions.png")
```

If the target distribution must have a positive mean, such as in the Inverse Gaussian or Gamma, then the Identity or Inverse links are poor choices because they allow for negative values; the mean range is $(-\infty, \infty)$.  The other link functions force the mean to be positive. 

## Interpretation of coefficients

The GLM's interpretation depends on the choice of link function.  

### Identity link

This is the easiest to interpret.  For each one-unit increase in $X_j$, the expected value of the target, $E[Y]$, increases by $\beta_j$, assuming that all other variables are held constant.

### Log link

This is the most popular choice when the results need to be easy to understand.  Simply take the exponent of the coefficients and the model turns into a product of numbers being multiplied together.

$$
log(\hat{Y}) = X\beta \Rightarrow \hat{Y} = e^{X \beta}
$$

For a single observation $Y_i$, this is

$$
\text{exp}(\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + ... + \beta_p X_{ip}) = \\
e^{\beta_0} e^{\beta_1 X_{i1}}e^{\beta_2 X_{i2}} ...  e^{\beta_p X_{ip}} = 
R_{i0} R_{i2} R_{i3} ... R_{ip}
$$

$R_{ik}$ is known as the *relativity* is known as the relativity of the kth variable. This terminology is from insurance ratemaking, where actuaries need to explain the impact of each variable to insurance regulators.

Another advantage to the log link is that the coefficients can be interpreted as having a percentage change on the target. Here is an example for a GLM with variables $X_1$ and $X_2$ and a log link function. This holds any continuous target distribution.

| Variable    | $\beta_j$ | $e^{\beta_j} - 1$ | Interpretation                                    | 
|-------------|-------------|----------------------|---------------------------------------------------| 
| (intercept) | 0.100       | 0.105                |                                                   | 
| $X_1$          | 0.400       | 0.492                | 49% increase in $E[Y]$ for each unit increase in $X_1$* | 
| $X_2$          | -0.500      | -0.393               | 39% decrease in $E[Y]$ for each unit increase in $X_2$* | 


If categorical predictors are used, then the interpretation is very similar.  Say that there is one predictor, `COLOR`, which takes on values of `YELLOW` (reference level), `RED`, and `BLUE`.  


| Variable    | $\beta_j$ | $e^{\beta_j} - 1$  | Interpretation                                          | 
|-------------|-------------|----------------------|---------------------------------------------------------| 
| (intercept) | 0.100       | 0.105                |                                                         | 
| Color=RED   | 0.400       | 0.492                | 49% increase  in $E[Y]$ for RED cars as opposed to YELLOW cars*| 
| Color=BLUE  | -0.500      | -0.393               | 39% decrease in $E[Y]$ for BLUE cars rather than YELLOW cars*| 

\* Assuming all other variables are held constant.


</br>

```{block, type='studytip'}
**Warning:** Never take the log of Y with a GLM! This is a common mistake because we handled skewness for multiple linear regression models, but that was before we had the GLM in our toolbox. Do not move on until you understand the difference between these two models: 

`glm(y ~ x, family = gaussian(link = "log"), data = data)` 

`glm(log(y) ~ x, family = gaussian(link = "identity"), data = data)`

The first says that the target has a Gaussian distribution which has a mean equal to the log of the linear predictor. The second says that the target's log has a Guassian distribution that is exactly equal to the linear predictor. You will remember from Exam P that when you apply a transform to a random variable, the distribution changes completely. Try running the above examples on real data and see if you can spot the differences in the results.
```

</br>


## Other links

The other link functions are not straightforward to interpret using math. One solution is to use the **model-demo-method**. See the example at the end of this next chapter.


# GLMs for classification

For classification, the predicted values need to be a category instead of a number. Using a discrete target distribution ensures that this will be the case. The probability of an event occurring is $E[Y] = p$. Unlike the continuous case, all of the link functions have the same range between 0 and 1 because this is a probability.

These StatQuest videos explain the most common type of GLM classification model: Logistic regression.


<iframe width="560" height="315" src="https://www.youtube.com/embed/PaFPbb66DxQ?rel=0&showinfo=1&playlist=yIYKR4sgzI8,vN5cNN2-HWE,BfKanl1aSG0,xxFYro8QuXA,C4N3_XJJ-jU" frameborder="0" allowfullscreen></iframe>


## Binary target

When $Y$ is binary, then the Binomial distribution is the only choice. If there are multiple categories, then the Multinomial should be used.

## Count target 

When $Y$ is a count, the Poisson distribution is the only choice. Two examples are counting the number of claims a policy has in a given year or counting the number of people visiting the ER in a given month. The key ingredients are 1) some events and 2) some fixed periods.

Statistically, the name for this is a Poisson Process, which describes a series of discrete events where the average time between events is known, called the "rate" $\lambda$, but the exact timing of events is unknown. For a time interval of length $m$, the expected number of events is $\lambda m$.  

By using a GLM, we can fit a different rate for each observation. In the ER example, each patient would have a different rate. Those who are unhealthy or who work in risky environments would have a higher rate of ER visits than those who are healthy and work in offices.

$$Y_i|X_i \sim \text{Poisson}(\lambda_i m_i)$$


When all observations have the same exposure, $m = 1$.  When the mean of the data is far from the variance, an additional parameter known as the dispersion parameter is used. A classic example is when modeling insurance claim counts, which have a lot of zero claims. Then the model is said to be an “over-dispersed Poisson” or “zero-inflated” model.  

## Link functions

There are four link functions. The most common are the Logit and Probit, but the Cauchit and Cloglog did appear on the Hospital Readmissions practice exam of SOA in 2019. The identity link does not make sense for classification because it would result in predictions being outside of $(0,1)$

```{r echo = F, fig.align="center", warning=F, message = F, out.width="1200%"}
knitr::include_graphics("images/discrete_link_functions.png")
```

> The *logit* is also known as the *standard logistic function* or *sigmoid* and is also used in deep learning.

Below we see how the linear predictor (x-axis) gets converted to a probability (y-axis).  

```{r warning = F, message = F, echo = F, fig.align="center"}
library(purrr)
tibble(z = seq(-9, 9, 0.01)) %>% 
  mutate(
    logit = exp(z)/(1 + exp(z)),
    probit = map_dbl(z, ~pnorm(.x)),
    cauchit = 1/3.14*atan(z) + 1/2,
    cloglog = 1 - exp(-exp(z))
         ) %>% 
  gather(link_function, mean_target, -z) %>% 
  ggplot(aes(z, mean_target, color = link_function)) +
  geom_line() + 
  theme_bw() + 
  xlab("Z = Linear Predictor") + 
  ylab("Predicted Probability") + 
  theme(legend.position = "top")
```

- <span style="color:#85C1E9 ">Logit: </span> Most commonly used; default in R; canonical link for the binomial distribution.
- <span style="color:purple ">Probit: </span> Sharper curves than the other links which may have best performance for certain data; Inverse CDF of a standard normal distribution makes it easy to explain.
- <span style="color:red ">Cauchit: </span> Very gradual curves may be best for certain data;  CDF for the standard Cauchy distribution which is a t distribution with one degree of freedom.
- <span style="color:green">Complimentary Log-Log (cloglog)</span> Asymmetric; Important in survival analysis (not on this exam).


## Interpretation of coefficients

Interpreting the coefficients in classification is trickier than in classification because the result must always be within $(0,1)$.  

### Logit

The link function $log(\frac{p}{1-p})$ is known as the log-odds, where the odds are $\frac{p}{1-p}$.  These come up in gambling, where bets are placed on the odds of some event occurring. For example: if the probability of a claim is $p = 0.8$, then the probability of no claim is 0.2 and the odds of a claim occurring are 0.8/0.2 = 4.  

The transformation from probability to odds is monotonic.  This is a fancy way of saying that if $p$ increases, then the odds of $p$ increases as well, and vice versa if $p$ decreases.  The log transform is monotonic as well.  

The net result is that when a variable increases the linear predictor, this increases the log odds, increasing the log of the odds, and vice versa if the linear predictor decreases. In other words, the signs of the coefficients indicate whether the variable increases or decreases the probability of the event.

### Probit, Cauchit, Cloglog

These link functions are still monotonic, so the sign of the coefficients can be interpreted to mean that the variable has a positive or negative impact on the target.

More extensive interpretation is not straightforward. In the case of the Probit, instead of dealing with the log-odds function, we have the inverse CDF of a standard Normal distribution (a.k.a., a Gaussian distribution with mean 0 and variance 1). There is no way of taking this inverse directly.


## Demo the model for interpretation

For uglier link functions, we can rely on trial-and-error to interpret the result. We will call this the **“model-demo method**, " which, as the name implies, involves running example cases and seeing how the results change.

This method works not only for categorical GLMs, but any other type of models such as a continuous GLM, GBM, or random forest.

See the example from **SOA PA 12/12/19** below to learn how this works.



## Example - Auto Claims

Using the `auto_claim` data, we predict whether or not a policy has a claim.  This is also known as the *claim frequency*.

```{r}
auto_claim %>% count(CLM_FLAG)
```

About 40% do not have a claim while 60% have at least one claim.

```{r}
set.seed(42)
index <- createDataPartition(y = auto_claim$CLM_FLAG, 
                             p = 0.8, list = F) %>% as.numeric()
auto_claim <- auto_claim %>% 
  mutate(target = as.factor(ifelse(CLM_FLAG == "Yes", 1,0)))
train <-  auto_claim %>% slice(index)
test <- auto_claim %>% slice(-index)

frequency <- glm(target ~ AGE + GENDER + MARRIED + CAR_USE + 
                   BLUEBOOK + CAR_TYPE + AREA, 
                 data=train, 
                 family = binomial(link="logit"))
```

All of the variables except for the `CAR_TYPE` and `GENDERM` are highly significant.  The car types `SPORTS CAR` and `SUV` appear to be significant, and so if we wanted to make the model simpler we could create indicator variables for `CAR_TYPE == SPORTS CAR` and `CAR_TYPE == SUV`.

```{r}
frequency %>% summary()
```

The signs of the coefficients tell if the probability of having a claim is either increasing or decreasing by each variable. For example, the likelihood of an accident

* Decreases as the age of the car increases
* Is lower for men 
* Is higher for sports cars and SUVs

The p-values tell us if the variable is significant.

- `Age`, `MarriedYes`, `CAR_USEPrivate`, `BLUEBOOK`, and `AreaUrban` are significant.
- Certain values of `CAR_TYPE` are significant but others are not.

The output is a predicted probability.  We can see that this is centered around a probability of about 0.3.  

```{r fig.cap="Distribution of Predicted Probability", fig.width=5, fig.height=3, message = F}
preds <- predict(frequency, newdat=test,type="response")
qplot(preds) 
```

In order to convert these values to predicted 0's and 1's, we assign a *cutoff* value so that if $\hat{y}$ is above this threshold we use a 1 and 0 otherwise. The default cutoff is 0.5. We change this to 0.3 and see that there are 763 policies predicted to have claims.

```{r}
test <- test %>% mutate(pred_zero_one = as.factor(1*(preds>.3)))
summary(test$pred_zero_one)
```

How do we decide on this cutoff value? We need to compare cutoff values based on some evaluation metrics. For example, we can use *accuracy*.

$$\text{Accuracy} = \frac{\text{Correct Guesses}}{\text{Total Guesses}}$$

This results in an accuracy of 70%.  But is this good?

```{r}
test %>% summarise(accuracy = mean(pred_zero_one == target))
```

Consider what would happen if we just predicted all 0's.  The accuracy is 74%.

```{r}
test %>% summarise(accuracy = mean(0 == target))
```

For policies which experience claims the accuracy is 63%.

```{r}
test %>% 
  filter(target == 1) %>% 
  summarise(accuracy = mean(pred_zero_one == target))
```

But for policies that don't actually experience claims this is 72%.  

```{r}
test %>% 
  filter(target == 0) %>% 
  summarise(accuracy = mean(pred_zero_one == target))
```

How do we know if this is a good model? We can repeat this process with a different cutoff value and get different accuracy metrics for these groups. Let us use a cutoff of 0.6.

75%

```{r}
test <- test %>% mutate(pred_zero_one = as.factor(1*(preds>.6)))
test %>% summarise(accuracy = mean(pred_zero_one == target))
```

10% for policies with claims and 98% for policies without claims.  

```{r}
test %>% 
  filter(target == 1) %>% 
  summarise(accuracy = mean(pred_zero_one == target))

test %>% 
  filter(target == 0) %>% 
  summarise(accuracy = mean(pred_zero_one == target))
```

The punchline is that the accuracy depends on the cutoff value, and changing the cutoff value changes whether the model is accurate for the “true = 1” classes (policies with actual claims) vs. the “false = 0” classes (policies without claims).

# Classification metrics

For regression problems, when the output is a whole number, we can use the sum of squares $\text{RSS}$, the r-squared $R^2$, the mean absolute error $\text{MAE}$, and the likelihood.  For classification problems we need to a new set of metrics.  
  
A *confusion matrix* shows is a table that summarizes how the model classifies each group.

- No claims and predicted to not have claims - **True Negatives (TN) = 1,489**
- Had claims and predicted to have claims - **True Positives (TP) = 59**
- No claims but predicted to have claims - **False Positives (FP) = 22**
- Had claims but predicted not to - **False Negatives (FN) = 489**

```{r}
confusionMatrix(test$pred_zero_one,factor(test$target))$table
```

These definitions allow us to measure performance on the different groups.

*Precision* answers the question "out of all of the positive predictions, what percentage were correct?"

$$\text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}$$

*Recall* answers the question "out of all of positive examples in the data set, what percentage were correct?"

$$\text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}$$

The choice of using precision vs. recall depends on the relative cost of making an FP or an FN error. If FP errors are expensive, then use precision; if FN errors are expensive, then use recall.

**Example A:** the model is trying to detect a deadly disease, which only 1 out of every 1,000 patients survives without early detection. Then the goal should be to optimize **recall** because we would want every patient that has the disease to get detected.

**Example B:** the model is detecting which emails are spam or not. If an important email is flagged as spam incorrectly, the cost is 5 hours of lost productivity. In this case, **precision** is the main concern.

In some cases, we can compare this “cost” in actual values. For example, if a federal court is predicting if a criminal will recommit or not, they can agree that “1 out of every 20 guilty individuals going free” in exchange for “90% of those who are guilty being convicted”. 

A dollar amount can be used when money is involved: flagging non-spam as spam may cost $100, whereas missing a spam email may cost $2. Then the cost-weighted accuracy is


$$\text{Cost} = (100)(\text{FN}) + (2)(\text{FP})$$

The cutoff value can be tuned in order to find the minimum cost.

Fortunately, all of this is handled in a single function called `confusionMatrix`.

```{r}
confusionMatrix(test$pred_zero_one,factor(test$target))
```

## Area Under the ROC Curve (AUC)

What if we look at both the true-positive rate (TPR) and false-positive rate (FPR) simultaneously? That is, for each value of the cutoff, we can calculate the TPR and TNR. 

For example, say that we have 10 cutoff values, $\{k_1, k_2, ..., k_{10}\}$.  Then for each value of $k$ we calculate both the true positive rates

$$\text{TPR} = \{\text{TPR}(k_1), \text{TPR}(k_2), .., \text{TPR}(k_{10})\} $$ 

and the true negative rates

$$\{\text{FNR} = \{\text{FNR}(k_1), \text{FNR}(k_2), .., \text{FNR}(k_{10})\}$$

Then we set `x = TPR` and `y = FNR` and graph x against y. The resulting plot is called the **Receiver Operator Curve (ROC)** and the the Area Under the Curve is called the AUC.

<iframe width="560" height="315" src="https://www.youtube.com/embed/4jRBRDbJemM" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

You can also think of AUC as being a **probability.**  Unlike a conventional probability, this ranges between 0.5 and 1 instead of 0 and 1. In the Logit example, we were predicting whether or not an auto policy would file a claim. Then you can interpret the AUC as

- The expected proportion of positives ranked before a uniformly drawn random negative

- The probability that a model prediction for a policy that filed a claim is greater than the model prediction for a policy that did not file a claim

- The expected true positive rate if the ranking is split just before a uniformly drawn random negative.

- The expected proportion of negatives ranked after a uniformly drawn random positive.

- The expected false positive rate if the ranking is split just after a uniformly drawn random positive.

You can save yourself time by memorizing these three scenarios:

$$ \text{AUC} = 1.0 $$

```{r echo = F, fig.align="center", warning=F, out.width="50%"}
knitr::include_graphics("images/auc_one.png")
```


This is a perfect model that predicts the correct class for new data each time. It will have a ROC plot showing the curve approaching the top left corner so that the square area is 1.0.

$$ \text{AUC} = 0.5 $$*

```{r echo = F, fig.align="center", warning=F, out.width="50%"}
knitr::include_graphics("images/auc_point_five.png")
```

When the ROC curve runs along the diagonal, then the area is 0.5. This performance is no better than randomly selecting the class for new data such that the proportions of each class match that of the data.

$$ \text{AUC} < 0.5 $$
Any model having an AUC less than 0.5 means providing predictions that are worse than random selection, with a near 0 AUC indicating that the model makes the wrong classification almost every time. This can occur in two ways

1)	The model is overfitting. For example, the AUC on the train data set may be higher than 0.8 but only 0.2 on the test data set. This indicates that you need to adjust the parameters of your model. See the chapter on the Bias-Variance Tradeoff.

2)	There is an error in the AUC calculation or model prediction.


## Example - Auto Claims

Let's create an ROC curve and find the AUC for our logit.  

```{r message=F, fig.cap="AUC for auto_claim"}
library(pROC)
roc(test$target, preds, plot = T)
```

If we just randomly guess, the AUC would be 0.5, represented by the 45-degree line. A perfect model would maximize the curve to the upper-left corner.

The AUC of 0.76 is decent. If we had multiple models, we could compare them based on the AUC.  

In general, AUC is preferred over Accuracy when there are many more “true” classes than “false” classes, which is known as having *class imbalance*. An example is bank fraud detection: 99.99% of bank transactions are “false” or “0” classes, and so optimizing for accuracy alone will result in a low sensitivity for detecting actual fraud.


## Example: SOA HR, Task 5

The following question is from the Hospital Readmissions sample project from 2018.

<iframe src="https://player.vimeo.com/video/467845050?title=0&byline=0&portrait=0" width="640" height="360" frameborder="0" allow="autoplay; fullscreen" allowfullscreen></iframe>

Already enrolled?  Watch the full video: <a target="_parent" href="https://course.exampa.net/mod/page/view.php?id=202
">Practice Exams</a> | <a target="_parent" href="https://course.exampa.net/mod/page/view.php?id=153
">Practice Exams + Lessons</a>

>"With the target variable being only 0 or 1, the binomial distribution is the only reasonable choice. Your assistant has done some research and learned that for the glm package in R, five link functions could be used with the binomial distribution. They are shown below (the inverse of the link function is presented here as it represents how the linear predictor is transformed into the actual response), where $\nu$ is the linear predictor and $\p$ is the response.

```{r echo = F, fig.align="center", warning=F, message = F, out.width="1200%"}
knitr::include_graphics("images/hospital_readmissions_link.png")
```

## Example: SOA PA 12/12/19, Task 11

<iframe src="https://player.vimeo.com/video/467848221?title=0&byline=0&portrait=0" width="640" height="360" frameborder="0" allow="autoplay; fullscreen" allowfullscreen></iframe>

Already enrolled?  Watch the full video: <a target="_parent" href="https://course.exampa.net/mod/page/view.php?id=206
">Practice Exams</a> | <a target="_parent" href="https://course.exampa.net/mod/page/view.php?id=157
">Practice Exams + Lessons</a>

>Marketing has asked to demonstrate how your model is to be used with examples of cases that predict high value and cases that predict low value. Your assistant has prepared some sample cases that can be run through your model. You may need to adjust some of them to obtain illustrative examples of interest in marketing.

>Write, in language appropriate for marketing, the illustration and demonstration they are looking for. This demonstration should be more detailed than what will go into your executive summary (including an example).

>The sample cases are provided here and in your report template if you wish to include them in your report.

probability of each case. You need to create the column **Prob of high** using your GLM. Then you assign each case as being “High” or “Low” depending on if this value is above the cutoff.

The values that change are in bold.

|age|education num|marital status    |occupation|cap_ga in|hours_per week|score|**Prob of high**|**Value**|
|---|-------------|------------------|----------|---------|--------------|-----|------------|-----|
|39 |10           |Married-spouse    |Group 3   |0        |40            |60   |0.32     |High |
|39 |10           |**Never-married**    |Group 3   |0        |40            |60   |0.24      |**Low** |
|39 |**5**           |Married-spouse    |Group 3   |0        |40            |60   |0.39       |High |
|39 |10           |Married-spouse    |**Group 5**   |0        |40            |60   |0.80     |High |
|39 |10           |Married-spouse    |Group 3   |0        |**30**            |60   |0.18        |**Low**  |

You can interpret this as:

- The typical profitable customer is middle aged (39 years old), has 10 years of education, is married, and in group 3
- Having never been married decreases profitability
- Being less educated does not decreases profitability.  You can see this because the customer with `education_num = 5` has the same characteristics as the first customer
- Being in Group 5 increases profitability.  You can see this because `Prob of high` increases to 0.8.
- Working fewer than 40 hours per week decreases profitability

This was a difficult question. “Do not be afraid to be assertive and think creatively or to change the values that they give you,” the solution of SOA say.

>Many candidates struggled with this task. Candidates needed to include sample cases that resulted in low and high-value predictions and clearly describe the analysis for the marketing team.

>Candidates were encouraged to modify the supplied cases. Few elected to test changes in both directions from the base case.

## Additional reading

| Title | Source           |
|---------|-----------------|
| An Overview of Classification   | ISL 4.1 |
| [Understanding AUC - ROC Curv](https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5#:~:targetText=What%20is%20AUC%20%2D%20ROC%20Curve%3F,capable%20of%20distinguishing%20between%20classes.)| Sarang Narkhede, Towards Data Science   |
| [Precision vs. Recall](https://towardsdatascience.com/precision-vs-recall-386cf9f89488#:~:targetText=Precision%20and%20recall%20are%20two,correctly%20classified%20by%20your%20algorithm.)     | Shruti Saxena, Towards Data Science    |


# Additional GLM topics

As you can tell, PA has a lot of small topics related to GLMs.  This chapter completes some of the *residual* (no pun intended) topics.  

## Residuals

Learning from mistakes is the path to improvement. For GLMs, the residual analysis looks for patterns in the errors to find ways of improving the model.

<iframe width="563" height="383" src="https://www.youtube.com/embed/9T0wlKdew6I" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

### Raw residuals

The word “residual” by itself means the “raw residual” in GLM language. This is the difference in actual vs. predicted values.

$$\text{Raw Residual} = y_i - \hat{y_i}$$

### Deviance residuals

This is not meant for GLMs with non-Gaussian distributions. To adjust for other distributions, we need the concept of deviance residuals.

<iframe width="563" height="383" src="https://www.youtube.com/embed/JC56jS2gVUE" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Deviance is a way of assessing the adequacy of a model by comparing it with a more general model with the maximum number of parameters that can be estimated. It is referred to as the saturated model and it has one parameter per observation. 

The deviance assesses the goodness of fit for the model by looking at the difference between the log-likelihood functions of the saturated model and the model under investigation, i.e. $l(b_{sat},y) - l(b,y)$. Here sat $b_{sat}$ denotes the maximum likelihood
estimator of the parameter vector of the saturated model, $\beta_{sat}$ , and $b$ is the maximum
likelihood estimator of the parameters of the model under investigation, $\beta$. The maximum likelihood estimator is the estimator that maximizes the likelihood function.  **The deviance is defined as**

$$D = 2[l(b_{sat},y) - l(b,y)]$$

The deviance residual uses the deviance of the ith observation $d_i$ and then takes the square root and applies the same sign (aka, the + or - part) of the raw residual.

$$\text{Deviance Residual} = \text{sign}(y_i - \hat{y_i})\sqrt{d_i}$$

## Example

Just as with OLS, there is a `formula` and `data argument`.  In addition, we need to specify the target distribution and link function.

```{r}
model = glm(formula = charges ~ age + sex + smoker, 
            family = Gamma(link = "log"),
            data = health_insurance)
```

We see that `age`, `sex`, and `smoker` are all significant (p <0.01).  Reading off the coefficient signs, we see that claims

- Increase as age increases
- Are higher for women
- Are higher for smokers

```{r}
model %>% tidy()
```

Below you can see graph of deviance residuals vs. the predicted values.

**If this were a perfect model, all of these below assumptions would be met:**

- Scattered around zero? 
- Constant variance? 
- No obvious pattern? 

```{r}
plot(model, which = 3)
```

The quantile-quantile (QQ) plot shows the quantiles of the deviance residuals (i.e., after adjusting for the Gamma distribution) against theoretical Gaussian quantiles.  

**In a perfect model, all of these assumptions would be met:**

- Points lie on a straight line?  
- Tails are not significantly above or below line?  Some tail deviation is ok.
- No sudden "jumps"?  This indicates many $Y$'s which have the same value, such as insurance claims which all have the exact value of \$100.00 or $0.00.

```{r}
plot(model, which = 2)
```

## Log transforms of predictors

When a log link is used, taking the natural logs of continuous variables allows for the scale of each predictor to match the scale of the thing that they are predicting, the log of the mean of the response. In addition, when the distribution of the continuous variable is skewed, taking the log helps to make it more symmetric.

After taking the log of a predictor, the interpretation becomes a power transform of the original variable.


For $\mu$ the mean response,

$$log(\mu) = \beta_0 + \beta_1 log(X)$$
To solve for $\mu$, take the exponent of both sides

$$\mu = e^{\beta_1} e^{\beta_1 log(X)} = e^{\beta_0} X^{\beta_1}$$
### Example

In the Hospital Readmission sample project, one of the predictor variables, “Length of stay”, is the number of days since a person has been readmitted to the hospital. You can tell that it is right-skewd because the median is higher than the mean.

```{r}
summary(readmission$LOS)
```

But it could also be thought of as a discrete variable because it only takes on 36 values. **Should you still apply a log transform?**

```{r}
readmission %>% count(LOS)
```

Here are the histograms

```{r echo = F, warning=F, message = F, fig.height=4}
library(ggpubr)
p1 <- readmission %>% ggplot(aes(LOS)) + geom_histogram() + ggtitle("Without Log Transform")
p2 <- readmission %>% ggplot(aes(log(LOS))) + geom_histogram() + ggtitle("With Log Transform")
ggarrange(p1,p2)
```

**Yes**, the SOA's solution applys the log transform.

## Reference levels

When a categorical variable is used in a GLM, the model actually uses indicator variables for each level.  The default reference level is the order of the R factors.  For the `sex` variable, the order is `female` and then `male`.  This means that the base level is `female` by default.

```{r}
health_insurance$sex %>% as.factor() %>% levels()
```

Why does this matter?  Statistically, the coefficients are most stable when there are more observations.

```{r}
health_insurance$sex %>% as.factor() %>% summary()
```

There is already a function to do this in the `tidyverse` called `fct_infreq`.  Let's quickly fix the `sex` column so that these factor levels are in order of frequency.

```{r}
health_insurance <- health_insurance %>% 
  mutate(sex = fct_infreq(sex))
```

Now `male` is the base level.

```{r}
health_insurance$sex %>% as.factor() %>% levels()
```

## Interactions

An interaction occurs when the effect of a variable on the response is different depending on the level of other variables in the model.

Consider this model:


Let $x_2$ be an indicator variable, which is 1 for some observations and 0 otherwise.  

$$\hat{y_i} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2$$

There are now two different linear models depending on whether `x_1` is 0 or 1.

When $x_1 = 0$,

$$\hat{y_i} = \beta_0  + \beta_2 x_2$$

and when $x_1 = 1$

$$\hat{y_i} = \beta_0 + \beta_1 + \beta_2 x_2 + \beta_3 x_2$$
By rewriting this we can see that the intercept changes from $\beta_0$ to $\beta_0^*$ and the slope changes from $\beta_1$ to $\beta_1^*$

$$
(\beta_0 + \beta_1) + (\beta_2 + \beta_3 ) x_2 \\
 = \beta_0^* + \beta_1^* x_2
$$
Here is an example from the `auto_claim` data. The lines show the slope of a linear model, assuming that only `BLUEBOOK` and `CAR_TYPE` were predictors in the model.  You can see that the slope for Sedans and Sports Cars is higher than for Vans and Panel Trucks.  

```{r fig.cap="Example of strong interaction" ,warning = F}
auto_claim %>% 
  sample_frac(0.2) %>% 
  ggplot(aes(log(CLM_AMT), log(BLUEBOOK), color = CAR_TYPE)) + 
  geom_point(alpha = 0.3) + 
  geom_smooth(method = "lm", se = F) + 
  labs(title = "Kelly Bluebook Value vs Claim Amount")
```

Any time that effect of one variable on the response is different depending on the value of other variables, we say that there is an interaction. We can also use a hypothesis test with a GLM to check this. Simply include an interaction term and see if the coefficient is zero at the desired significance level.


## Offsets

In certain situations, it is convenient to include a constant term in the linear predictor. This is the same as including a variable that has a coefficient equal to 1. We call this an *offset*.


$$g(\mu) = \beta_0 + \beta_1 X_1 + ... + \beta_p X_p + \text{offset}$$
On Exam PA, offsets will only be used for one special case:

1) With Poisson regression
2) With a log link function
3) As a measure of exposure (usually length of policy period)

While it is technically possible to use offsets in other ways, this is not likely to appear on PA.

If modeling the spread of COVID, the exposure would be the number of people exposed to the virus and the response would be the number of people infected.

In auto insurance, the exposure might be the number of months of coverage, and the response would be the claims incurred. Consider a very simple model which only uses the year that the car was manufactured as a predictor. This expected value of the claims, the target variable, would be


$$log(E[\frac{\text{Claims}}{\text{Months}}]) = \beta_0 + \beta_1  \text{Year}$$
Then you can use the property of the log where $log(\frac{A}{B}) = log(A) - log(B)$ to move things around.  Because $\text{Months}$ is known, you can remove the expected value.  This is the offset term.


$$log(E[\text{Claims}]) = \beta_0 + \beta_1  \text{Year} + \text{Months}$$

## Tweedie regression

While this topic is briefly mentioned in the modules, the only R libraries which support Tweedie Regression (`statmod` and `tweedie`) are not on the syllabus, and so there is no way that the SOA could ask you to build a tweedie model. This means that you can safely skip this section.

## Combinations of Link Functions and Target Distributions

What is an example of when to use a log link with a Gaussian response? What about a Gamma family with an inverse link? What about an inverse 

Gaussian response and an inverse square link? As these questions illustrate, there are many combinations of link and response families. In the real world, a model never fits perfectly, and so often, these choices come down to the judgment of the modeler - which model is the best fit and meets the business objectives?

However, there is one way that we can know for certain which link and response family is the best, and that is if we generate the data ourselves.

Recall that a GLM has two parts:


1. A **random component**: $Y|X \sim \text{some exponential family distribution}$

2. A **link function**: between the random component and the covariates: $g(\mu(X)) = X\beta$ where $\mu = E[Y|X]$

**Following this recipe, we can simulate data from any combination of link function and response family.  This helps us to understand the GLM framework very clearly.**

### Gaussian Response with Log Link

We create a function that takes in data $x$ and returns a Gaussian random variable that has mean equal to the inverse link, which in the case of a log link is the exponent.  We add 10 to $x$ so that the values will always be positive, as will be described later on.

```{r}
sim_norm <- function(x) {
  rnorm(1, mean = exp(10 + x), sd = 1)
}
```

The values of $X$ do not need to be normal.  The above assumption is merely that the mean of the response $Y$ is related to $X$ through the link function, `mean = exp(10 + x)`, and that the distribution is normal.  This has been accomplished with `rnorm` already.  For illustration, here we use $X$'s from a uniform distribution.

```{r}
data <- tibble(x = runif(500)) %>% 
  mutate(y = x %>% map_dbl(sim_norm))
```

We already know what the answer is: a Gaussian response with a log link.  We fit a GLM and see a perfect fit.

```{r}
glm <- glm(y ~ x, family = gaussian(link = "log"), data = data)

summary(glm)
par(mfrow = c(2,2))
plot(glm, cex = 0.4)
```

### Gaussian Response with Inverse Link

The same steps are repeated except the link function is now the inverse, `mean = 1/x`.  We see that some values of $Y$ are negative, which is ok.

```{r warning = F}
sim_norm <- function(x) {
  rnorm(1, mean = 1/x, 1)
}

data <- tibble(x = runif(500)) %>% 
  mutate(y = x %>% map_dbl(sim_norm))
summary(data)
```

```{r}
glm <- glm(y ~ x, family = gaussian(link = "inverse"), data = data)

summary(glm)
par(mfrow = c(2,2))
plot(glm, cex = 0.4)
```

### Gaussian Response with Identity Link

And now the link is the identity, `mean = x`.

```{r}
sim_norm <- function(x) {
  rnorm(1, mean = x, 1)
}

data <- tibble(x = rnorm(500)) %>% 
  mutate(y = x %>% map_dbl(sim_norm))

glm <- glm(y ~ x, family = gaussian(link = "identity"), data = data)

summary(glm)
par(mfrow = c(2,2))
plot(glm, cex = 0.4)
```

### Gaussian Response with Log Link and Negative Values

By Gaussian response we say that the *mean* of the response is Gaussian.  The range of a normal random variable is $(-\infty, +\infty)$, which means that negative values are always possible. If the mean is a large positive number, then negative values are much less likely but still possible: about 95% of the observations will be within 2 standard deviations of the mean.

We see below that there are some $Y$ values which are negative.

```{r}
sim_norm <- function(x) {
  rnorm(1, mean = exp(x), sd = 1)
}

data <- tibble(x = runif(500)) %>% 
  mutate(y = x %>% map_dbl(sim_norm))
summary(data)
```

We can also see this from the histogram.

```{r fig.height=3, fig.align="center"}
data %>% ggplot(aes(y)) + geom_density( fill = 1, alpha = 0.3)
```

If we try to fit a GLM with a log link, there is an error.  

```{r eval = F}
glm <- glm(y ~ x, family = gaussian(link = "log"), data = data)
```

`Error in eval(family$initialize) : cannot find valid starting values: please specify some`

This is because the domain of the natural logarithm only includes positive numbers, and we just tried to take the log of negative numbers.

Our initial reaction might be to add some constant to each $Y$, say 10, for instance, so that they are all positive. This does produce a model which is a good fit.


```{r}
glm <- glm(y + 10 ~ x, family = gaussian(link = "log"), data = data)
summary(glm)
par(mfrow = c(2,2))
plot(glm, cex = 0.4)
```

We see that on average, the predictions are 10 higher than the target.  This is no surprise since $E[Y + 10] = E[Y] + 10$.

```{r}
y <- data$y 
y_hat <- predict(glm, type = "response")
mean(y_hat) - mean(y)
```

However, we see that the actual predictions are bad. If we were to look at the R-squared, MAE, RMSE, or any other metric, it would tell us the same story. This is because our GLM assumption **not** that $Y$ is related to the link function of $X$, but that the **mean** of $Y$ is.

```{r}
tibble(y = y, y_hat = y_hat - 10) %>% ggplot(aes(y, y_hat)) + geom_point()
```

One solution is to adjust the $X$ which the model is based on.  Add a constant term to $X$ so that the mean of $Y$ is larger, and hence $Y$ is non zero.  While is a viable approach in the case of only one predictor variable, with more predictors this would not be easy to do.

```{r}
data <- tibble(x = runif(500) + 10) %>% 
  mutate(y = x %>% map_dbl(sim_norm))
summary(data)
glm <- glm(y ~ x, family = gaussian(link = "log"), data = data)
par(mfrow = c(2,2))
plot(glm, cex = 0.4)
```

A better approach may be to use an inverse link even though the data was generated from a log link. This is a good illustration of the saying “all models are wrong, but some are useful” in that the statistical assumption of the model is not correct but the model still works.

```{r}
data <- tibble(x = runif(500)) %>% 
  mutate(y = x %>% map_dbl(sim_norm))
glm <- glm(y ~ x, family = gaussian(link = "inverse"), data = data)
par(mfrow = c(2,2))
plot(glm, cex = 0.4)
summary(glm)
```

### Gamma Response with Log Link

The gamma distribution with rate parameter $\alpha$ and scale parameter $\theta$ is density.

$$f(y) = \frac{(y/\theta)^\alpha}{x \Gamma(\alpha)}e^{-x/\theta}$$

The mean is $\alpha\theta$.

Let's use a gamma with shape 2 and scale 0.5, which has mean 1.  

```{r}
gammas <- rgamma(500, shape=2, scale = 0.5)
mean(gammas)
```

We then generate random gamma values. Because the mean now depends on two parameters instead of one, which was just $\mu$ in the Gaussian case, we need to use a slightly different approach to simulate the random values. The link function here is seen in `exp(x)`.

```{r}
#random component
x <- runif(1000, min=0, max=100)

#relate Y to X with a log link function
y <- gammas*exp(x)

data <- tibble(x = x, y  = y)
summary(data)
```

As expected, the residual plots are all perfect because the model is perfect.

```{r}
glm <- glm(y ~ x, family = Gamma(link = "log"), data = data)
par(mfrow = c(2,2))
plot(glm, cex = 0.4)
```

If we had tried using an inverse instead of the log, the residual plots would look much worse.

```{r}
glm <- glm(y ~ x, family = Gamma(link = "inverse"), data = data)
par(mfrow = c(2,2))
plot(glm, cex = 0.4)
```


### Gamma with Inverse Link

With the inverse link, the mean has a factor `1/(x + 1)`.  Note that we need to add 1 to x to avoid dividing by zero.

```{r}
#relate Y to X with a log link function
y <- gammas*1/(x + 1)

data <- tibble(x = x, y  = y)
summary(data)
```

```{r}
glm <- glm(y ~ x, family = Gamma(link = "inverse"), data = data)
par(mfrow = c(2,2))
plot(glm, cex = 0.4)
```

# GLM variable selection

Predictive Analytics is about using results to solve business problems. Complex models are almost useless if they cannot be explained. This chapter will explain how to make GLMs easier to explain by either removing variables entirely or lessening their impact.

## Stepwise subset selection

In theory, we could test all possible combinations of variables and interaction terms. This includes all $p$ models with one predictor, all p-choose-2 models with two predictors, all p-choose-3 models with three predictors, and so on. Then we take whichever model has the best performance as the final model.

This “brute force” approach is statistically ineffective: the more searched, the higher the chance of finding models that overfit.
A subtler method, known as the stepwise selection, reduces the chances of over-fitting by only looking at the most promising models.
  

**Forward Stepwise Selection:**

1. Start with no predictors in the model;
2. Evaluate all $p$ models which use only one predictor and choose the one with the best performance (highest $R^2$ or lowest $\text{RSS}$);
3. Repeat the process when adding one additional predictor, and continue until there is a model with one predictor, a model with two predictors, a model with three predictors, and so forth until there are $p$ models;
4. Select the single best model which has the best $\text{AIC}$,$\text{BIC}$, or adjusted $R^2$.

**Backward Stepwise Selection:**

1. Start with a model that contains all predictors;
2. Create a model which removes all predictors;
3. Choose the best model which removes all-but-one predictor;
4. Choose the best model which removes all-but-two predictors;
5. Continue until there are $p$ models;
6. Select the single best model which has the best $\text{AIC}$,$\text{BIC}$, or adjusted $R^2$.

**Both Forward & Backward Selection:**

A hybrid approach is to consider using both forward and backward selection. This is done by creating two lists of variables at each step, one from forward and backward selection. 

Then variables from both lists are tested to see if adding or subtracting from the current model would improve the fit or not. ISLR does not mention this directly. However, by default, the `stepAIC` function uses a default of `both`.

>**Tip**: Always load the `MASS` library before `dplyr` or `tidyverse`.  Otherwise there will be conflicts as there are functions named `select()` and `filter()` in both.  Alternatively, specify the library in the function call with `dplyr::select()`.

| Readings |  | 
|-------|---------|
| [CAS Monograph 5 Chapter 2](https://contentpreview.s3.us-east-2.amazonaws.com/CAS+Monograph+5+-+Generalized+Linear+Models+for+Insurance+Ratemaking.pdf) |  |

## Example: SOA PA 6/12/19, Task 6

<iframe src="https://player.vimeo.com/video/467840434?title=0&byline=0&portrait=0" width="640" height="360" frameborder="0" allow="autoplay; fullscreen" allowfullscreen></iframe>

Already enrolled?  Watch the full video: <a target="_parent" href="https://course.exampa.net/mod/page/view.php?id=204
">Practice Exams</a> | <a target="_parent" href="https://course.exampa.net/mod/page/view.php?id=155
">Practice Exams + Lessons</a>

> AIC and BIC are among the available techniques for feature selection. Briefly describe them and outline the differences in the two criteria. Make a recommendation as to which one should be used for this problem. Use only your recommended criterion when completing this task.

>Some of the features may lack predictive power and lead to overfitting. Determine which features should be retained. Use the stepAIC function (from the MASS package) to make this determination. When using this function, there are two decisions to make. Make each decision based on the business problem. Use ?stepAIC to learn more about these parameters (note that the MASS package must be loaded before help on this function can be accessed).

>Use direction = “backward” or direction = “forward” Use AIC (k = 2) or BIC (k=log(nrow(train)))


## Penalized Linear Models

One of the main weaknesses of the GLM, including all linear models in this chapter, is that the features need to be selected by hand. Stepwise selection helps to improve this process but fails when the inputs are correlated and often has a strong dependence on seemingly arbitrary choices of evaluation metrics such as using AIC or BIC and forward or backward directions.

The Bias Variance Trade-off is about finding the lowest error by changing the flexibility of the model. Penalization methods use a parameter to control for this flexibility directly.

Earlier on, we said that the linear model minimizes the sum of square terms, known as the residual sum of squares (RSS)


$$
\text{RSS} = \sum_i(y_i - \hat{y})^2 = \sum_i(y_i - \beta_0 - \sum_{j = 1}^p\beta_j x_{ij})^2
$$

This loss function can be modified so that models which include more (and larger) coefficients are considered as worse.  In other words, when there are more $\beta$'s, or $\beta$'s which are larger, the RSS is higher.

## Ridge Regression

Ridge regression adds a penalty term which is proportional to the square of the sum of the coefficients.  This is known as the "L2" norm.

$$
\sum_i(y_i - \beta_0 - \sum_{j = 1}^p\beta_j x_{ij})^2 + \lambda \sum_{j = 1}^p\beta_j^2
$$

<iframe width="563" height="383" src="https://www.youtube.com/embed/Q81RR3yKn30" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

This $\lambda$ controls how much of a penalty is imposed on the size of the coefficients.  When $\lambda$ is high, simpler models are treated more favorably because the $\sum_{j = 1}^p\beta_j^2$ carries more weight.  Conversely, then $\lambda$ is low, complex models are more favored.  When $\lambda = 0$, we have an ordinary GLM.

## Lasso

The official name is the Least Absolute Shrinkage and Selection Operator, but the common name is just “the lasso.” 

Just as with Ridge regression, we want to favor simpler models; however, we also want to select variables. This is the same as forcing some coefficients to be equal to 0.

Instead of taking the square of the coefficients (L2 norm), we take the absolute value (L1 norm).
  

$$
\sum_i(y_i - \beta_0 - \sum_{j = 1}^p\beta_j x_{ij})^2 + \lambda \sum_{j = 1}^p|\beta_j|
$$

<iframe width="563" height="383" src="https://www.youtube.com/embed/NGf0voTMlcs" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

In ISLR, Hastie et al show that this results in coefficients being forced to be exactly 0.  This is extremely useful because it means that by changing $\lambda$, we can select how many variables to use in the model.

**Note**: While any response family is possible with penalized regression, in R, only the Gaussian family is possible in the library `glmnet`, and so this is the only type of question that the SOA can ask.

## Elastic Net

The Elastic Net uses a penalty term which is between the L1 and L2 norms. The penalty term is a weighted average using the mixing parameter $0 \leq \alpha \leq 1$. The loss function is then

$$\text{RSS} + (1 - \alpha) \sum_{j = 1}^{p}\beta_j^2 + \alpha \sum_{j = 1}^p |\beta_j|$$
When $\alpha = 1$, the model is known as the Lasso, and when $\alpha = 0$, the model is known as Ridge Regression. 

<iframe width="560" height="315" src="https://www.youtube.com/embed/PaFPbb66DxQ?rel=0&showinfo=1&playlist=1dKRdX9bfIo,ctmNq7FgbvI" frameborder="0" allowfullscreen></iframe>


Luckily, none of this needs to be memorized.  On the exam, read the documentation in R to refresh your memory.  For the Elastic Net, the function is `glmnet`, and so running `?glmnet` will give you this info.

>**Shortcut**: When using complicated functions on the exam, use `?function_name` to get the documentation.

## Advantages and disadvantages

**Elastic Net/Lasso/Ridge Advantages**

- All benefits from GLMS
- Automatic variable selection for Lasso; smaller coefficients for Ridge
- Better predictive power than GLM

**Elastic Net/Lasso/Ridge Disadvantages**

- All cons of GLMs

| Readings |  | 
|-------|---------|
| ISLR 6.1 Subset Selection  | |
| ISLR 6.2 Shrinkage Methods|  |


```{r, global_options, message=FALSE, warning=FALSE, echo=FALSE}
require(knitr)
opts_chunk$set(warning = F, message = F)
```

## Example: Ridge Regression

```{r include=F, message = F}
library(ISLR)
library(glmnet)
library(dplyr)
library(tidyr)
```

We will use the `glmnet` package in order to perform ridge regression and
the lasso. The main function in this package is `glmnet()`,which can be used to fit ridge regression models, lasso models, and more. 

This function has a slightly different syntax from other model-fitting functions that we have encountered thus far in this book. In particular, we must pass in an $x$ matrix as well as a $y$ vector, and we do not use the $y \sim x$ syntax.

Before proceeding, let us first ensure that the missing values have been removed from the data described in the previous lab.

```{r}
Hitters = na.omit(Hitters)
```

We will now perform ridge regression and the lasso in order to predict `Salary` on
the `Hitters` data. Let's set up our data:

```{r}

x = model.matrix(Salary~., Hitters)[,-1] # trim off the first column
                                         # leaving only the predictors
y = Hitters %>%
  select(Salary) %>%
  unlist() %>%
  as.numeric()
```

The `model.matrix()` function is particularly useful for creating $x$; not only does it produce a matrix corresponding to the 19 predictors but it also automatically transforms any qualitative variables into dummy variables.

The latter property is important because `glmnet()` can only take numerical,
quantitative inputs.

The `glmnet()` function has an alpha argument that determines what type of model is fit. If `alpha = 0` then a ridge regression model is fit, and if `alpha = 1` then a lasso model is fit. We first fit a ridge regression model:

```{r}
grid = 10^seq(10, -2, length = 100)
ridge_mod = glmnet(x, y, alpha = 0, lambda = grid)
```

By default the `glmnet()` function performs ridge regression for an automatically selected range of $\lambda$ values. However, here we have chosen to implement the function over a grid of values ranging from $\lambda = 10^10$ to $\lambda = 10^{-2}$, essentially covering the full range of scenarios from the null model containing
only the intercept, to the least squares fit. 

As we will see, we can also compute model fits for a particular value of $\lambda$ that is not one of the original grid values. Note that by default, the `glmnet()` function standardizes the
variables so that they are on the same scale. To turn off this default setting, use the argument `standardize = FALSE`.

Associated with each value of $\lambda$ is a vector of ridge regression coefficients,
stored in a matrix that can be accessed by `coef()`. In this case, it is a $20 \times 100$
matrix, with 20 rows (one for each predictor, plus an intercept) and 100 columns (one for each value of $\lambda$).

```{r}
dim(coef(ridge_mod))
```

We expect the coefficient estimates to be much smaller, in terms of $l_2$ norm,
when a large value of $\lambda$ is used, as compared to when a small value of $\lambda$ is
used. These are the coefficients when $\lambda = 11498$, along with their $l_2$ norm:

```{r}
ridge_mod$lambda[50] #Display 50th lambda value
coef(ridge_mod)[,50] # Display coefficients associated with 50th lambda value
sqrt(sum(coef(ridge_mod)[-1,50]^2)) # Calculate l2 norm
```

In contrast, here are the coefficients when $\lambda = 705$, along with their $l_2$
norm. Note the much larger $l_2$ norm of the coefficients associated with this
smaller value of $\lambda$.

```{r}
ridge_mod$lambda[60] #Display 60th lambda value
coef(ridge_mod)[,60] # Display coefficients associated with 60th lambda value
sqrt(sum(coef(ridge_mod)[-1,60]^2)) # Calculate l2 norm
```

We can use the `predict()` function for a number of purposes. For instance,
we can obtain the ridge regression coefficients for a new value of $\lambda$, say 50:

```{r}
predict(ridge_mod, s=50, type="coefficients")[1:20,]
```

We now split the samples into a training set and a test set in order to estimate the test error of ridge regression and the lasso.

```{r}
set.seed(1)

train = Hitters %>%
  sample_frac(0.5)

test = Hitters %>%
  setdiff(train)

x_train = model.matrix(Salary~., train)[,-1]
x_test = model.matrix(Salary~., test)[,-1]

y_train = train %>%
  select(Salary) %>%
  unlist() %>%
  as.numeric()

y_test = test %>%
  select(Salary) %>%
  unlist() %>%
  as.numeric()
```

Next we fit a ridge regression model on the training set, and evaluate its MSE on the test set, using $\lambda = 4$. Note the use of the `predict()` function again: this time we get predictions for a test set, by replacing
`type="coefficients"` with the `newx` argument.

```{r}
ridge_mod = glmnet(x_train, y_train, alpha=0, lambda = grid, thresh = 1e-12)
ridge_pred = predict(ridge_mod, s = 4, newx = x_test)
mean((ridge_pred - y_test)^2)
```

The test MSE is 101242.7. If we had instead simply fit a model with just an intercept, we would have predicted each test observation using the mean of the training observations. In that case, we could compute the test set MSE like this:

```{r}
mean((mean(y_train) - y_test)^2)
```

We could also get the same result by fitting a ridge regression model with a very large value of $\lambda$. Note that `1e10` means $10^{10}$.

```{r}
ridge_pred = predict(ridge_mod, s = 1e10, newx = x_test)
mean((ridge_pred - y_test)^2)
```

So fitting a ridge regression model with $\lambda = 4$ leads to a much lower test MSE than fitting a model with just an intercept. We now check whether
there is any benefit to performing ridge regression with $\lambda = 4$ instead of just performing least squares regression. Recall that least squares is simply ridge regression with $\lambda = 0$.

\* Note: In order for `glmnet()` to yield the **exact** least squares coefficients when $\lambda = 0$, we use the argument `exact=T` when calling the `predict()` function. Otherwise, the
`predict()` function will interpolate over the grid of $\lambda$ values used in fitting the
`glmnet()` model, yielding approximate results. Even when we use `exact = T`, there remains
a slight discrepancy in the third decimal place between the output of `glmnet()` when
$\lambda = 0$ and the output of `lm()`; this is due to numerical approximation on the part of
`glmnet()`.

```{r}
ridge_pred = predict(ridge_mod, s = 0, x = x_train, y = y_train, newx = x_test, exact = T)
mean((ridge_pred - y_test)^2)

lm(Salary~., data = train)
predict(ridge_mod, s = 0, x = x_train, y = y_train, exact = T, type="coefficients")[1:20,]
```

IIt looks like we are indeed improving over regular least-squares! Side note: in general, if we want to fit a (unpenalized) least squares model, then we should use the 'lm()' function, since that function provides more useful outputs, such as standard errors and $p$-values for the coefficients.

Instead of arbitrarily choosing $\lambda = 4$, it would be better to use cross-validation to choose the tuning parameter $\lambda$. We can do this using the built-in cross-validation function, `cv.glmnet()`. By default, the function
performs 10-fold cross-validation, though this can be changed using the argument `folds`. Note that we set a random seed first so our results will be
reproducible, since the choice of the cross-validation folds is random.

```{r}
set.seed(1)
cv.out = cv.glmnet(x_train, y_train, alpha = 0) # Fit ridge regression model on training data
plot(cv.out) # Draw plot of training MSE as a function of lambda
bestlam = cv.out$lambda.min  # Select lamda that minimizes training MSE
bestlam
```

Therefore, we see that the value of $\lambda$ that results in the smallest cross-validation
error is 339.1845 What is the test MSE associated with this value of
$\lambda$?

```{r}
ridge_pred = predict(ridge_mod, s = bestlam, newx = x_test) # Use best lambda to predict test data
mean((ridge_pred - y_test)^2) # Calculate test MSE
```

This represents a further improvement over the test MSE that we got using
$\lambda = 4$. Finally, we refit our ridge regression model on the full data set,
using the value of $\lambda$ chosen by cross-validation, and examine the coefficient
estimates.

```{r}
out = glmnet(x, y, alpha = 0) # Fit ridge regression model on full dataset
predict(out, type = "coefficients", s = bestlam)[1:20,] # Display coefficients using lambda chosen by CV
```

As expected, none of the coefficients are exactly zero - ridge regression does not perform variable selection!

## Example: The Lasso

We saw that ridge regression with a wise choice of $\lambda$ can outperform least squares and the null model on the Hitters data set. We now ask whether the lasso can yield either a more accurate or a more interpretable model than ridge regression. In order to fit a lasso model, we once again use the `glmnet()` function; however, this time we use the argument `alpha=1`. Other than that change, we proceed just as we did in fitting a ridge model:

```{r}
lasso_mod = glmnet(x_train, y_train, alpha = 1, lambda = grid) # Fit lasso model on training data
plot(lasso_mod)                                          # Draw plot of coefficients
```

Notice that some of the coefficients are exactly equal to zero in the coefficient plot depending on the choice of the tuning parameter. We now perform cross-validation and compute the associated test error:

```{r}
set.seed(1)
cv.out = cv.glmnet(x_train, y_train, alpha = 1) # Fit lasso model on training data
plot(cv.out) # Draw plot of training MSE as a function of lambda
bestlam = cv.out$lambda.min # Select lamda that minimizes training MSE
lasso_pred = predict(lasso_mod, s = bestlam, newx = x_test) # Use best lambda to predict test data
mean((lasso_pred - y_test)^2) # Calculate test MSE
```

This is substantially lower than the test set MSE of the null model and least squares and very similar to the test MSE of ridge regression with  $\lambda$ chosen by cross-validation.

However, the lasso has a substantial advantage over ridge regression in that the resulting coefficient estimates are sparse. Here we see that 12 of the 19 coefficient estimates are exactly zero:

```{r}
out = glmnet(x, y, alpha = 1, lambda = grid) # Fit lasso model on full dataset
lasso_coef = predict(out, type = "coefficients", s = bestlam)[1:20,] # Display coefficients using lambda chosen by CV
lasso_coef
```

Selecting only the predictors with non-zero coefficients, we see that the lasso model with $\lambda$ chosen by cross-validation contains only seven variables:

```{r}
lasso_coef[lasso_coef!=0] # Display only non-zero coefficients
```

Practice questions:

 * How do ridge regression and the lasso improve on simple least squares?
 * In what cases would you expect ridge regression outperform the lasso, and vice versa?
 
# Bias-variance trade-off

This is a big topic in machine learning in general but only has had a handful of questions on PA. Without stating this explicitly as “the bias-variance tradeoff,” you have already been using this concept. We first need some definitions:

**Mean Squared Error (MSE):**

The sum of the squared difference between the predictions and target.  

**Variance of model:**

The variance of the parameters, $var(f(X))$.  When variance is high, the model is often overfitting.

**Bias:**

The difference between the expected value of the estimate and the actual expected value. When the bias is high, the model is under fitting and is not complex enough to capture the signal in the data.

$$\text{Model Bias} = E(Y) - f(X)$$

```{r echo = F, fig.align="center", warning=F, out.width="100%"}
knitr::include_graphics("images/over-under-fitting.PNG")
```


**Irreducible Error:**

Random noise in the data that can never be understood. This is “irreducible,” meaning that the model cannot reduce it, but you can reduce it by cleaning the data, transforming variables, and engineering additional features.

The Bias-variance trade-off says that when the bias of the parameter estimates increases, the variance decreases, and vice versa as the bias decreases.
 

$$\text{MSE} = \text{Variance of model} + \text{Bias}^2 + \text{Irreducible Error}$$
Your goal is to make the MSE as small as possible. When you test different models, tune parameters, and perform shrinkage or variable selection, you change the bias and the variance.

A helpful way to remember this relationship is with the following picture. Imagine that you are at a shooting range and am firing a pistol at a target. Your goal is to get as close to the center of the bullseye as possible.

Ideally, your bullets would have low bias and low variance (upper left). This would mean that you consistently hit the center of the target. In the worst case, your bullets would have high bias and high variance (lower right). You would be changing your aim between shots and would not be centered at the bullseye.

The other diagonals (lower left and upper right) are the more common outcomes. Either you keep your arm steady and do not change your aim between shots but miss the center, or you move around too much and have high variance.
  

```{r echo = F, fig.align="center", warning=F, out.width="100%"}
knitr::include_graphics("images/bias-variance-target.PNG")
```

You can decrease the variance by using more data. From Exam P, you may remember that the variance of the sample means decreases as the square root of $N$, the sample size, increases.  To decrease the bias, you can change the type of model being used.

Model **flexibility** is the amount that the model can change. The easiest way to understand flexibility is in the case of the linear model. A GLM with 1 predictor has low flexibility. 

A GLM with 100 predictors has high flexibility. This is a general definition because you technically need to consider the size of the coefficients as well. It is easy to confuse flexibility with the variance of model, but the two concepts are different. In this GLM example, the variance would be determined by the standard errors on the coefficients. 

A model with high variance would have large p-values, but this could still be inflexible if only a few predictors are included. Conversely, a model could have high flexibility by having many predictor variables and interaction terms but have low variance if all of the p-values were small.

Your goal of PA is to solve a business problem. There is a constant balance between making an interpretable model and one that has good performance. Highly flexible models, which are often called black boxes, are only useful for making predictions.
  

```{r echo = F, fig.align="center", warning=F, out.width="100%"}
knitr::include_graphics("images/flexibility vs interpretability.png")
```

The parameters that you change also have an impact.  In the case of the lasso or ridge regression, by increasing $\lambda$ you can decrease the flexibility. For stepwise selection, the value of k controls the amount by which the log-likelihood is adjusted based on the number of parameters. As you will see in the next chapter on trees, decision trees have flexibility adjusted by CP, and random forests (RFs) and gradient boosted machines (GBMs) have their parameters.

<!--chapter:end:04-linear-models.Rmd-->

# Visualization

"A picture speaks a thousand words" is a saying that artists and painters use but which applies to data visualization as well.  You will learn to communicate your message using graphs as well as statistics so that you can communicate your message quickly.  

## Learning Outcomes

- How to make graphs in R
- The different graph types
    - Histogram
    - Box plot
    - Scatterplot
    - Boxplot (Univariate)
    - Barplot (Bivariate)
- How to interpret each graph
- Exam Question Examples

## How to make graphs in R

Let us create a histogram of the claims.  The first step is to create a blank canvas that holds the columns that are needed.  The library to make this is called (ggplot2)[https://ggplot2.tidyverse.org/].  

The `aesthetic` argument, `aes`, means that the variable shown will the the claims.

```{r message = F, echo = F}
library(Cairo)
library(tidyverse)
theme_set(theme_bw())
```

```{r message=F}
library(ExamPAData)
library(tidyverse)
p <- health_insurance %>% ggplot(aes(charges))
```

If we look at `p`, we see that it is nothing but white space with axis for `count` and `income`.

```{r message=F}
p
```

## Add a plot

We add a histogram

```{r warning = F, message=F}
p + geom_histogram()
```

Different plots are called "geoms" for "geometric objects".  Geometry = Geo (space) + meter (measure), and graphs measure data.  For instance, instead of creating a histogram, we can draw a gamma distribution with `stat_density`.

```{r message=F}
p + stat_density()
```

Create an xy plot by adding and `x` and a `y` argument to `aesthetic`.

```{r message=F}
health_insurance %>% 
  ggplot(aes(x = bmi, y = charges)) + 
  geom_point()
```

## Data manipulation chaining

Pipes allow for data manipulations to be chained with visualizations.

```{r message=F}
termlife %>% 
  filter(FACE > 0) %>% 
  mutate(INCOME_AGE_RATIO = INCOME/AGE) %>% 
  ggplot(aes(INCOME_AGE_RATIO, FACE)) + 
  geom_point() + 
  theme_bw()
```

## The different graph types

### Histogram

The (histogram)[https://ggplot2.tidyverse.org/reference/geom_histogram.html] is used when you want to look at the probability distribution of a continuous variable.  

### Box plot

### Scatterplot

### Boxplot (Univariate)

### Barplot (Bivariate)

<!--chapter:end:04-visualization.Rmd-->

---
output:
  html_document: default
  pdf_document: default
---

```{r include = F}
library(ExamPAData)
library(tidyverse)
```

# Tree-based models

The models up to this point have been linear.  This means that $Y$ changes gradually as $X$ changes.

Tree-based models allow for abrupt changes in $Y$.

[![Lecture](images/lesson-decisiontrees.PNG)](https://exampa.net/)

Already enrolled?  Watch the full video: <a target="_parent" href="https://course.exampa.net/mod/page/view.php?id=143">Practice Exams + Lessons</a>

## Decision Trees

<iframe width="560" height="315" src="https://www.youtube.com/embed/PaFPbb66DxQ?rel=0&showinfo=1&playlist=7VeUPuFGJHk,wpNl-JwwplA,g9c66TUylZ4,D0efHEJsfHo" frameborder="0" allowfullscreen></iframe>


Decision trees can be used for either classification or regression problems. The model structure is a series of yes/no questions. Depending on how each observation answers these questions, a prediction is made.

The below example shows how a single tree can predict health claims.

- For non-smokers, the predicted annual claims are 8,434. This represents 80% of the observations
- For smokers with a `bmi` of less than 30, the predicted annual claims are 21,000. 10% of patients fall into this bucket.
- For smokers with a `bmi` of more than 30, the prediction is 42,000. This bucket accounts for 11% of patients.

```{r message = F, echo= F}
library(rpart)
library(rpart.plot)
tree <- rpart(charges ~ smoker + bmi, data = health_insurance)
rpart.plot(tree, type = 3)
```

We can cut the data set up into these groups and look at the claim costs. From this grouping, we can see that `smoker` is the most important variable as the difference in average claims is about 20,000.

```{r message = F, echo = F}
library(scales)
library(kableExtra)

health_insurance %>% 
  sample_frac(0.2) %>% 
  mutate(bmi_30 = ifelse(bmi < 30, "bmi < 30", "bmi >= 30")) %>% 
  group_by(smoker,bmi_30) %>% 
  summarise(mean_claims = dollar(mean(charges)),
            n = n()) %>% 
  ungroup() %>% 
  mutate(percent = round(n/sum(n),2)) %>% 
  select(-n) %>% 
  kable("markdown")
```

This was a very simple example because there were only two variables. If we have more variables, the tree will get large very quickly. This will result in overfitting; there will be a good performance on the training data but poor performance on the test data.

The step-by-step process of building a tree is



**Step 1: Find the best predictor-split-point combination**  

This variable could be any one of `age`, `children`, `charges`, `sex`, `smoker`, `age_bucket`, `bmi`, or `region`.

The split observation which best separates observations out based on the value of $y$.  A good split is one where the $y$'s are very different. * **

In this case, `smoker` was chosen.  Then it can be only split in one way: `smoker = "yes"` or `smoker = "no"`.  Notice that although this is a categorical variable, the tree does not need to binanize the variables. Instead, like the table above shows, the data gets partitioned by the categories directly.

Then, for each of these groups, smokers and non-smokers find the next variable and split observation that best separates the claims. In this case, for no-smokers, age was chosen. To find the best cut point of age, look at all possible age cut points from 18, 19, 20, 21, …, 64 and choose the one which best separates the data.

There are three ways of deciding where to split


- *Entropy* (aka, information gain)
- *Gini*
- *Classification error*

Of these, just the first two are commonly used. The exam is not going to ask you to calculate either of these. It is important to remember that neither method will work better on all data sets, and so the best practice is to test both and compare the performance. 

**Step 2: Continue doing this until a stopping criterion is reached. For example, the minimum number of observations is 5 or less.** 

As you can see, this results in a very deep tree.

```{r}
tree <- rpart(formula = charges ~  ., data = health_insurance,
              control = rpart.control(cp = 0.003))
rpart.plot(tree, type = 3)
```

**Step 3: Apply cost complexity pruning to simplify the tree**

Intuitively, we know that the above model would perform poorly due to overfitting. We want to make it simpler by removing nodes. This is very similar to how linear models reduce complexity by reducing the number of coefficients.

A measure of the depth of the tree is the *complexity*.  A simple way of measuring this from the number of terminal nodes, called $|T|$.  In the above example, $|T| = 8$.  The amount of penalization is controlled by $\alpha$.  This is very similar to $\lambda$ in the Lasso.

Intuitively, only looking at the number of nodes by itself is too simple because not all data sets will have the same characteristics such as $n$, $p$, the number of categorical variables, correlations between variables, and so on. In addition, if we just looked at the error (squared error in this case), we would overfit very easily. To address this issue, we use a cost function that takes into account the error and $|T|$.

To calculate the cost of a tree, number the terminal nodes from $1$ to $|T|$, and let the set of observations that fall into the $mth$ bucket be $R_m$.  Then add up the squared error over all terminal nodes to the penalty term.

$$
\text{Cost}_\alpha(T) = \sum_{m=1}^{|T|} \sum_{R_m}(y_i - \hat{y}_{Rm})^2 + \alpha |T|
$$


**Step 4: Use cross-validation to select the best alpha**

The cost is controlled by the `CP` parameter.  In the above example, did you notice the line `rpart.control(cp = 0.003)`?  This is telling `rpart` to continue growing the tree until the CP reaches 0.003.  At each subtree, we can measure the cost `CP` as well as the cross-validation error `xerror`.

This is stored in the `cptable` 

```{r message = F}
tree <- rpart(formula = charges ~  ., data = health_insurance,
              control = rpart.control(cp = 0.0001))
cost <- tree$cptable %>% 
  as_tibble() %>% 
  select(nsplit, CP, xerror) 

cost %>% head()
```

As more splits are added, the cost continues to decrease, reaches a minimum, and then begins to increase.  

```{r echo = F}
cost %>% 
  filter(nsplit > 1) %>% 
  mutate(min = ifelse(xerror == min(cost$xerror),"y", "n")) %>% 
  ggplot(aes(nsplit, xerror, color = min)) + 
  geom_line() + 
  geom_point() + 
  theme_bw() + 
  theme(legend.position = "none") + 
  scale_color_manual(values = c("black", "red")) + 
  annotate("text", x = 25, y = 0.17, label = "Minimum Error", color = "red")
```

To optimize performance, choose the number of splits that have the lowest error. The goal of using a decision tree is to create a simple model. In this case, we can error the side of a lower `nsplit` so that the tree is shorter and more interpretable. So far, all of the questions have only used decision trees for interpretability, and a different model method has been used when predictive power is needed.

Once we have selected $\alpha$, the tree is pruned.  This table below shows 6 different trees.  The `xerror` column is the missclassification error.  The `rel error` is the relative error, which is the missclassification error divided by its smallest value.  This rescales `xerror` so that the tree with the smallest error is given a `rel error` of 1.00.

```{r include = T}
tree$cptable %>% 
  as_tibble() %>% 
  select(nsplit, CP, xerror, `rel error`) %>% 
  head()
```

The SOA may give you code to find the lowest CP value, such as below. You could always find this value yourself by inspecting the CP table and choosing the value of `CP` which has the lowest `xerror`.

```{r}
pruned_tree <- prune(tree,
                     cp = tree$cptable[which.min(tree$cptable[, "xerror"]), "CP"])
```

To make a simple tree, there are a few options

- Set the maximum depth of a tree with `maxdepth`
- Manually set `cp` to be higher
- Use fewer input variables and avoid categories with many levels
- Force a high number of minimum observations per terminal node with `minbucket`

For instance, using these suggestions allows for a simpler tree to be fit.

```{r message = F}
library(caret)
set.seed(42)
index <- createDataPartition(y = health_insurance$charges, 
                             p = 0.8, list = F)
train <- health_insurance %>% slice(index)
test <- health_insurance %>% slice(-index)

simple_tree <- rpart(formula = charges ~  ., 
              data = train,
              control = rpart.control(cp = 0.0001, 
                                      minbucket = 200,
                                      maxdepth = 10))
rpart.plot(simple_tree, type = 3)
```

We evaluate the performance on the test set.  Because the target variable `charges` are highly skewed, we use the Root Mean Squared Log Error (RMSLE). We see that the thorny tree has the best (lowest) error and has 8 terminal nodes. The simple tree with only three terminal nodes has a worse (higher) error, but this is still an improvement over the mean prediction.

```{r}
tree_pred <- predict(tree, test)
simple_tree_pred <- predict(simple_tree, test)

get_rmsle <- function(y, y_hat){
  sqrt(mean((log(y) - log(y_hat))^2))
}

get_rmsle(test$charges, tree_pred)
get_rmsle(test$charges, simple_tree_pred)
get_rmsle(test$charges, mean(train$charges))
```
### Example: SOA PA 6/18/2020, Task 6

<iframe src="https://player.vimeo.com/video/467846520?title=0&byline=0&portrait=0" width="640" height="360" frameborder="0" allow="autoplay; fullscreen" allowfullscreen></iframe>

> Describe what pruning does and why it might be considered for this business problem.

> Construct an unpruned regression tree using the code provided.

> Review the complexity parameter table and plot for this tree. State the optimal complexity parameter and the number of leaves resulting from the tree pruned using that value.

> Prune the tree using a complexity parameter that will result in eight leaves. If eight is not a possible option, select the largest number less than eight that is possible.

> Calculate and compare the Pearson goodness-of-fit statistic on the test set for both trees (original and pruned).

> Interpret the entire pruned tree (all leaves) in the context of the business problem. 

### Advantages and disadvantages

**Advantages**

- Easy to interpret 
- Performs variable selection
- Categorical variables do not require binarization in order for each level to be used as a separate predictor
- Captures non-linearities 
- Captures interaction effects
- Handles missing values

**Disadvantages**

- Is a “weak learner” because of low predictive power
- Does not work on small data sets
- Is often a simplification of the underlying process because all observations at terminal nodes have equal predicted values
- High variance (which can be alleviated with stricter parameters) leads the “easy to interpret results” to change upon retraining Unable to predict beyond the range of the training data for regression (because each predicted value is an average of training samples)

| Readings |  | 
|-------|---------|
| ISLR 8.1.1 Basics of Decision Trees  | |
| ISLR 8.1.2 Classification Trees|  |
| [rpart Documentation (Optional)](https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf) ||

## Ensemble learning

The “wisdom of crowds” says that often many are smarter than the few. In the context of modeling, the models which we have looked at so far have been single guesses; however, often, the underlying process is more complex than any single model can explain. If we build separate models and then combine them, known as *ensembling*, performance can be improved. Instead of creating a single perfect model, many simple models, known as *weak learners* are combined into a *meta-model*.

The two main ways that models are combined are through *bagging* and *boosting*.

### Bagging

To start, we create many “copies” of the training data by sampling with replacement. Then we fit a simple model to each data set. Typically is would be a decision tree or linear model because each model is looking at different areas of the data, the predictions are different. The final model is a weighted average of each of the individual models.

### Boosting

Boosting always uses the original training data and iteratively fits models to the error of the prior models. These weak learners are ineffective by themselves but powerful when added together. Unlike with bagging, the computer must train these weak learners *sequentially* instead of in parallel.

## Random Forests

A random forest is the most common example of bagging. As the name implies, a forest is made up of *trees*. Separate trees are fit to sampled data sets. There is one minor modification for random forests: to make each model even more different; each tree selects a *random subset of variables*.

<iframe width="560" height="315" src="https://www.youtube.com/embed/PaFPbb66DxQ?rel=0&showinfo=1&playlist=J4Wdy0Wc_xQ,nyxTdL_4Q-Q,6EXPYzbfLCE" frameborder="0" allowfullscreen></iframe>


If we had to explain why a random forest works in three steps, they would be:

1. Assume that the underlying process, $Y$, has some signal within the data $\mathbf{X}$.
2. Introduce randomness (variance) to capture the signal.
3. Remove the variance by taking an average.

When using only a single tree, there can only be as many predictions as terminal nodes. In a random forest, predictions can be more granular due to the contribution of each of the trees.

The below graph illustrates this. A single tree (left) has stair-like, step-wise predictions, whereas a random forest is free to predict any value. The color represents the predicted value (yellow = highest, black = lowest).


```{r message = F, echo = F, fig.height = 4}
library(randomForest)
tree <- rpart(charges ~ age + bmi, data = health_insurance)
df <- health_insurance %>% mutate(
  prediction = predict(tree, health_insurance))

p1 <- df %>% 
  ggplot(aes(bmi, age, color = prediction)) + 
  geom_point() + 
  theme_bw() + 
  labs(title ="Decision Tree") + 
  theme(legend.position = "none")

rf <- randomForest(charges ~ age + bmi, 
                   data = health_insurance, ntree = 50)
df <- health_insurance %>% mutate(
  prediction = predict(rf, health_insurance))

p2 <- df %>% 
  ggplot(aes(bmi, age, color = prediction)) + 
  geom_point() + 
  theme_bw() + 
  labs(title ="Random Forest") + 
  theme(legend.position = "none")

library(ggpubr)
ggarrange(p1,p2)
```

Unlike decision trees, random forest trees do not need to be pruned. Overfitting is less of a problem: if one tree overfits, other trees overfit in other areas to compensate.  

In most applications, only the `mtry` parameter, which controls how many variables to consider at each split, needs to be tuned. Tuning the `ntrees` parameter is not required; however, the SOA may still ask you to.

### Example

Using the basic `randomForest` package we fit a model with 500 trees. 

This expects only numeric values.  We create dummy (indicator) columns. 

```{r}
rf_data <- health_insurance %>% 
  sample_frac(0.2) %>% 
  mutate(sex = ifelse(sex == "male", 1, 0),
         smoker = ifelse(smoker == "yes", 1, 0),
         region_ne = ifelse(region == "northeast", 1,0),
         region_nw = ifelse(region == "northwest", 1,0),
         region_se = ifelse(region == "southeast", 1,0),
         region_sw = ifelse(region == "southwest", 1,0)) %>% 
  select(-region)
rf_data %>% glimpse(50)
```


```{r message = F}
library(caret)
set.seed(42)
index <- createDataPartition(y = rf_data$charges, 
                             p = 0.8, list = F)
train <- rf_data %>% slice(index)
test <- rf_data %>% slice(-index)
```

```{r fig.height=4, include = T}
rf <- randomForest(charges ~ ., data = train, ntree = 400)
plot(rf)
```

We again use RMSLE.  This is lower (better) than a model that uses the average as a baseline.

```{r}
pred <- predict(rf, test)
get_rmsle <- function(y, y_hat){
  sqrt(mean((log(y) - log(y_hat))^2))
}

get_rmsle(test$charges, pred)
get_rmsle(test$charges, mean(train$charges))
```

### Variable Importance

*Variable importance* is a way of measuring how each variable contributes to the overall performance of the model. For single decision trees, the variable “higher up” in the tree have greater influence. Statistically, there are two ways of measuring this:

1) Look at the mean reduction in accuracy when the variable is randomly permuted versus using the actual values from the data. This is done with `type = 1` (default).

2) Use the total decrease in node impurities from splitting on the variable, averaged over all trees. For classification, the node impurity is measured by the Gini index; for regression, it is measured by the residual sum of squares $\text{RSS}$.  This is `type = 2`.

`smoker`, `bmi`, and `age` are the most important predictors of charges. As you can imagine, variable importance is a highly useful tool for building models. We could use this to test out newly engineered features or perform feature selection by taking the top-n features and use them in a different model. Random forests can handle very high dimensional data, which allows for many tests to be run at once.

```{r}
varImpPlot(x = rf)
```

### Partial dependence

We know which variables are important, but what about the direction of the change? In a linear model, we would be able to just look at the sign of the coefficient. In tree-based models, we have a tool called *partial dependence*.  This attempts to measure the change in the predicted value by taking the average $\hat{\mathbf{y}}$ after removing the effects of all other predictors.

Although this is commonly used for trees, this approach is model-agnostic in that any model could be used.


Take a model of two predictors, $\hat{\mathbf{y}} = f(\mathbf{X}_1, \mathbf{X_2})$.  For simplicity, say that $f(x_1, x_2) = 2x_1 + 3x_2$. 

The data looks like this

```{r}
df <- tibble(x1 = c(1,1,2,2), x2 = c(3,4,5,6)) %>% 
  mutate(f = 2*x1 + 3*x2)
df
```

Here is the partial dependence of `x1` on to `f`.  

```{r}
df %>% group_by(x1) %>% summarise(f = mean(f))
```

This method of using the mean is know as the *Monte Carlo* method.  There are other methods for partial dependence that are not on the syllabus.

For the Random Forest, this is done with `pdp::partial()`.

```{r message = F,fig.cap="Partial Dependence", fig.height=4}
library(pdp)
bmi <- pdp::partial(rf, pred.var = "bmi", 
                    grid.resolution = 15) %>% 
  autoplot() + theme_bw()
age <- pdp::partial(rf, pred.var = "age", 
                    grid.resolution = 15) %>% 
  autoplot() + theme_bw()

ggarrange(bmi, age)
```

### Advantages and disadvantages

**Advantages**

- Resilient to overfitting due to bagging 
- •	Only one parameter to tune (mtry, the number of features considered at each split)
- Very good a multi-class prediction
- Nonlinearities
- Interaction effects
- Handles missing data
- Deals with unbalanced after over/undersampling

**Disadvantages**

- Does not work on small data sets
- Weaker performance than other methods (GBM, NN)
- Unable to predict beyond training data for regression

| Readings |  | 
|-------|---------|
| ISLR 8.2.1 Bagging  | |
| ISLR 8.1.2 Random Forests|  |

## Gradient Boosted Trees

Another ensemble learning method is *gradient boosting*, also known as the Gradient Boosted Machine (GBM). This is one of the most widely-used and powerful machine learning algorithms that are in use today.

Before diving into gradient boosting, understanding the AdaBoost algorithm is helpful.


### Gradient Boosting

<iframe width="560" height="315" src="https://www.youtube.com/embed/PaFPbb66DxQ?rel=0&showinfo=1&playlist=LsK-xG1cLYA,3CC4N4z3GJc,2xudPOBz-vs,jxuNLH5dXCs,StWY5QWMXCw" frameborder="0" allowfullscreen></iframe>

</br>

### Notation

Start with an initial model, which is just a constant prediction of the mean.

$$f = f_0(\mathbf{x_i}) = \frac{1}{n}\sum_{i=1}^ny_i$$

Then we update the target (what the model is predicting) by subtracting off the previously predicted value.  

$$ \hat{y_i} \leftarrow y_i - f_0(\mathbf{x_i})$$

This $\hat{y_i}$ is called the *residual*.  In our example, instead of predicting `charges`, this would be predicting the residual of $\text{charges}_i - \text{Mean}(\text{charges})$.  We now use this model for the residuals to update the prediction.

If we updated each prediction with the prior residual directly, the algorithm would be unstable. To make this process more gradual, we use a
 *learning rate* parameter.

At step 2, we have

$$f = f_0 + \alpha f_1$$

Then we go back and fit another weak learner to this residual and repeat.

$$f = f_0 + \alpha f_1 + \alpha f_2$$

We then iterate through this process hundreds or thousands of times, slowly improving the prediction.

Because each new tree is fit to *residuals* instead of the response itself, the process continuously improves the prediction. As the prediction improves, the residuals get smaller and smaller. In random forests or other bagging algorithms, the model performance is more limited by the individual trees because each only contributes to the overall average. The name is *gradient boosting* because the residuals are an approximation of the gradient, and gradient descent is how the loss functions are optimized.

Similar to how GLMs can be used for classification problems through a logit transform (aka logistic regression), GBMs can also be used for classification.


### Parameters

For random forests, the individual tree parameters do not get tuned. For GBMs, however, these parameters can make a significant difference in model performance. 

**Boosting parameters:**

- `n.trees`: Integer specifying the total number of trees to fit. This is equivalent to the number of iterations and the number of basis functions in the additive expansion. Default is 100.

- `shrinkage`: a shrinkage parameter applied to each tree in the expansion. Also known as the learning rate or step-size reduction, 0.001 to 0.1 usually works. A lower learning rate typically requires more trees. Default is 0.1.

**Tree parameters:**

- `interaction.depth`: Integer specifying the maximum depth of each tree (i.e., the highest level of variable interactions allowed). A value of 1 implies an additive model, a value of 2 implies a model with up to 2-way interactions, etc. Default is 1.

- `n.minobsinnode`: Integer specifying the minimum number of observations in the terminal nodes of the trees. Note that this is the actual number of observations, not the total weight.

GBMs are easy to overfit, and the parameters need to be carefully tuned using cross-validation. In the Examples section, we go through how to do this.


</br>

```{block, type='studytip'}
**Tip:** Whenever fitting a model, use `?model_name` to get the documentation.  The parameters below are from `?gbm`. 
```

</br>

### Example

We fit a gbm below without tuning the parameters for the sake of example.  

```{r message = F, cache=T}
library(gbm)
gbm <- gbm(charges ~ ., data = train,
           n.trees = 100,
           interaction.depth = 2,
           n.minobsinnode = 50,
           shrinkage = 0.1)

pred <- predict(gbm, test, n.trees = 100)

get_rmsle(test$charges, pred)
get_rmsle(test$charges, mean(train$charges))
```

### Advantages and disadvantages

This exam covers the basics of GBMs. There are many variations of GBMs not covered in detail, such as `xgboost`.

**Advantages**

- High prediction accuracy
- Shown to work empirically well on many types of problems
- Nonlinearities, interaction effects, resilient to outliers, corrects for missing values
- Deals with class imbalance directly by weighting observations

**Disadvantages**

- Requires large sample size
- Longer training time
- Does not detect linear combinations of features. These must be engineered Can overfit if not tuned correctly

| Readings |  | 
|-------|---------|
| ISLR 8.2.3 Boosting  | |


## Exercises

```{r message = F}
library(ExamPAData)
library(tidyverse)
```


Run this code on your computer to answer these exercises.

### 1. RF tuning with `caret`

The best practice of tuning a model is with cross-validation.  This can only be done in the `caret` library.  If the SOA asks you to use `caret`, they will likely ask you a question related to cross-validation as below.

An actuary has trained a predictive model, chosen the best hyperparameters, cleaned the data, and performed feature engineering. However, they have one problem: the error on the training data is far lower than on new, unseen test data. Read the code below and determine their problem. Find a way to lower the error on the test data *without changing the model or the data.*  Explain the rational behind your method.

```{r eval = F, cache=T}
set.seed(42)
#Take only 250 records 
#Uncomment this when completing this exercise
data <- health_insurance %>% sample_n(250) 

index <- createDataPartition(
  y = data$charges, p = 0.8, list = F) %>% 
  as.numeric()
train <-  health_insurance %>% slice(index)
test <- health_insurance %>% slice(-index)

control <- trainControl(
  method='boot', 
  number=2, 
  p = 0.2)

tunegrid <- expand.grid(.mtry=c(1,3,5))
rf <- train(charges ~ .,
            data = train,
            method='rf', 
            tuneGrid=tunegrid, 
            trControl=control)

pred_train <- predict(rf, train)
pred_test <- predict(rf, test)

get_rmse <- function(y, y_hat){
  sqrt(mean((y - y_hat)^2))
}

get_rmse(pred_train, train$charges)
get_rmse(pred_test, test$charges)
```

### 2. Tuning a GBM with `caret`

If the SOA asks you to tune a GBM, they will need to give you starting hyperparameters that are close to the “best” values due to how slow the Prometric computers are. Another possibility is that they pre-train a GBM model object and ask that you use it.

This example looks at 135 combinations of hyper parameters.

```{r, eval = F}
set.seed(42)
index <- createDataPartition(y = health_insurance$charges, 
                             p = 0.8, list = F)
#To make this run faster, only take 50% sample
df <- health_insurance %>% sample_frac(0.50) 
train <- df %>% slice(index) 
test <- df %>% sample_frac(0.05)%>% slice(-index)

tunegrid <- expand.grid(
    interaction.depth = c(1,5, 10),
    n.trees = c(50, 100, 200, 300, 400), 
    shrinkage = c(0.5, 0.1, 0.0001),
    n.minobsinnode = c(5, 30, 100)
    )
nrow(tunegrid)

control <- trainControl(
  method='repeatedcv', 
  number=5, 
  p = 0.8)
```

```{r cache=T, eval = F}
gbm <- train(charges ~ .,
            data = train,
            method='gbm', 
            tuneGrid=tunegrid, 
            trControl=control,
            #Show detailed output
            verbose = FALSE
            )
```

The output shows the RMSE for each of the 135 models tested.

(Part 1 of 3)

Identify the hyperpameter combination that has the lowest training error.

(Part 2 of 3)

2.Suppose that the optimization measure was RMSE. The below table shows the results from three models. Explain why some sets of parameters have better RMSE than the others.  

```{r, eval = F}
results <- gbm$results %>% arrange(RMSE)
top_result <- results %>% slice(1)%>% mutate(param_rank = 1)
tenth_result <- results %>% slice(10)%>% mutate(param_rank = 10)
twenty_seventh_result <- results %>% slice(135)%>% mutate(param_rank = 135)

rbind(top_result, tenth_result, twenty_seventh_result) %>% 
  select(param_rank, 1:5)
```

3. The partial dependence of `bmi` onto `charges` makes it appear as if `charges` increases monotonically as `bmi` increases.

```{r fig.width=5, fig.height=5, cache=T, eval = F}
pdp::partial(gbm, pred.var = "bmi", grid.resolution = 15, plot = T)
```

However, when we add in the `ice` curves, we see that there is something else going on.  Explain this graph.  Why are there two groups of lines?

```{r cache=T, eval = F}
pdp::partial(gbm, pred.var = "bmi", grid.resolution = 20, plot = T, ice = T, alpha = 0.1, palette = "viridis")
```

**Solutions**:

Already enrolled?  Watch the full video: <a target="_parent" href="https://course.exampa.net/mod/page/view.php?id=214
">Practice Exams</a> | <a target="_parent" href="https://course.exampa.net/mod/page/view.php?id=165">Practice Exams + Lessons</a>

<!--chapter:end:05-tree-based-models.Rmd-->

---
output:
  html_document: default
  pdf_document: default
---

# Unsupervised Learning

This chapter covers the eighth learning objective along with one additional topic of correlation analysis.


[![Lecture](images/lesson-unsupervised-learning.PNG)](https://exampa.net/)

Already enrolled?  Watch the full video: <a target="_parent" href="https://course.exampa.net/mod/page/view.php?id=147">Practice Exams + Lessons</a>

```{r echo = F, fig.align="center", warning=F, out.width="400%"}
knitr::include_graphics("images/learning_obj8.png")
```

As of the fall of 2020, there have been eight different PA exams. The table below shows the types of questions in each. Your exam in December will likely have a question on either k-means or hierarchical clustering because these topics have not frequently been tested. The Hospital Readmissions sample project did have one question related to k-means, but that was in 2018. Our ExamPA.net practice exams contain questions on both topics.

```{r echo = F, fig.align="center", warning=F, out.width="400%"}
knitr::include_graphics("images/prior_unsupervised_qs.png")
```

## Types of Learning

You are in a classroom and your teacher is giving a lesson which has a correct answer, such as 1 + 2 = 2.  This is called *supervised learning*.  The teacher is the supervisor, who is responsible for minimizing the number of incorrect answers. When a question is answered correctly, there is a reward, and when a mistake is made, there is a penalty. In machine learning, we measure the performance using metrics such as RMSE, and we say that the model is a good “fit” when this metric is low.

```{r, echo=FALSE,out.width="49%", out.height="20%",fig.cap="Supervised (Left), Unsupervised (Right)",fig.show='hold',fig.align='center'}
knitr::include_graphics(c("images/classroom.png","images/legos.png"))
``` 

However, if you are given a pile of **Legos** and told to build whatever you want, that would be *unsupervised learning*.  There are no right or wrong answers and to goal is to explore patterns in the data. There are no performance metrics, and it is impossible to say whether a model is a good fit or not.

Up to this point, you have been working with supervised learning. This is the major focus of PA because predictive analytics often is used to predict a future outcome. We now move on to the unsupervised learning algorithms. Here are all of the learning algorithms on PA:


|Supervised                   |Unsupervised                      |
|-----------------------------|----------------------------------|
|GLM                          |Correlation analysis              |
|Lasso, Ridge, and Elastic Net|Principal component analysis (PCA)|
|Decision Tree                |K-means clustering                |
|Bagged Tree                  |Hierarchical clustering           |
|Boosted Tree                 |                                  |

**Semi-Supervised Learning** is a mix of the two. One example of this is using PCA or Clustering to create features used in a supervised model.

## Correlation Analysis

- Two variables are said to be positively correlated when increasing one tends to increase the other and negatively correlated when increasing one decreases the other
- Correlation is unsupervised because it does not depend on the target variable
- Correlation is only defined for numeric variables. It is possible to calculate the correlation between binary variables if coded as 0 or 1, but these questions have never appeared on PA.

### Correlation does not equal causation

This is a common saying of statisticians. Two things happening at the same time are not sufficient evidence to suggest that one causes the other. A [spurious correlation](https://tylervigen.com/old-version.html) is when two unrelated variables have a positive or negative correlation by random chance.

Examples:

**Number of Drownings and Ice Cream Sales:** Drownings rise when ice cream sales rise because the heat causes more people to go swimming and want ice cream.

**Executives who say "please" and "thank you" more often enjoy better performance reviews:**  It might at first appear that this is due to brown-nosing or flattery, but a more likely explanation is that people who take the extra effort to be polite also take the extra effort to do their jobs well. People who have good performance may also be polite, but not all polite people are high performers.

## Principal Component Analysis (PCA)

Often there are variables that contain redundant information.  PCA is one method of simplifying them.

<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/FgakZw6K1QQ" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

StatQuest. “Principal Component Analysis (PCA), Step-by-Step” YouTube, Joshua Starmer, 2 Apr 2018, https://www.youtube.com/watch?v=FgakZw6K1QQ

PCA is a *dimensionality reduction* method which reduces the number of variables needed to retain most of the information in a matrix.  If there are predictor variables $(x_1, x_2, x_3)$, then running PCA and choosing the first two Principal Components (PCs) would reduce the dimension from 3 to 2. You can imagine this as observations in three-dimensional space being projected down onto a plane. The coordinates of this plane are $(\text{PC}_1, \text{PC}_2)$.

```{r echo = F, message = F, warning = F, fig.align="center"}
library(png)
library(tidyverse)
knitr::include_graphics("images/3d_dim_reduction.png")
```

Each PC is a linear combination of the original variables. For example, PC1 might be

$$PC_1 = 0.2X_1 + 0.3X_2 - 0.2X_3$$
The weights here are also called *loadings* or *rotations*, and are (0.2, 0.3, -0.2) in this example.  

| Readings |  | 
|-------|---------|
| ISLR 10.2 Principal Component Analysis|  |
| ISLR 10.3 Clustering Methods|  |

### Example: US Arrests

In this example, we perform PCA on the `USArrests` data set, which is part of
the base `R` package. The rows of the data set contain the 50 states, in
alphabetical order:

```{r message = F}
library(tidyverse)
states=row.names(USArrests)
states
```

The columns of the data set contain four variables relating to various crimes:

```{r}
glimpse(USArrests)
```

Let us start by taking a quick look at the column means of the data. 

```{r}
USArrests %>% summarise_all(mean)
```

We see right away the the data have **vastly** different means. We can also examine the variances of the four variables.

```{r}
USArrests %>% summarise_all(var)
```

Not surprisingly, the variables also have vastly different variances: the `UrbanPop` variable measures the percentage of the population in each state living in an urban area, which is not comparable to the number of crimes committed in each state per 100,000 individuals. If we failed to scale the variables before performing PCA, then most of the principal components we observed would be driven by the `Assault` variable, since it has by far the largest mean and variance. 

Thus, it is important to standardize the variables to have mean zero and standard deviation 1 before performing PCA. We will perform principal components analysis using the `prcomp()` function, which is one of several functions that perform PCA. By default, this centers the variables to have a mean zero. By using the option `scale=TRUE`, we scale the variables to have standard
deviation 1:

```{r}
pca = prcomp(USArrests, scale=TRUE)
```

The output from `prcomp()` contains a number of useful quantities:

```{r}
names(pca)
```

The `center` and `scale` components correspond to the means and standard deviations of the variables that were used for scaling prior to implementing PCA:

```{r}
pca$center
pca$scale
```

The rotation matrix provides the principal component loadings; each column
of `pr.out\$rotation` contains the corresponding principal component
loading vector:

```{r}
pca$rotation
```

We see that there are four distinct principal components. This is to be
expected because there are in general `min(n − 1, p)` informative principal
components in a data set with $n$ observations and $p$ variables.

Using the `prcomp()` function, we do not need to explicitly multiply the data by the principal component loading vectors in order to obtain the principal component score vectors. Rather the 50 × 4 matrix  $x$ has as its columns the principal component score vectors. That is, the $k^{th` column is the $k^{th` principal component score vector. We'll take a look at the first few states:

```{r}
head(pca$x)
```

We can plot the first two principal components using the `biplot()` function:

```{r fig.width=8, fig.height=6}
biplot(pca, scale=0)
```

The `scale=0` argument to `biplot()` ensures that the arrows are scaled to represent the loadings; other values for `scale` give slightly different bi plots with different interpretations.

The `prcomp()` function also outputs the standard deviation of each principal component. We can access these standard deviations as follows:

```{r}
pca$sdev
```

The variance explained by each principal component is obtained by squaring
these:

```{r}
pca_var=pca$sdev^2
pca_var
```

To compute the proportion of variance explained by each principal component, we simply divide the variance explained by each principal component by the total variance explained by all four principal components:

```{r}
pve=pca_var/sum(pca_var)
pve
```

We see that the first principal component explains 62.0% of the variance in the data, the next principal component explains 24.7% of the variance, and so forth. We can plot the PVE explained by each component as follows:

```{r}
plot(pve, xlab="Principal Component", ylab="Proportion of Variance Explained", ylim=c(0,1),type='b')
```

We can also use the function `cumsum()`, which computes the cumulative sum of the elements of a numeric vector, to plot the cumulative PVE:

```{r}
plot(cumsum(pve), xlab="Principal Component", ylab="Cumulative Proportion of Variance Explained", ylim=c(0,1),type='b')
a=c(1,2,8,-3)
cumsum(a)
```


### Example: SOA PA 6/12/19, Task 3

>3. (9 points) Use observations from principal components analysis (PCA) to generate a new feature

>Your assistant has provided code to run a PCA on three variables. Run the code on these three variables. Interpret the output, including the loadings on significant principal components. Generate one new feature based on your observations (which may also involve dropping some current variables). Your assistant has provided some notes on using PCA on factor variables in the .Rmd file.

<iframe src="https://player.vimeo.com/video/465063620" width="640" height="360" frameborder="0" allow="autoplay; fullscreen" allowfullscreen></iframe>

Already enrolled?  Watch the full video: <a target="_parent" href="https://course.exampa.net/mod/page/view.php?id=204">Practice Exams</a> | <a target="_parent" href="https://course.exampa.net/mod/page/view.php?id=155">Practice Exams + Lessons</a>

### Example: PCA on Cancel Cells

The data  `NCI60` contains expression levels of 6,830 genes from 64 cancel cell lines. Cancer type is also recorded.

```{r}
library(ISLR)
nci_labs=NCI60$labs
nci_data=NCI60$data
```

We first perform PCA on the data after scaling the variables (genes) to have a standard deviation one, although one could reasonably argue that it is better not to scale the genes:

```{r}
pca = prcomp(nci_data, scale=TRUE)
```

We now plot the first few principal component score vectors in order to visualize the data. The observations (cell lines) corresponding to a given cancer type will be plotted in the same color to see to what extent the observations within a cancer type are similar to each other. We first create a simple function that assigns a distinct color to each element of a numeric vector. The function will be used to assign a color to each of the 64 cell lines, based on the cancer type to which it corresponds. We will use the `rainbow()` function, which takes its argument a positive integer and returns a vector containing that number of distinct colors.

```{r}
Cols=function(vec){
    cols=rainbow(length(unique(vec)))
    return(cols[as.numeric(as.factor(vec))])
  }
```

We now can plot the principal component score vectors:

```{r}
par(mfrow=c(1,2))
plot(pca$x[,1:2], col=Cols(nci_labs), pch=19,xlab="Z1",ylab="Z2")
plot(pca$x[,c(1,3)], col=Cols(nci_labs), pch=19,xlab="Z1",ylab="Z3")
```

On the whole, cell lines corresponding to a single cancer type tend to have similar values on the first few principal component score vectors. 

This indicates that cell lines from the same cancer type tend to have pretty similar gene expression levels.

We can obtain a summary of the proportion of variance explained (PVE) of the first few principal components using the (PVE) of the first few principal components using the `summary()` method for a `prcomp` object:

```{r}
summary(pca)
```

Using the `plot()` function, we can also plot the variance explained by the first few principal components:

```{r}
plot(pca)
```

Note that the height of each bar in the bar plot is given by squaring the corresponding element of `pr.out\$sdev`. However, it is generally more informative to plot the PVE of each principal component (i.e. a **scree plot**) and the cumulative PVE of each principal component. This can be done with just a little tweaking:

```{r}
pve = 100*pca$sdev^2/sum(pca$sdev^2)
par(mfrow=c(1,2))
plot(pve,  type="o", ylab="PVE", xlab="Principal Component", col="blue")
plot(cumsum(pve), type="o", ylab="Cumulative PVE", xlab="Principal Component", col="brown3")
```

Together, we see that the first seven principal components explain around 40% of the variance in the data. This is not a huge amount of the variance. However, looking at the scree plot, we see that while each of the first seven principal components explains a substantial amount of variance, there is a marked decrease in the variance explained by other principal components. That is, there is an **elbow** in the plot after approximately the seventh principal component. This suggests that there may be little benefit to examining more than seven or so principal components (phew! even examining seven principal components may be difficult).

## Clustering

Imagine that you are a large retailer interested in understanding the customer base. There may be several “types” of customers, such as those shopping for a business with corporate accounts, those shopping for leisure, or debt-strapped grad students. Each of these customers would exhibit different behavior and should be treated differently statistically. However, how can the “type” of customer be defined? Especially for large customer data sets in the millions, one can imagine how this problem can be challenging.

Clustering algorithms look for groups of observations that are similar to one another. Because there is no target variable, measuring the quality of the “fit” is much more complicated. There are many clustering algorithms, but this exam only focuses on the two that are most common.



## K-Means Clustering

Kmeans takes continuous data and assigns observations into k clusters or groups. In the two-dimensional example, this is the same as drawing lines around points. 

<iframe width="560" height="315" src="https://www.youtube.com/embed/4b5d3muPQmA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

StatQuest. “ K-means clustering” YouTube, Joshua Starmer, 2 Apr 2018, https://www.youtube.com/watch?v=4b5d3muPQmA&t=268s

Kmeans consists of the following steps:

**a)** Start with two variables ($X_1$ on the X-axis, and $X_2$ on the Y-axis.)
**b)** Randomly assign cluster centers.
**c)** Put each observation into the cluster that is closest.
**d) - f)** Move the cluster center to the mean of the observations assigned to it and continue until the centers stop moving.
**g)** Repeated steps a) - f) a given number of times (controlled by `n.starts`).  This reduces the uncertainty from choosing the initial centers randomly.

```{r, message = FALSE, warning = FALSE, echo = FALSE, out.width="1000%"}
knitr::include_graphics("images/kmeans.png")
```

#### R Example

In `R`, the function `kmeans()` performs K-means clustering in R. We begin with a simple simulated example in which there truly are two clusters in the data: the first 25 observations have a mean shift relative to the next 25 observations.

```{r}
set.seed(2)
x = matrix(rnorm(50*2), ncol = 2)
x[1:25,1] = x[1:25,1]+3
x[1:25,2] = x[1:25,2]-4
```

We now perform K-means clustering with `K  =  2`:

```{r}
km_out = kmeans(x,2,nstart = 20)
```

The cluster assignments of the 50 observations are contained in
`km_out$cluster`:

```{r}
km_out$cluster
```

The K-means clustering perfectly separated the observations into two clusters even though we did not supply any group information to `kmeans()`. We can plot the data, with each observation colored according to its cluster assignment:

```{r}
plot(x, col = (km_out$cluster+1), main = "K-Means Clustering Results with K = 2", xlab = "", ylab = "", pch = 20, cex = 2)
```

Here the observations can be easily plotted because they are two-dimensional. If there were more than two variables, we could instead perform PCA and plot the first two principal components score vectors.
In this example, we knew that there were two clusters because we generated the data. However, for real data, in general, we do not know the true number of clusters. We could instead have performed K-means clustering on this example with `K  =  3`. If we do this, K-means clustering will split up the two “real” clusters since it has no information about them:

```{r}
set.seed(4)
km_out = kmeans(x, 3, nstart = 20)
km_out
plot(x, col = (km_out$cluster+1), main = "K-Means Clustering Results with K = 3", xlab = "", ylab = "", pch = 20, cex = 2)
```

To run the `kmeans()` function in R with multiple initial cluster assignments, we use the nstart argument. If a value of `nstart` greater than one is used, then K-means clustering will be performed using multiple random assignments, and the `kmeans()` function will report only the best results. Here we compare using `nstart = 1`:

```{r}
set.seed(3)
km_out = kmeans(x, 3, nstart = 1)
km_out$tot.withinss
```

to `nstart = 20`:

```{r}
km_out = kmeans(x,3,nstart = 20)
km_out$tot.withinss
```

Note that `km_out\$tot.withinss` is the total within-cluster sum of squares, which we seek to minimize by performing K-means clustering. The individual within-cluster sum-of-squares are contained in the vector `km_out\$withinss`.

It is generally recommended to always run K-means clustering with a large value of `nstart`, such as 20 or 50, to avoid getting stuck in an undesirable local optimum.

When performing K-means clustering and using multiple initial cluster assignments, it is also important to set a random seed using the 
`set.seed()` function. This way, the initial cluster assignments can be replicated, and the K-means output will be fully reproducible.

## Hierarchical Clustering

Kmeans required that we choose the number of clusters, k. Hierarchical clustering is an alternative that does not require that we choose only one value of k.

<iframe width="560" height="315" src="https://www.youtube.com/embed/7xHsRkOdVwo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

StatQuest. “ Hierarchical Clustering” YouTube, Joshua Starmer, 20 Jun, 2017, https://www.youtube.com/watch?v=7xHsRkOdVwo&t=137s

The most common type of hierarchical clustering uses a *bottom-up* approach.  This starts with a single **observation** and then looks for others close and puts them into a cluster. Then it looks for other **clusters** that are similar and groups these together into a **mega cluster**.  It continues to do this until all observations are in the same group.

This is analyzed with a graph called a dendrogram (dendro = tree, gram = graph).  The height represents “distance,” or how similar the clusters are to one another. The clusters on the bottom, which are vertically close to one another, have similar data values; the clusters that are further apart vertically are less similar.

Choosing the value of the cutoff height changes the number of clusters that result.


```{r, message = FALSE, warning = FALSE, echo = FALSE, fig.align="center", out.width="1000%"}
knitr::include_graphics("images/HClustering.png")
```

Certain data have a natural hierarchical structure. For example, say that the variables are City, Town, State, Country, and Continent. If we used hierarchical clustering, this pattern could be established even if we did not have labels for Cities, Towns, and so forth.

The `hclust()` function implements hierarchical clustering in R. In the following example; we use the data of previous section to plot the hierarchical clustering dendrogram using full, single, and average linkage clustering Euclidean distance as the dissimilarity measure. We begin by clustering observations using complete linkage. The `dist()` function is used
to compute the 50 $\times$ 50 inter-observation Euclidean distance matrix:

```{r}
hc_complete = hclust(dist(x), method = "complete")
```

We could just as easily perform hierarchical clustering with average or single linkage instead:

```{r}
hc_average = hclust(dist(x), method = "average")
hc_single = hclust(dist(x), method = "single")
```

We can now plot the dendrograms obtained using the usual `plot()` function.
The numbers at the bottom of the plot identify each observation:

```{r}
par(mfrow = c(1,3))
plot(hc_complete,main = "Complete Linkage", xlab = "", sub = "", cex = .9)
plot(hc_average, main = "Average Linkage", xlab = "", sub = "", cex = .9)
plot(hc_single, main = "Single Linkage", xlab = "", sub = "", cex = .9)
```

To determine the cluster labels for each observation associated with a given cut of the dendrogram, we can use the `cutree()` function:

```{r}
cutree(hc_complete, 2)
cutree(hc_average, 2)
cutree(hc_single, 2)
```

For this data, complete and average linkage generally separate the observations into their correct groups. However, single linkage identifies one observation as belonging to its own cluster. A more sensible answer is obtained when four clusters are selected, although there are still two singletons:

```{r}
cutree(hc_single, 4)
```

To scale the variables before performing hierarchical clustering of the
observations, we can use the `scale()` function:

```{r}
xsc = scale(x)
plot(hclust(dist(xsc), method = "complete"), main = "Hierarchical Clustering with Scaled Features")
```

Correlation-based distance can be computed using the `as.dist()` function, which converts an arbitrary square symmetric matrix into a form that the `hclust()` function recognizes as a distance matrix. However, this only makes sense for data with **at least three features** since the absolute correlation between any two observations with measurements on two features is always 1. Let us generate and cluster a three-dimensional data set:

```{r}
x = matrix(rnorm(30*3), ncol = 3)
dd = as.dist(1-cor(t(x)))
plot(hclust(dd, method = "complete"), main = "Complete Linkage with Correlation-Based Distance", xlab = "", sub = "")
```

### Example: Clustering Cancel Cells

Unsupervised techniques are often used in the analysis of genomic data. In this example, we will see how hierarchical and K-means clustering compare on the `NCI60` cancer cell line microarray data, which consists of 6,830 gene expression measurements on 64 cancer cell lines: 

```{r}
# The NCI60 data
library(ISLR)
nci_labels = NCI60$labs
nci_data = NCI60$data
```

Each cell line is labeled with a cancer type. We will ignore the cancer types in performing clustering, as these are unsupervised techniques. After performing clustering, we will use this column to see the extent to which these cancer types agree with the results of these unsupervised techniques.

The data has 64 rows and 6,830 columns.


```{r}
dim(nci_data)
```

Let's take a look at the cancer types for the cell lines:

```{r}
table(nci_labels)
```

We now proceed to hierarchically cluster the cell lines in the `NCI60` data to determine whether or not the observations cluster into distinct types of cancer. To begin, we standardize the variables to have mean zero and standard deviation one. This step is optional and needs only be performed if we want each gene to be on the same scale:

```{r}
sd_data = scale(nci_data)
```

We now perform hierarchical clustering of the observations using complete, single, and average linkage. We will use standard Euclidean distance as the dissimilarity measure:

```{r}
par(mfrow = c(1,3))
data_dist = dist(sd_data)
plot(hclust(data_dist), labels = nci_labels, main = "Complete Linkage", xlab = "", sub = "",ylab = "")
plot(hclust(data_dist, method = "average"), labels = nci_labels, main = "Average Linkage", xlab = "", sub = "",ylab = "")
plot(hclust(data_dist, method = "single"), labels = nci_labels,  main = "Single Linkage", xlab = "", sub = "",ylab = "")
```

We see that the choice of linkage certainly does affect the results obtained. Typically, a single linkage will tend to yield trailing clusters: very large clusters onto which individual observations attach one-by-one. On the other hand, complete and average linkage tends to yield more balanced, attractive clusters. For this reason, the complete and average linkage is generally preferred to single linkage. Cell lines within a single cancer type tend to cluster together, although the clustering is not perfect.

Let us use our complete linkage hierarchical clustering for the analysis. We can cut the dendrogram at the height that will yield a particular number of clusters, say 4:


```{r}
hc_out = hclust(dist(sd_data))
hc_clusters = cutree(hc_out,4)
table(hc_clusters,nci_labels)
```

There are some clear patterns. All the leukemia cell lines fall in cluster 3, while the breast cancer cell lines are spread out over three different clusters. We can plot the cut on the dendrogram that produces these four clusters using the `abline()` function, which draws a straight line on top of any existing plot in R:

```{r}
par(mfrow = c(1,1))
plot(hc_out, labels = nci_labels)
abline(h = 139, col = "red")
```

Printing the output of `hclust` gives a useful brief summary of the object:

```{r}
hc_out
```

We claimed earlier that K-means clustering and hierarchical clustering with the dendrogram cut to obtain the same number of clusters could yield **very** different results. How do these `NCI60` hierarchical clustering results compare to what we get if we perform K-means clustering
with `K  =  4`?

```{r}
set.seed(2)
km_out = kmeans(sd_data, 4, nstart = 20)
km_clusters = km_out$cluster
```

We can use a confusion matrix to compare the differences in how the two methods assigned observations to clusters:

```{r}
table(km_clusters,hc_clusters)
```

We see that the four clusters obtained using hierarchical clustering and Kmeans clustering are somewhat different. Cluster 2 in K-means clustering is identical to cluster 3 in hierarchical clustering. However, the other clusters differ: for instance, cluster 4 in K-means clustering contains a portion of the observations assigned to cluster 1 by hierarchical clustering and all of the observations assigned to cluster 2 by hierarchical clustering.

### References

These examples are an adaptation of p. 404-407, 410-413 of “Introduction to Statistical Learning with Applications in R” by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani. Adapted by R. Jordan Crouser at Smith College for SDS293: Machine Learning (Spring 2016), and re-implemented in Fall 2016 `tidyverse` format by Amelia McNamara and R. Jordan Crouser at Smith College.

Used with permission from Jordan Crouser at Smith College and to the following contributors on GitHub:

* github.com/jcrouser
* github.com/AmeliaMN
* github.com/mhusseinmidd
* github.com/rudeboybert
* github.com/ijlyttle

<!--chapter:end:06-unsupervised-learning.Rmd-->

---
output:
  html_document: default
  pdf_document: default
---

# Writing and Communication

30-40% of the points on this exam are from communication. In all prior sittings of this exam, candidates did most of their writing at the end of the report. This was called an Executive Summary. As of 4/3/21, the writing questions were moved into the specific tasks with instructions to write “for a general audience.” In these cases, we will teach you how to use non-technical language and other communication techniques previously used in the executive summary.

## Spelling and Grammar

[![Lecture](images/lesson-writing.PNG)](https://exampa.net/)

## How to Write for PA

[![Lecture](images/lesson-executive-summary.PNG)](https://exampa.net/)





<!--chapter:end:07-writing.Rmd-->

---
output:
  html_document: default
  pdf_document: default
---

# References

Burkov, Andriy.  2019.  *The Hundred-Page Machine Learning Book.*  http://themlbook.com/

Goldburd, Mark et al. 2016.*Generalized Linear Models for Insurance Rating: CAS Monograph Series Number 5*. https://contentpreview.s3.us-east-2.amazonaws.com/CAS+Monograph+5+-+Generalized+Linear+Models+for+Insurance+Ratemaking.pdf

Hastie, Trevor, et al. 2002.  *The Elements of Statistical Learning*. Print.

James, Gareth, et al.  2017.  *An Introduction to Statistical Learning*.
http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf

Piech, Chris and Ng, Andrew.  2019.  Stanford CS221.  Course Notes.  https://stanford.edu/~cpiech/cs221/handouts/kmeans.html

Rigollet, Philippe (2017).  *Lecture 21: Generalized Linear Models*.  Video.  https://www.youtube.com/watch?v=X-ix97pw0xY&t=899s

Wickham, Hadley. 2019.  *R for Data Science*.  https://r4ds.had.co.nz/

"Complimentary Log Log Model".  University of Alberta.  Accessed 2020.  http://www.stat.ualberta.ca/~kcarrier/STAT562/comp_log_log.pdf

The examples of the Ridge and Lasso are an adaptation of p. 251-255 of "Introduction to
Statistical Learning with Applications in R" by Gareth James, Daniela Witten, Trevor Hastie and Robert
Tibshirani. Adapted by R. Jordan Crouser at Smith College for SDS293: Machine Learning (Spring 2016), and re-implemented in Fall 2016 in `tidyverse` format by Amelia McNamara and R. Jordan Crouser at Smith College.

Used with permission from Jordan Crouser at Smith College.  Additional Thanks to the following contributors on github:

* github.com/jcrouser
* github.com/AmeliaMN
* github.com/mhusseinmidd
* github.com/rudeboybert
* github.com/ijlyttle





<!--chapter:end:09-references.Rmd-->

